{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, re, string, itertools\n",
    "import logging\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "from nltk.stem.porter import *\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "from nltk.chunk import tree2conlltags\n",
    "from pandas import DataFrame\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn import svm                                       #library for creating the classifier, SVM\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###sorting candidates based on 15 keywords\n",
    "def get_top_candidates(candidates_list, number_keyphrases):\n",
    "    best_candidates=[]\n",
    "    for doc in candidates_list:\n",
    "        #sort candidates by tf-idf value\n",
    "        sorted_candidates=sorted(doc, key=lambda x: x[1], reverse=True)[:number_keyphrases]\n",
    "        #best_candidates.append(sorted_candidates)\n",
    "        best_candidates.append([x for x,_ in sorted_candidates])\n",
    "        #remove overlapping keywords\n",
    "    return best_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###compare candidates to goldstandard\n",
    "def extract_goldkeyphrase(gold_data):\n",
    "    \n",
    "    r_plus=re.compile(\"^.*\\+.*$\")\n",
    "    r_slash=re.compile(\"^.*\\s.*\\/.*$\")\n",
    "    \n",
    "    gold_standard=[]\n",
    "    for line in gold_data.split('\\n'):\n",
    "        doc=[]      \n",
    "        for key in line[6:].split(','):\n",
    "            if key[0]==' ':\n",
    "                doc.append(key[1:])\n",
    "            elif re.search(r_plus, key):\n",
    "                split=[]\n",
    "                for element in key.split('+'):\n",
    "                    doc.append(element)\n",
    "            elif re.search(r_slash, key):\n",
    "                split=[]\n",
    "                for element in key.split('/'):\n",
    "                    doc.append(element)\n",
    "            else:\n",
    "                doc.append(key)\n",
    "        gold_standard.append(doc)\n",
    "    return gold_standard\n",
    "\n",
    "def calculate_fmeasure(candidates_list, gold_data):\n",
    "    #true positive\n",
    "    all_matches=[]\n",
    "    for index in range(len(candidates_list)):\n",
    "        #store all measure per document in dic\n",
    "        value={'tp': None, 'fp': None, 'fn': None, 'gold': None}\n",
    "        value['gold']=len(gold_data[index])\n",
    "        #counter true positive per document\n",
    "        true_positive=0\n",
    "        #loop between elements\n",
    "        for element_candidate in candidates_list[index]:                    \n",
    "            for element_goldkeyphrase in gold_data[index]:\n",
    "                #matched predicted keyword in gold keyphrase\n",
    "                if element_candidate==element_goldkeyphrase:\n",
    "                    #matches_perdoc.append(element_candidate)\n",
    "                    true_positive+=1\n",
    "            #if need the detail of evaluation\n",
    "            value['tp']=int(true_positive) #matched pair\n",
    "            value['fp']=int(15-true_positive) #depend how many keyword should we use\n",
    "            value['fn']=int(value['gold']-value['tp'])\n",
    "        #return all metrics per document\n",
    "        all_matches.append(value)\n",
    "\n",
    "    true_positive=sum(doc['tp'] for doc in all_matches)\n",
    "    false_positive=sum(doc['fp'] for doc in all_matches)\n",
    "    false_negative=sum(doc['fn'] for doc in all_matches)\n",
    "    \n",
    "    #matched/total top n\n",
    "    precision=float(true_positive/(false_positive+true_positive))\n",
    "    #matched/total gold standard\n",
    "    recall=float(true_positive/(false_negative+true_positive))\n",
    "    # calculate with micro averagedprecision\n",
    "    f_measure=float(\"{0:.2F}\".format(2*(precision*recall)/(precision+recall)*100))\n",
    "    return f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_frequency(section):\n",
    "     #porter stemmer\n",
    "    stemmer=PorterStemmer()\n",
    "    \n",
    "    #eliminate ngram which starts or ends from stopwords\n",
    "    class NewCountVectorizer(CountVectorizer):\n",
    "        def _word_ngrams(self, tokens, stop_words=None):\n",
    "            # First get tokens without stop words\n",
    "            tokens = super(CountVectorizer, self)._word_ngrams(tokens, None)\n",
    "            if stop_words is not None:\n",
    "                new_tokens=[]\n",
    "                for token in tokens:\n",
    "                    split_words = token.split(' ')\n",
    "                    # Only check the first and last word for stop words\n",
    "                    if len(token)>2 and split_words[0] not in stop_words and split_words[-1] not in stop_words:\n",
    "                        #stem every word in token\n",
    "                        if len(split_words)==1 and len(split_words[0])>2:\n",
    "                            new_tokens.append(stemmer.stem(token))\n",
    "                        elif len(split_words)==2 and split_words[-1]==\"'\":\n",
    "                            del(token)\n",
    "                        elif len(split_words[0])<3 and len(split_words[1])<3:\n",
    "                            del(token)\n",
    "                        elif split_words[1]==\"'\" and split_words[2]==\"s\":\n",
    "                            new_tokens.append(stemmer.stem(split_words[0])+split_words[1]+split_words[2])\n",
    "                        else:\n",
    "                            new_tokens.append(' '.join(list(stemmer.stem(word) for word in word_tokenize(token))))\n",
    "                return new_tokens\n",
    "            return tokens\n",
    "    \n",
    "    stop_words=text.ENGLISH_STOP_WORDS\n",
    "    \n",
    "    count_vect=NewCountVectorizer(ngram_range=(1,5), stop_words=stop_words,\n",
    "                                token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    \n",
    "    matrix=count_vect.fit_transform(section)\n",
    "    feature_names=count_vect.get_feature_names()\n",
    "\n",
    "    #how to print tf-idf from https://stackoverflow.com/questions/34449127/\n",
    "    #sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-document\n",
    "    ngrams=[]\n",
    "    for doc in range(0,len(section)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        count_vect_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_count_vect=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in count_vect_doc]]\n",
    "        ngrams.append(names_count_vect)\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(candidates):\n",
    "    #convert the format from candidate from tuple to list\n",
    "    def feature_candidate_length(candidates):\n",
    "        feature4=[]\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_feature in range(len(candidates[n_doc])):\n",
    "                doc.append(len(candidates[n_doc][n_feature][0]))\n",
    "            feature4.append(doc)\n",
    "        return feature4\n",
    "    \n",
    "    feature4=feature_candidate_length(candidates)\n",
    "    \n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_candidate in range(len(candidates[n_doc])):\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature4[n_doc][n_candidate],)  \n",
    "   \n",
    "    x_data=[]\n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_candidate in range(len(candidates[n_doc])):\n",
    "            #append only values of features. without word\n",
    "            x_data.append(list(candidates[n_doc][n_candidate][1:]))\n",
    "    return x_data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create label for training or testing\n",
    "def create_label(candidates, label):        \n",
    "    y_label=[]\n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_cand in range(len(candidates[n_doc])):\n",
    "            keyphrase_document=list(label[n_doc])\n",
    "            if candidates[n_doc][n_cand][0] not in keyphrase_document:\n",
    "                y_label.append(0)\n",
    "            else:\n",
    "                y_label.append(1)\n",
    "    return y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_to_fmeasure(predict_proba, candidates, labels, models):\n",
    "    #all_fmeasure=[]\n",
    "    for model in range(0, len(predict_proba)):\n",
    "        probability=[]\n",
    "        counter=0\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                doc.append((candidates[n_doc][n_cand][0], predict_proba[model][counter]))\n",
    "                counter+=1\n",
    "            probability.append(doc)\n",
    "        fmeasure=calculate_fmeasure(get_top_candidates(probability, 15), labels)\n",
    "        print(\"Model %s: %.3f\" % (models[model][0], fmeasure))\n",
    "        #all_fmeasure.append((models[model][0], fmeasure))\n",
    "    return 'finish'\n",
    "\n",
    "def predict_data(x_train, y_train, x_test, y_test, candidates, labels):\n",
    "    seed = 7 #just randomly select the number\n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression(C=10))) \n",
    "    #models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    #models.append(('NB', GaussianNB()))\n",
    "    models.append(('SVM', SVC(probability=True, C=10, gamma=0.1)))\n",
    "    models.append(('RF', RF(n_estimators=15, max_depth=4)))\n",
    "    models.append(('AdaBoost', AdaBoostClassifier(n_estimators=60, learning_rate=2.0)))\n",
    "    models.append(('Bagging', BaggingClassifier(n_estimators=15)))\n",
    "    models.append(('GradientBoosting', (GradientBoostingClassifier(n_estimators=120, learning_rate=0.2))))\n",
    "    models.append(('MLP', (MLPClassifier(learning_rate_init=0.002))))\n",
    "    models.append(('Multinomial', (MultinomialNB(alpha=2.0))))\n",
    "    #models.append(('', ()))\n",
    "    #models.append(('', ()))\n",
    "    #models.append(('', ()))\n",
    "    results = []\n",
    "    names = []\n",
    "    scoring='accuracy'\n",
    "    #print(\"\\nAccuracy on testing data:\")\n",
    "    \n",
    "    print(\"Full features:\")\n",
    "    all_predict_proba=[]\n",
    "    for name, model in models:\n",
    "        #accuracy\n",
    "        #print(\"%s: %.3f\" % (name, accuracy_score(model.fit(x_train, y_train).predict(x_test), y_test)))\n",
    "        all_predict_proba.append(model.fit(x_train, y_train).predict_proba(x_test)[:,1])\n",
    "    print(\"Measuring fscore\")\n",
    "    all_fmeasure=probability_to_fmeasure(all_predict_proba, candidates, labels, models)\n",
    "  \n",
    "    return 'all predictions have been completed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def create_pickle(data, name):\n",
    "    with open('%s.pickle' % name,'wb') as handle:\n",
    "        result=pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return result\n",
    "\n",
    "def open_pickle(name):\n",
    "    with open('%s.pickle' % name,'rb') as handle:\n",
    "        result=pickle.load(handle)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening all pickles\n",
      "creating example on training..\n",
      "creating label on training..\n",
      "creating example on testing..\n",
      "creating label on testing..\n",
      "F-measure with machine learning (only TF-IDF feature)\n",
      "Full features:\n",
      "Measuring fscore\n",
      "Model LR: 14.740\n",
      "Model SVM: 0.070\n",
      "Model RF: 20.190\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-73ea93189d1f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m#next time if features hsve been complete, put in the pickle, test in here\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"F-measure with machine learning (only TF-IDF feature)\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[0mngram_prediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_x_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_y_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_x_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_y_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_ngram_candidates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'F-measure on ngram'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mngram_prediction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m#nounphrase_prediction=predict_data(nounphrase_x_train, nounphrase_y_train, nounphrase_x_test, nounphrase_y_test, test_nounphrase_candidates, test_label)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-b40006c92e91>\u001b[0m in \u001b[0;36mpredict_data\u001b[1;34m(x_train, y_train, x_test, y_test, candidates, labels)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mall_predict_proba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Measuring fscore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m     \u001b[0mall_fmeasure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprobability_to_fmeasure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_predict_proba\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m'all predictions have been completed'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-b40006c92e91>\u001b[0m in \u001b[0;36mprobability_to_fmeasure\u001b[1;34m(predict_proba, candidates, labels, models)\u001b[0m\n\u001b[0;32m     10\u001b[0m                 \u001b[0mcounter\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mprobability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mfmeasure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcalculate_fmeasure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_top_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Model %s: %.3f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmeasure\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m#all_fmeasure.append((models[model][0], fmeasure))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-23052c9eac25>\u001b[0m in \u001b[0;36mcalculate_fmeasure\u001b[1;34m(candidates_list, gold_data)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0mrecall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrue_positive\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfalse_negative\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtrue_positive\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;31m# calculate with micro averagedprecision\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mf_measure\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{0:.2F}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf_measure\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "####with machine learning\n",
    "##NGRAM\n",
    "#open all pickle\n",
    "print(\"opening all pickles\")\n",
    "train_raw=open_pickle('txt train raw')\n",
    "train_data=open_pickle('txt train data')\n",
    "train_label=open_pickle('txt train label')\n",
    "\n",
    "test_raw=open_pickle('txt test raw')\n",
    "test_data=open_pickle('txt test data')\n",
    "test_label=open_pickle('txt test label')\n",
    "\n",
    "ngram_candidates=open_pickle('txt ngram candidates')\n",
    "test_ngram_candidates=open_pickle('txt test ngram candidates')\n",
    "#nounphrase_candidates=open_pickle('txt nounphrase candidates')\n",
    "#test_nounphrase_candidates=open_pickle('txt test nounphrase candidates')\n",
    "\n",
    "print(\"creating example on training..\")\n",
    "ngram_x_train=create_example(ngram_candidates)\n",
    "print(\"creating label on training..\")\n",
    "ngram_y_train=create_label(ngram_candidates, train_label)\n",
    "print(\"creating example on testing..\")\n",
    "ngram_x_test=create_example(test_ngram_candidates)\n",
    "print(\"creating label on testing..\")\n",
    "ngram_y_test=create_label(test_ngram_candidates, test_label)\n",
    "\n",
    "#create pickle for training data\n",
    "\n",
    "\n",
    "#nounphrase_x_train=create_example(train_raw, train_data, nounphrase_candidates, train_label)\n",
    "#nounphrase_y_train=create_label(nounphrase_candidates, train_label)\n",
    "#nounphrase_x_test=create_example(test_raw, test_data, test_nounphrase_candidates, test_label)\n",
    "#nounphrase_y_test=create_label(test_nounphrase_candidates, test_label)\n",
    "\n",
    "#next time if features hsve been complete, put in the pickle, test in here\n",
    "print(\"F-measure with machine learning (only TF-IDF feature)\")\n",
    "ngram_prediction=predict_data(ngram_x_train, ngram_y_train, ngram_x_test, ngram_y_test, test_ngram_candidates, test_label)\n",
    "print('F-measure on ngram', ngram_prediction)\n",
    "#nounphrase_prediction=predict_data(nounphrase_x_train, nounphrase_y_train, nounphrase_x_test, nounphrase_y_test, test_nounphrase_candidates, test_label)\n",
    "#print('F-measure on noun phrase', nounphrase_prediction)\n",
    "\n",
    "#print(len(x_train_ngram))#print(len(y_train_ngram))#print(len(x_test_ngram))#print(len(y_test_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####setting for searching best parameter\n",
    "from sklearn import svm, grid_search\n",
    "def svc_param_selection(X, y, nfolds):\n",
    "    Cs = [0.001, 0.01, 0.1, 1, 10]\n",
    "    gammas = [0.001, 0.01, 0.1, 1]\n",
    "    param_grid = {'C': Cs, 'gamma' : gammas}\n",
    "    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds)\n",
    "    grid_search.fit(X, y)\n",
    "    grid_search.best_params_\n",
    "    return grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidates_list=[['a','b','c','d','e','a1','b1','c1','d1','e1','a2','b2','c2','d2','e2'],\n",
    "                 ['a3','b3','c3','d3','e3','a31','b31','c31','d31','e31','a32','b32','c32','d32','e32'],\n",
    "                 ['a4','b4','c4','d4','e4','a41','b41','c41','d41','e41','a42','b42','c42','d42','e42']]\n",
    "\n",
    "gold_data=[['a1','b1','c1','d','e','a12','b12','c1','d12','e12','a22','b22'],\n",
    "                 ['a33','b33','c33','d33','e33','a313','b313','c313','a323','b32','c323','d323','e32'],\n",
    "                 ['a44','b44','c44','d44','e44','a441','d441','e441','a442','b442','c442','d442','e442']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing o compare tfidfvalue with one feature\n",
    "\n",
    "\n",
    "#feature phrase length\n",
    "#feature1=[]\n",
    "#for n_doc in range(len(tfidf)):\n",
    "#    doc=[]\n",
    "#    for n_feature in range(len(tfidf[n_doc])):\n",
    "#        doc.append(len(tfidf[n_doc][n_feature][0]))\n",
    "#    feature1.append(doc)\n",
    "#print(feature1)\n",
    "\n",
    "tfidf=[[('dog',1),('swimming',4),('car',7)],\n",
    "      [('air',11),('bowl',14),('cone',17),('done',17)],\n",
    "       [('air of water',21),('chocolate biscuit',24)],\n",
    "      [('air conditioner',21),('hot white chocolate',24)],]\n",
    "\n",
    "title=[[('dog',0),('rabbit',0),('snake',0),('car',0)],\n",
    "      [('bowl',0),('dog',0),('rabbit',0)],\n",
    "      [('chocolate biscuits',0),('a lot air of water',0),('rabbit',0),('snake',0)],\n",
    "      [('air conditioner',0),('hot white',0)]]\n",
    "\n",
    "#is_title, is_abstract, is etc, but extract section with ngram(1,5)\n",
    "feature2=[]\n",
    "for n_doc in range(len(tfidf)):\n",
    "    doc=[]\n",
    "    for n_feature in range(len(tfidf[n_doc])):\n",
    "        #title_feature=[feature for feature in title[n_doc]]\n",
    "        title_feature=[feature for feature, value in title[n_doc]]\n",
    "        if tfidf[n_doc][n_feature][0] not in title_feature:\n",
    "            doc.append(0)\n",
    "        else:\n",
    "            doc.append(1)\n",
    "    feature2.append(doc)\n",
    "print(feature2)\n",
    "\n",
    "#is_abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''If need cross validation per model\n",
    "###measure accuracy with k-fold\n",
    "print(\"Accuracy on training data with Cross-validation:\")\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, x_train_ngram, y_train_ngram, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %.3f (%.3f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates=[[('aa',1),('a',5),('a3',5),('a6',7)],\n",
    "            [('aq',3),('aw',4),('ag',2),('ar',8)]]\n",
    "\n",
    "feature1=[[3,4,5,6],\n",
    "            [7,9,6,5]]\n",
    "feature2=[[1,2,7,8],\n",
    "            [9,90,4,3]]\n",
    "\n",
    "for n_doc in range(len(candidates)):\n",
    "    for n_candidate in range(len(candidates[n_doc])):\n",
    "        candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature1[n_doc][n_candidate],)\n",
    "        candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate],)\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
