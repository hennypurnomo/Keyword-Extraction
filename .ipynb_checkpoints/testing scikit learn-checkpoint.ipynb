{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, re, string, itertools\n",
    "\n",
    "from nltk.stem.porter import *\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "from nltk.chunk import tree2conlltags\n",
    "from pandas import DataFrame\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "import numpy as np\n",
    "from scipy.sparse.csr import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(input_list):\n",
    "    result=[]\n",
    "    #remove unwanted character per line\n",
    "    for line in input_list:\n",
    "        clean=re.sub(\"(\\.)?\\n\",'', line) #remove \\n\n",
    "        clean=re.sub(\"'s\",'', clean) #remove [2]\n",
    "        clean=re.sub(\"\\[([0-9]{1,2}\\,?\\s?)+\\]\",'', clean) #remove [2]\n",
    "        clean=re.sub(\"\\(([0-9]{1,2}\\,?\\s?)+\\)\",'', clean) #remove (2)\n",
    "        #remove fig. 2 etc, need improvement to catch the sentence after it\n",
    "        clean=re.sub(\"([Ff]ig.|[Ff]igure|[Tt]ab.|[Tt]able)\\s?[0-9]{1,2}\",'', clean) #remove fig. 2 etc\n",
    "        result.append(clean.lower())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ngram_tfidf(corpus):\n",
    "    stemmer=PorterStemmer()\n",
    "    class NewTfidfVectorizer(TfidfVectorizer):\n",
    "        def _word_ngrams(self, tokens, stop_words=None):\n",
    "            tokens = super(TfidfVectorizer, self)._word_ngrams(tokens, None)\n",
    "            if stop_words is not None:\n",
    "                new_tokens=[]\n",
    "                for token in tokens:\n",
    "                    split_words = token.split(' ')\n",
    "                    # Only check the first and last word for stop words\n",
    "                    if len(token)>2 and split_words[0] not in stop_words and split_words[-1] not in stop_words:\n",
    "                        #stem every word in token\n",
    "                        if len(split_words)==1 and len(split_words[0])>2:\n",
    "                            new_tokens.append(stemmer.stem(token))\n",
    "                        elif len(split_words[0])<3 and len(split_words[1])<3:\n",
    "                            del(token)\n",
    "                        else:\n",
    "                            new_tokens.append(' '.join(list(stemmer.stem(word) for word in word_tokenize(token))))\n",
    "                return new_tokens\n",
    "            return tokens\n",
    "    stop_words=text.ENGLISH_STOP_WORDS\n",
    "    tfidf=NewTfidfVectorizer(ngram_range=(1,5), stop_words=stop_words,\n",
    "                                token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    matrix=tfidf.fit_transform(corpus)\n",
    "    vocab_perdoc=tfidf.inverse_transform(matrix)\n",
    "    return vocab_perdoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_custom_ngram_tfidf(corpus,voc):\n",
    "    stemmer=PorterStemmer()\n",
    "    class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "        def build_tokenizer(self):\n",
    "            tokenizer=super(TfidfVectorizer, self).build_tokenizer()\n",
    "            return lambda doc: (stemmer.stem(token) for token in tokenizer(doc))\n",
    "    #class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    #    def build_analyzer(self):\n",
    "    #        analyzer=super(TfidfVectorizer, self).build_analyzer()\n",
    "    #        return lambda doc: (stemmer.stem(word) for word in analyzer(doc))\n",
    "    stop_words=text.ENGLISH_STOP_WORDS\n",
    "    tfidf=StemmedTfidfVectorizer(ngram_range=(1,5), stop_words=stop_words, vocabulary=voc)\n",
    "    matrix=tfidf.fit_transform(corpus)\n",
    "    vocab_perdoc=tfidf.inverse_transform(matrix)\n",
    "    return vocab_perdoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate noun phrases based on corpus, just only noun phrase, not the keywords\n",
    "#search to find sentence boundary\n",
    "def vocabulary_nounphrases(raw_data):\n",
    "    stemmer=PorterStemmer()\n",
    "    #from http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\n",
    "    #grammar=r'NP: {(<JJ.*>* <NN.*>+ <IN>)? <JJ.*>* <NN.*>+}' #only detect noun phrases that contain specific pattern, hypen word is counted as one NN\n",
    "    grammar=r'NP: {(<NN.*>|<JJ.*>|<VBN>|<NN> <IN>|<NNS> <IN>)* (<NN.*>|<VBG>)}' \n",
    "    punct = set(string.punctuation) #list of punctuation\n",
    "    stop_words=text.ENGLISH_STOP_WORDS\n",
    "    chunker = RegexpParser(grammar) #chunker from nltk\n",
    "    def lambda_unpack(f):\n",
    "        return lambda args:f(*args)\n",
    "    postag_sents = pos_tag_sents(word_tokenize(sent) for sent in raw_data) #tokenize and create pos tag per sentenc\n",
    "    noun_phrases = list(chain.from_iterable(tree2conlltags(chunker.parse(tagged_sent)) for tagged_sent in postag_sents)) \n",
    "    merged_nounphrase = [' '.join(stemmer.stem(word) for word, pos, chunk in group if re.search(r\"(?u)\\b[A-Za-z-]+\\b\", word)).lower() for key, group in\n",
    "                    itertools.groupby(noun_phrases, lambda_unpack(lambda word, pos, chunk: chunk != 'O')) if key]\n",
    "    #merged_nounphrase = [' '.join(stemmer.stem(word) for word, pos, chunk in group if re.search(r\"(?u)\\b[A-Za-z-]+\\b\", word)).lower() for key, group in\n",
    "    #                itertools.groupby(noun_phrases, lambda_unpack(lambda word, pos, chunk: chunk != 'O')) if key]\n",
    "    all_nounphrases=[cand for cand in merged_nounphrase\n",
    "            if len(cand)>2 and cand not in stop_words and not all(char in punct for char in cand)]\n",
    "    vocabulary=(list(set(all_nounphrases)))\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nounphrase_tfidf(corpus, voc):\n",
    "    stemmer=PorterStemmer()\n",
    "    class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "        def build_analyzer(self):\n",
    "            analyzer=super(TfidfVectorizer, self).build_analyzer()\n",
    "            return lambda doc: (stemmer.stem(word) for word in analyzer(doc))\n",
    "    tfidf=StemmedTfidfVectorizer(vocabulary=voc, ngram_range=(1,5)) \n",
    "    matrix=tfidf.fit_transform(corpus)\n",
    "    feature_names=tfidf.get_feature_names()\n",
    "    candidates=[]\n",
    "    for doc in range(0,len(corpus)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        tfidf_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_tfidf=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in tfidf_doc]]\n",
    "        candidates.append(names_tfidf)\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['major implic from studi', 'today video', 'machin learn', 'money', 'major studi in comput scienc', 'colchest for longer period', 'colchest cat', 'anim']\n",
      "[array(['anim', 'today video'], dtype='<U28'), array(['machin learn'], dtype='<U28'), array(['anim', 'money'], dtype='<U28')]\n"
     ]
    }
   ],
   "source": [
    "#corpus=['A challenging problem faced by researchers and developers of distributed real-time and embedded (DRE) systems is devising and implementing effective adaptive resource management strategies that can meet end-to-end quality of service(QoS) requirements in varying operational conditions.',\n",
    "#        'HyARM is based on hybrid control theoretic techniques [8],which provide a theoretical framework for designing control of complex system with both continuous and discrete dynamics.']\n",
    " \n",
    "corpus=[\"Today's video is about the animal. Do not stay in Colchester for longer period!\",\n",
    "       \"Machine learning is one of major studies in Computer Science. I love it Colchester\",\n",
    "       \"Cat is an animal. Major implication from studies is money#\"]\n",
    "\n",
    "s_corpus=[\"Today video is about the anim. Do not stay in Colchest for longer period!\",\n",
    "       \"Machine learn is one of major studi in Comput Scienc. I love it Colchest\",\n",
    "       \"Cat is an anim. Major implic from studi is money#\"]\n",
    "\n",
    "#if document is not be stemmed or follow the rule from vocabulary, they cant match each other\n",
    "vngram=['video','colchest','stay in colchest','longer period','comput scienc','anim major','major implic']\n",
    "c_corpus=clean(corpus)\n",
    "voc=vocabulary_nounphrases(c_corpus)\n",
    "ngrams=calculate_custom_ngram_tfidf(c_corpus, vngram)\n",
    "#print(ngrams)\n",
    "print(voc)\n",
    "phrases=calculate_custom_ngram_tfidf(c_corpus,voc)\n",
    "print(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
