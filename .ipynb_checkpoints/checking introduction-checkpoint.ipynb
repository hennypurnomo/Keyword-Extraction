{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, re, string, itertools\n",
    "import logging\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "from nltk.stem.porter import *\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "from nltk.chunk import tree2conlltags\n",
    "from pandas import DataFrame\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn import svm                                       #library for creating the classifier, SVM\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(input_list):\n",
    "    result=[]\n",
    "    #remove unwanted character per line\n",
    "    for line in input_list:\n",
    "        clean=re.sub(\"(\\.)?\\n\",'', line) #remove \\n\n",
    "        clean=re.sub(\"('s)\",'', clean) #remove 's\n",
    "        clean=re.sub(\"\\[([0-9]{1,2}\\,?\\s?)+\\]\",'', clean) #remove [2]\n",
    "        clean=re.sub(\"\\(([0-9]{1,2}\\,?\\s?)+\\)\",'', clean) #remove (2)\n",
    "        #clean=re.sub(r\"\\b(iv|ix|x|v?i{0,3})+\\b\",'', clean) #remove roman number\n",
    "        #remove fig. 2 etc, need improvement to catch the sentence after it\n",
    "        #clean=re.sub(r\"\\b(i.e.g.|e.g.|i.e.)\",'', clean) #remove i.e.g., i.e., e.g.\n",
    "        clean=re.sub(\"([Ff]ig.|[Ff]igure|[Tt]ab.|[Tt]able)\\s?[0-9]{1,2}\",'', clean) #remove fig. 2 etc\n",
    "        clean=re.sub(r\"\\b((https?://|www.)[^\\s]+)\",'', clean) #remove email\n",
    "        result.append(clean)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(path):\n",
    "    raw=[]\n",
    "    for file in path:\n",
    "        dict_doc={'doc_id': None, 'title': None, 'abstract': None, 'introduction': None, 'full-text': None}\n",
    "        file_id=os.path.basename(file).rstrip('.txt.final') #catch only file name  \n",
    "        dict_doc['doc_id']=file_id\n",
    "        \n",
    "        source=open(file,encoding='utf-8').readlines()\n",
    "        source=clean(source)\n",
    "        \n",
    "        ##########detect title\n",
    "        beginning=re.sub(\"\\n\", \"\", source[0]) #retrieve title\n",
    "        candidate=re.sub(\"\\n\", \"\", source[1]) # retrieve title candidate\n",
    "        h_candidate=word_tokenize(re.sub(\"-\",' ',candidate)) #tokenize the candidate\n",
    "        \n",
    "        title=[]\n",
    "        name=[]\n",
    "        for word in h_candidate:\n",
    "            if wordnet.synsets(word): #check if title candidate exist on wordnet\n",
    "                title.append(word)\n",
    "            else:\n",
    "                name.append(word)\n",
    "            #if title>\n",
    "            if len(title)>len(name): \n",
    "                newtitle=beginning+' '+candidate\n",
    "            elif len(title)==len(name):\n",
    "                newtitle=beginning\n",
    "            else:\n",
    "                newtitle=beginning\n",
    "\n",
    "        dict_doc['title']=newtitle\n",
    "        \n",
    "        content=source[2:]\n",
    "        ######check header, inconsistency all file\n",
    "        r_intro=re.compile(\"^1\\.?\\s[A-Z]+\")\n",
    "        r_after_intro=re.compile(\"^2\\.?\\s[A-Z]+\")\n",
    "        r_ref=re.compile(\"[0-9]{1,2}?\\.?\\s?R[EFERENCES|eferences]\") #detect reference\n",
    "        #r_header=re.compile(\"[0-9]{1,2}?\\.?\\s?[A-Z]\")\n",
    "        \n",
    "        in_abstract=content.index('ABSTRACT')\n",
    "        in_authorkey=content.index('Categories and Subject Descriptors')\n",
    "        \n",
    "        list_intro=[i for i, item in enumerate(content) if re.search(r_intro, item)]\n",
    "        in_intro=list_intro[0]\n",
    "        list_after_intro=[i for i, item in enumerate(content) if re.search(r_after_intro, item)]\n",
    "        in_after_intro=list_after_intro[0]\n",
    "        list_ref=[i for i, item in enumerate(content) if re.search(r_ref, item)]\n",
    "        in_ref=list_ref[0]\n",
    "        \n",
    "        abstract=content[in_abstract+1:in_authorkey] #eliminate keyword and category\n",
    "        intro=content[in_intro+1:in_after_intro]\n",
    "        body=content[in_after_intro+1:in_ref] #remove reference \n",
    "        \n",
    "        #body=content[in_intro+1:in_ref] #remove reference       \n",
    "\n",
    "        list_title=[]\n",
    "        list_title.append(newtitle)\n",
    "        \n",
    "        #full_text=list(chain(list_title, abstract, intro, body))\n",
    "        #dict_doc['abstract']=abstract\n",
    "        dict_doc['introduction']=intro\n",
    "        #dict_doc['body']=body\n",
    "        #dict_doc['full_text']=full_text\n",
    "        \n",
    "        #per sentence in a document\n",
    "        raw.append(dict_doc)\n",
    "    return raw\n",
    "\n",
    "train_directory=glob.glob('./se_txt/train/dummy/*.txt.final')\n",
    "train_raw=load_files(train_directory)\n",
    "print(train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data to tfidfvectorizer format\n",
    "#corpus=['a','b','c']\n",
    "#RENAME TO CREATE CORPUS\n",
    "def create_corpus(raw_data):\n",
    "    train_data=[]\n",
    "    for doc in raw_data:\n",
    "        #add to list and join all element in full text into a text\n",
    "        train_data.append(' '.join(doc['full_text']))\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ngram_tfidf(corpus):\n",
    "    \n",
    "    #porter stemmer\n",
    "    stemmer=PorterStemmer()\n",
    "\n",
    "    #eliminate ngram which starts or ends from stopwords\n",
    "    #from https://stackoverflow.com/questions/49746555/sklearn-tfidfvectorizer-generate-custom\n",
    "    #-ngrams-by-not-removing-stopword-in-the/49775000#49775000\n",
    "    class NewTfidfVectorizer(TfidfVectorizer):\n",
    "        def _word_ngrams(self, tokens, stop_words=None):\n",
    "            # First get tokens without stop words\n",
    "            tokens = super(TfidfVectorizer, self)._word_ngrams(tokens, None)\n",
    "            if stop_words is not None:\n",
    "                new_tokens=[]\n",
    "                for token in tokens:\n",
    "                    split_words = token.split(' ')\n",
    "                    # Only check the first and last word for stop words\n",
    "                    if len(token)>2 and split_words[0] not in stop_words and split_words[-1] not in stop_words:\n",
    "                        #stem every word in token\n",
    "                        if len(split_words)==1 and len(split_words[0])>2:\n",
    "                            new_tokens.append(stemmer.stem(token))\n",
    "                        elif len(split_words)==2 and split_words[-1]==\"'\":\n",
    "                            del(token)\n",
    "                        elif len(split_words[0])<3 and len(split_words[1])<3:\n",
    "                            del(token)\n",
    "                        elif split_words[1]==\"'\" and split_words[2]==\"s\":\n",
    "                            new_tokens.append(stemmer.stem(split_words[0])+split_words[1]+split_words[2])\n",
    "                        else:\n",
    "                            new_tokens.append(' '.join(list(stemmer.stem(word) for word in word_tokenize(token))))\n",
    "                return new_tokens\n",
    "            return tokens\n",
    "    \n",
    "    stop_words=text.ENGLISH_STOP_WORDS\n",
    "    \n",
    "    tfidf=NewTfidfVectorizer(ngram_range=(1,5), stop_words=stop_words,\n",
    "                                token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    \n",
    "    matrix=tfidf.fit_transform(corpus)\n",
    "    feature_names=tfidf.get_feature_names()\n",
    "\n",
    "    #how to print tf-idf from https://stackoverflow.com/questions/34449127/\n",
    "    #sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-documen\n",
    "    candidates=[]\n",
    "    for doc in range(0,len(corpus)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        tfidf_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_tfidf=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in tfidf_doc]]\n",
    "        candidates.append(names_tfidf)\n",
    "    \n",
    "    #this is the candidates per document\n",
    "    #vocab_perdoc=tfidf.inverse_transform(matrix)\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate noun phrases based on corpus\n",
    "def create_phrase_vocabulary(raw_data):\n",
    "    \n",
    "    #porter stemmer\n",
    "    stemmer=PorterStemmer()\n",
    "    \n",
    "    #from http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\n",
    "    grammar=r'NP: {(<JJ.*>* <NN.*>+ <IN>)? (<JJ.*>* <NN.*>+)+}' #only detect noun phrases that contain specific pattern, hypen word is counted as one NN\n",
    "    \n",
    "    #test new grammar\n",
    "    #grammar=r'NP: {(<JJ>* <VBN>? <NN.*>+ <IN>)? <JJ>* <VBG>? <NN.*>+}' \n",
    "    \n",
    "    punct = set(string.punctuation) #list of punctuation\n",
    "    chunker = RegexpParser(grammar) #chunker from nltk\n",
    "    \n",
    "    def lambda_unpack(f):\n",
    "        return lambda args:f(*args)\n",
    "    \n",
    "    postag_sents = pos_tag_sents(word_tokenize(sent) for sent in raw_data) #tokenize and create pos tag per sentence\n",
    "    #list of IOB of noun phrases based on the specific grammar\n",
    "    noun_phrases = list(chain.from_iterable(tree2conlltags(chunker.parse(tagged_sent)) for tagged_sent in postag_sents)) \n",
    "    \n",
    "    #join B-NP and I-NP tags as one noun phrase excluding O tags    \n",
    "    merged_nounphrase = [' '.join(stemmer.stem(word) for word, pos, chunk in group).lower() for key, group in\n",
    "                    itertools.groupby(noun_phrases, lambda_unpack(lambda word, pos, chunk: chunk != 'O')) if key]\n",
    "    \n",
    "    #filter noun phrases from stopwords and punctuation\n",
    "    all_nounphrases=[cand for cand in merged_nounphrase\n",
    "            if len(cand)>2 and not all(char in punct for char in cand)]\n",
    "    \n",
    "    #select distinct noun phrases\n",
    "    vocabulary=(list(set(all_nounphrases)))\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nounphrase_tfidf(corpus, voc):\n",
    "    \n",
    "    stemmer=PorterStemmer()\n",
    "    \n",
    "    class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "        def build_tokenizer(self):\n",
    "            tokenizer=super(TfidfVectorizer, self).build_tokenizer()\n",
    "            return lambda doc: (stemmer.stem(token) for token in tokenizer(doc) if token not in stop_words)\n",
    "\n",
    "    stop_words=set(text.ENGLISH_STOP_WORDS)\n",
    "    s=['of','in','on','for']\n",
    "    stop_words=stop_words.difference(s)\n",
    "    tfidf=StemmedTfidfVectorizer(ngram_range=(1,5), stop_words=stop_words, vocabulary=voc, token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    \n",
    "    matrix=tfidf.fit_transform(corpus)\n",
    "    feature_names=tfidf.get_feature_names()\n",
    "\n",
    "    #how to print tf-idf from https://stackoverflow.com/questions/34449127/\n",
    "    #sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-documen\n",
    "    candidates=[]\n",
    "    for doc in range(0,len(corpus)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        tfidf_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_tfidf=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in tfidf_doc]]\n",
    "        candidates.append(names_tfidf)\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###sorting candidates based on 15 keywords\n",
    "def get_top_candidates(candidates_list, number_keyphrases):\n",
    "    best_candidates=[]\n",
    "    for doc in candidates_list:\n",
    "        #sort candidates by tf-idf value\n",
    "        sorted_candidates=sorted(doc, key=lambda x: x[1], reverse=True)[:number_keyphrases]\n",
    "        #best_candidates.append(sorted_candidates)\n",
    "        best_candidates.append([x for x,_ in sorted_candidates])\n",
    "        #remove overlapping keywords\n",
    "    return best_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###compare candidates to goldstandard\n",
    "def extract_goldkeyphrase(gold_data):\n",
    "    \n",
    "    r_plus=re.compile(\"^.*\\+.*$\")\n",
    "    r_slash=re.compile(\"^.*\\s.*\\/.*$\")\n",
    "    \n",
    "    gold_standard=[]\n",
    "    for line in gold_data.split('\\n'):\n",
    "        doc=[]      \n",
    "        for key in line[6:].split(','):\n",
    "            if key[0]==' ':\n",
    "                doc.append(key[1:])\n",
    "            elif re.search(r_plus, key):\n",
    "                split=[]\n",
    "                for element in key.split('+'):\n",
    "                    doc.append(element)\n",
    "            elif re.search(r_slash, key):\n",
    "                split=[]\n",
    "                for element in key.split('/'):\n",
    "                    doc.append(element)\n",
    "            else:\n",
    "                doc.append(key)\n",
    "        gold_standard.append(doc)\n",
    "    return gold_standard\n",
    "\n",
    "def calculate_fmeasure(candidates_list, gold_data):\n",
    "    #true positive\n",
    "    all_matches=[]\n",
    "    for index in range(len(candidates_list)):\n",
    "        #store all measure per document in dic\n",
    "        value={'tp': None, 'fp': None, 'fn': None, 'gold': None}\n",
    "        value['gold']=len(gold_data[index])\n",
    "        #counter true positive per document\n",
    "        true_positive=0\n",
    "        #loop between elements\n",
    "        for element_candidate in candidates_list[index]:                    \n",
    "            for element_goldkeyphrase in gold_data[index]:\n",
    "                #matched predicted keyword in gold keyphrase\n",
    "                if element_candidate==element_goldkeyphrase:\n",
    "                    #matches_perdoc.append(element_candidate)\n",
    "                    true_positive+=1\n",
    "            #if need the detail of evaluation\n",
    "            value['tp']=int(true_positive) #matched pair\n",
    "            value['fp']=int(15-true_positive) #depend how many keyword should we use\n",
    "            value['fn']=int(value['gold']-value['tp'])\n",
    "        #return all metrics per document\n",
    "        all_matches.append(value)\n",
    "\n",
    "    true_positive=sum(doc['tp'] for doc in all_matches)\n",
    "    false_positive=sum(doc['fp'] for doc in all_matches)\n",
    "    false_negative=sum(doc['fn'] for doc in all_matches)\n",
    "    \n",
    "    #matched/total top n\n",
    "    precision=float(true_positive/(false_positive+true_positive))\n",
    "    #matched/total gold standard\n",
    "    recall=float(true_positive/(false_negative+true_positive))\n",
    "    # calculate with micro averagedprecision\n",
    "    f_measure=float(\"{0:.2F}\".format(2*(precision*recall)/(precision+recall)*100))\n",
    "    return f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_frequency(section):\n",
    "     #porter stemmer\n",
    "    stemmer=PorterStemmer()\n",
    "    \n",
    "    #eliminate ngram which starts or ends from stopwords\n",
    "    class NewCountVectorizer(CountVectorizer):\n",
    "        def _word_ngrams(self, tokens, stop_words=None):\n",
    "            # First get tokens without stop words\n",
    "            tokens = super(CountVectorizer, self)._word_ngrams(tokens, None)\n",
    "            if stop_words is not None:\n",
    "                new_tokens=[]\n",
    "                for token in tokens:\n",
    "                    split_words = token.split(' ')\n",
    "                    # Only check the first and last word for stop words\n",
    "                    if len(token)>2 and split_words[0] not in stop_words and split_words[-1] not in stop_words:\n",
    "                        #stem every word in token\n",
    "                        if len(split_words)==1 and len(split_words[0])>2:\n",
    "                            new_tokens.append(stemmer.stem(token))\n",
    "                        elif len(split_words)==2 and split_words[-1]==\"'\":\n",
    "                            del(token)\n",
    "                        elif len(split_words[0])<3 and len(split_words[1])<3:\n",
    "                            del(token)\n",
    "                        elif split_words[1]==\"'\" and split_words[2]==\"s\":\n",
    "                            new_tokens.append(stemmer.stem(split_words[0])+split_words[1]+split_words[2])\n",
    "                        else:\n",
    "                            new_tokens.append(' '.join(list(stemmer.stem(word) for word in word_tokenize(token))))\n",
    "                return new_tokens\n",
    "            return tokens\n",
    "    \n",
    "    stop_words=text.ENGLISH_STOP_WORDS\n",
    "    \n",
    "    count_vect=NewCountVectorizer(ngram_range=(1,5), stop_words=stop_words,\n",
    "                                token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    \n",
    "    matrix=count_vect.fit_transform(section)\n",
    "    feature_names=count_vect.get_feature_names()\n",
    "\n",
    "    #how to print tf-idf from https://stackoverflow.com/questions/34449127/\n",
    "    #sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-document\n",
    "    ngrams=[]\n",
    "    for doc in range(0,len(section)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        count_vect_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_count_vect=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in count_vect_doc]]\n",
    "        ngrams.append(names_count_vect)\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------TF-IDF version\n",
    "###load training data\n",
    "train_directory=glob.glob('./se_txt/train/*.txt.final')\n",
    "train_raw=load_files(train_directory)\n",
    "pickle_train_raw=create_pickle(train_raw,'txt train raw')\n",
    "train_data=create_corpus(train_raw)\n",
    "pickle_train_data=create_pickle(train_data,'txt train data')\n",
    "\n",
    "#load gold keyphrase\n",
    "train_label_directory=open('./se_txt/train/train.combined.stem.final', encoding='utf-8').read()\n",
    "train_label=extract_goldkeyphrase(train_label_directory)\n",
    "pickle_train_label=create_pickle(train_label, 'txt train label')\n",
    "\n",
    "###Load testing data\n",
    "test_directory=glob.glob('./se_txt/test/*.txt.final')\n",
    "test_raw=load_files(test_directory)\n",
    "pickle_test_raw=create_pickle(test_raw,'txt test raw')\n",
    "test_data=create_corpus(test_raw)\n",
    "pickle_test_data=create_pickle(test_data,'txt test data')\n",
    "\n",
    "test_label_directory=open('./se_txt/test_answer/test.combined.stem.final', encoding='utf-8').read()\n",
    "test_label=extract_goldkeyphrase(test_label_directory)\n",
    "pickle_test_label=create_pickle(test_label, 'txt test label')\n",
    "\n",
    "#### Ngram version\n",
    "print(\"N-gram TF-IDF version\")\n",
    "ngram_candidates=calculate_ngram_tfidf(train_data) \n",
    "pickle_ngram_candidates=create_pickle(ngram_candidates, 'txt ngram candidates')\n",
    "#ngram_top_keyphrases=get_top_candidates(ngram_candidates, 15)\n",
    "#ngram_fmeasure=calculate_fmeasure(ngram_top_keyphrases, train_label)\n",
    "#print(\"F-measure on training:\", ngram_fmeasure)\n",
    "\n",
    "test_ngram_candidates=calculate_ngram_tfidf(test_data)\n",
    "pickle_test_ngram_candidates=create_pickle(test_ngram_candidates, 'txt test ngram candidates')\n",
    "#test_ngram_top_candidates=get_top_candidates(test_ngram_candidates, 15)\n",
    "#test_ngram_fmeasure=calculate_fmeasure(test_ngram_top_candidates, test_label)\n",
    "#print(\"F-measure on testing:\", test_ngram_fmeasure)\n",
    "\n",
    "\n",
    "#### Noun phrase version\n",
    "print(\"Noun phrase TF-IDF version\")\n",
    "nounphrase_vocabulary=create_phrase_vocabulary(train_data)\n",
    "nounphrase_candidates=calculate_nounphrase_tfidf(train_data, nounphrase_vocabulary)\n",
    "pickle_nounphrase_candidates=create_pickle(nounphrase_candidates, 'txt nounphrase candidates')\n",
    "#nounphrase_top_keyphrases=get_top_candidates(nounphrase_candidates, 15)\n",
    "#nounphrase_fmeasure=calculate_fmeasure(nounphrase_top_keyphrases, train_label)\n",
    "#print(\"F-measure on training:\", nounphrase_fmeasure)\n",
    "\n",
    "test_nounphrase_vocabulary=create_phrase_vocabulary(test_data)\n",
    "test_nounphrase_candidates=calculate_nounphrase_tfidf(test_data, test_nounphrase_vocabulary)\n",
    "pickle_test_nounphrase_candidates=create_pickle(test_nounphrase_candidates, 'txt test nounphrase candidates')\n",
    "#test_nounphrase_top_candidates=get_top_candidates(test_nounphrase_candidates, 15)\n",
    "#test_nounphrase_fmeasure=calculate_fmeasure(test_nounphrase_top_candidates, test_label)\n",
    "#print(\"F-measure on testing:\", test_nounphrase_fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus):\n",
    "    clean=[]\n",
    "    stemmer=PorterStemmer()\n",
    "    for doc in corpus:\n",
    "        cleaned_words=\" \".join([word for word in word_tokenize(doc.lower()) if re.search(r\"\\b[A-Za-z-]+\\b\", word) and len(word)>2])\n",
    "        stemmed_words=[stemmer.stem(word) for word in cleaned_words.split()]\n",
    "        clean.append(\" \".join([word for word in stemmed_words]))\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(raw_data, corpus, candidates, label):\n",
    "    \n",
    "    def feature_is_title(candidates, raw_data):\n",
    "        titles=[doc['title'] for doc in raw_data]\n",
    "        title_tf=calculate_term_frequency(titles)\n",
    "        feature2=[]\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_feature in range(len(candidates[n_doc])):\n",
    "                features_perdoc=[feature for feature, value in title_tf[n_doc]]\n",
    "                if candidates[n_doc][n_feature][0] not in features_perdoc:\n",
    "                    doc.append(0)\n",
    "                else:\n",
    "                    doc.append(1)\n",
    "            feature2.append(doc)\n",
    "        return feature2\n",
    "    \n",
    "    #refine with similarity\n",
    "    def feature_is_abstract(candidates, raw_data):\n",
    "        abstracts=[' '.join(doc['abstract']) for doc in raw_data]\n",
    "        abstract_tf=calculate_term_frequency(abstracts)\n",
    "        feature3=[]\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_feature in range(len(candidates[n_doc])):\n",
    "                features_perdoc=[feature for feature, value in abstract_tf[n_doc]]\n",
    "                if candidates[n_doc][n_feature][0] not in features_perdoc:\n",
    "                    doc.append(0)\n",
    "                else:\n",
    "                    doc.append(1)\n",
    "            feature3.append(doc)\n",
    "        return feature3\n",
    "    \n",
    "    def feature_candidate_length(candidates):\n",
    "        feature4=[]\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_feature in range(len(candidates[n_doc])):\n",
    "                doc.append(len(candidates[n_doc][n_feature][0]))\n",
    "            feature4.append(doc)\n",
    "        return feature4\n",
    "    \n",
    "    def feature_term_frequency(corpus):\n",
    "        term_frequency=calculate_term_frequency(corpus) #save as pickle for term frequency, it can be used for counting n title or n abstract\n",
    "        feature5=[]\n",
    "        for n_doc in range(len(term_frequency)):\n",
    "            doc=[]\n",
    "            for n_feature in range(len(term_frequency[n_doc])):\n",
    "                doc.append(term_frequency[n_doc][n_feature][1])\n",
    "            feature5.append(doc)\n",
    "        return feature5\n",
    "    \n",
    "    def feature_supervised_keyphraseness(corpus, label): #make sure this is only keyphrase per document or all keyphrase compare\n",
    "        term_frequency=calculate_term_frequency(corpus)\n",
    "        merged_labels=list(chain.from_iterable(label))\n",
    "        feature6=[]\n",
    "        for n_doc in range(len(term_frequency)):\n",
    "            doc=[]\n",
    "            for n_feature in range(len(term_frequency[n_doc])):\n",
    "                #gold_label=list(label[n_doc])\n",
    "                if term_frequency[n_doc][n_feature][0] not in merged_labels:\n",
    "                    doc.append(0)\n",
    "                else:\n",
    "                    doc.append(term_frequency[n_doc][n_feature][1])\n",
    "            feature6.append(doc)\n",
    "        return feature6\n",
    "    \n",
    "    def feature_first_occurence(candidates, corpus):\n",
    "        feature7=[]\n",
    "        cleaned_corpus=clean_corpus(corpus)\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                index=cleaned_corpus[n_doc].lower().find(candidates[n_doc][n_cand][0])\n",
    "                preceding_words=len(cleaned_corpus[n_doc][:index].split(\" \"))-1\n",
    "                doc.append(preceding_words)\n",
    "            feature7.append(doc)\n",
    "        return feature7\n",
    "\n",
    "    def feature_distance(candidates, corpus):\n",
    "        #cleaning the CORPUS from \n",
    "        feature8=[]\n",
    "        cleaned_corpus=clean_corpus(corpus)\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            corpus_words=len(cleaned_corpus[n_doc].split(\" \"))\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                index=cleaned_corpus[n_doc].lower().find(candidates[n_doc][n_cand][0])\n",
    "                preceding_words=len(cleaned_corpus[n_doc][:index].split(\" \"))-1\n",
    "                position=float(\"{0:.2F}\".format(preceding_words/corpus_words))\n",
    "                doc.append(position)\n",
    "            feature8.append(doc)\n",
    "        return feature8\n",
    "    \n",
    "    #lists of feature\n",
    "    feature2=feature_is_title(candidates, raw_data)\n",
    "    feature3=feature_is_abstract(candidates, raw_data)\n",
    "    feature4=feature_candidate_length(candidates)\n",
    "    feature5=feature_term_frequency(corpus)\n",
    "    feature6=feature_supervised_keyphraseness(corpus, label)\n",
    "    feature7=feature_first_occurence(candidates, corpus) #important feature\n",
    "    feature8=feature_distance(candidates, corpus)\n",
    "    \n",
    "    #add values of all features into candidate list\n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_candidate in range(len(candidates[n_doc])):\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate],)\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature3[n_doc][n_candidate],)\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature4[n_doc][n_candidate],)  \n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature5[n_doc][n_candidate],)\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature6[n_doc][n_candidate],)\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature7[n_doc][n_candidate],)\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature8[n_doc][n_candidate],)\n",
    "            \n",
    "    #convert the format from candidate from tuple to list\n",
    "    x_data=[]\n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_candidate in range(len(candidates[n_doc])):\n",
    "            #append only values of features. without word\n",
    "            x_data.append(list(candidates[n_doc][n_candidate][1:]))\n",
    "    return x_data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create label for training or testing\n",
    "def create_label(candidates, label):\n",
    "    y_label=[]\n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_cand in range(len(candidates[n_doc])):\n",
    "            keyphrase_document=list(label[n_doc])\n",
    "            if candidates[n_doc][n_cand][0] not in keyphrase_document:\n",
    "                y_label.append(0)\n",
    "            else:\n",
    "                y_label.append(1)\n",
    "    return y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(x_train, y_train, x_test, y_test, candidates, labels):\n",
    "    seed = 7 #just randomly select the number\n",
    "    models = []\n",
    "    models.append(('LR', LogisticRegression()))\n",
    "    models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    models.append(('NB', GaussianNB()))\n",
    "    models.append(('SVM', SVC(probability=True)))\n",
    "    models.append(('RF', RF(n_estimators=10, max_depth=3)))\n",
    "    models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "    models.append(('Bagging', BaggingClassifier()))\n",
    "    models.append(('GradientBoosting', (GradientBoostingClassifier())))\n",
    "    models.append(('MLP', (MLPClassifier())))\n",
    "    models.append(('Multinomial', (MultinomialNB())))\n",
    "    results = []\n",
    "    names = []\n",
    "    scoring='accuracy'\n",
    "    #print(\"\\nAccuracy on testing data:\")\n",
    "    all_predict_proba=[]\n",
    "    for name, model in models:\n",
    "        #accuracy\n",
    "        #print(\"%s: %.3f\" % (name, accuracy_score(model.fit(x_train, y_train).predict(x_test), y_test)))\n",
    "        all_predict_proba.append(model.fit(x_train, y_train).predict_proba(x_test)[:,1])\n",
    "    \n",
    "    all_fmeasure=[]\n",
    "    for model in range(0, len(all_predict_proba)):\n",
    "        probability=[]\n",
    "        counter=0\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                doc.append((candidates[n_doc][n_cand][0], all_predict_proba[model][counter]))\n",
    "                counter+=1\n",
    "            probability.append(doc)\n",
    "        fmeasure=calculate_fmeasure(get_top_candidates(probability, 15), labels)\n",
    "        all_fmeasure.append((models[model][0], fmeasure))\n",
    "    return all_fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def create_pickle(data, name):\n",
    "    with open('%s.pickle' % name,'wb') as handle:\n",
    "        result=pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return result\n",
    "\n",
    "def open_pickle(name):\n",
    "    with open('%s.pickle' % name,'rb') as handle:\n",
    "        result=pickle.load(handle)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "####with machine learning\n",
    "##NGRAM\n",
    "#open all pickle\n",
    "train_raw=open_pickle('txt train raw')\n",
    "train_data=open_pickle('txt train data')\n",
    "train_label=open_pickle('txt train label')\n",
    "\n",
    "test_raw=open_pickle('txt test raw')\n",
    "test_data=open_pickle('txt test data')\n",
    "test_label=open_pickle('txt test label')\n",
    "\n",
    "ngram_candidates=open_pickle('txt ngram candidates')\n",
    "test_ngram_candidates=open_pickle('txt test ngram candidates')\n",
    "nounphrase_candidates=open_pickle('txt nounphrase candidates')\n",
    "test_nounphrase_candidates=open_pickle('txt test nounphrase candidates')\n",
    "\n",
    "\n",
    "ngram_x_train=create_example(train_raw, train_data, ngram_candidates, train_label)\n",
    "ngram_y_train=create_label(ngram_candidates, train_label)\n",
    "ngram_x_test=create_example(test_raw, test_data, test_ngram_candidates, test_label)\n",
    "ngram_y_test=create_label(test_ngram_candidates, test_label)\n",
    "\n",
    "nounphrase_x_train=create_example(train_raw, train_data, nounphrase_candidates, train_label)\n",
    "nounphrase_y_train=create_label(nounphrase_candidates, train_label)\n",
    "nounphrase_x_test=create_example(test_raw, test_data, test_nounphrase_candidates, test_label)\n",
    "nounphrase_y_test=create_label(test_nounphrase_candidates, test_label)\n",
    "\n",
    "\n",
    "print(\"F-measure with machine learning (testing)\")\n",
    "ngram_prediction=predict_data(ngram_x_train, ngram_y_train, ngram_x_test, ngram_y_test, test_ngram_candidates, test_label)\n",
    "print('F-measure on ngram', ngram_prediction)\n",
    "nounphrase_prediction=predict_data(nounphrase_x_train, nounphrase_y_train, nounphrase_x_test, nounphrase_y_test, test_nounphrase_candidates, test_label)\n",
    "print('F-measure on noun phrase', nounphrase_prediction)\n",
    "\n",
    "#print(len(x_train_ngram))#print(len(y_train_ngram))#print(len(x_test_ngram))#print(len(y_test_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidates_list=[['a','b','c','d','e','a1','b1','c1','d1','e1','a2','b2','c2','d2','e2'],\n",
    "                 ['a3','b3','c3','d3','e3','a31','b31','c31','d31','e31','a32','b32','c32','d32','e32'],\n",
    "                 ['a4','b4','c4','d4','e4','a41','b41','c41','d41','e41','a42','b42','c42','d42','e42']]\n",
    "\n",
    "gold_data=[['a1','b1','c1','d','e','a12','b12','c1','d12','e12','a22','b22'],\n",
    "                 ['a33','b33','c33','d33','e33','a313','b313','c313','a323','b32','c323','d323','e32'],\n",
    "                 ['a44','b44','c44','d44','e44','a441','d441','e441','a442','b442','c442','d442','e442']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing o compare tfidfvalue with one feature\n",
    "\n",
    "\n",
    "#feature phrase length\n",
    "#feature1=[]\n",
    "#for n_doc in range(len(tfidf)):\n",
    "#    doc=[]\n",
    "#    for n_feature in range(len(tfidf[n_doc])):\n",
    "#        doc.append(len(tfidf[n_doc][n_feature][0]))\n",
    "#    feature1.append(doc)\n",
    "#print(feature1)\n",
    "\n",
    "tfidf=[[('dog',1),('swimming',4),('car',7)],\n",
    "      [('air',11),('bowl',14),('cone',17),('done',17)],\n",
    "       [('air of water',21),('chocolate biscuit',24)],\n",
    "      [('air conditioner',21),('hot white chocolate',24)],]\n",
    "\n",
    "title=[[('dog',0),('rabbit',0),('snake',0),('car',0)],\n",
    "      [('bowl',0),('dog',0),('rabbit',0)],\n",
    "      [('chocolate biscuits',0),('a lot air of water',0),('rabbit',0),('snake',0)],\n",
    "      [('air conditioner',0),('hot white',0)]]\n",
    "\n",
    "#is_title, is_abstract, is etc, but extract section with ngram(1,5)\n",
    "feature2=[]\n",
    "for n_doc in range(len(tfidf)):\n",
    "    doc=[]\n",
    "    for n_feature in range(len(tfidf[n_doc])):\n",
    "        #title_feature=[feature for feature in title[n_doc]]\n",
    "        title_feature=[feature for feature, value in title[n_doc]]\n",
    "        if tfidf[n_doc][n_feature][0] not in title_feature:\n",
    "            doc.append(0)\n",
    "        else:\n",
    "            doc.append(1)\n",
    "    feature2.append(doc)\n",
    "print(feature2)\n",
    "\n",
    "#is_abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''If need cross validation per model\n",
    "###measure accuracy with k-fold\n",
    "print(\"Accuracy on training data with Cross-validation:\")\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, x_train_ngram, y_train_ngram, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %.3f (%.3f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates=[[('aa',1),('a',5),('a3',5),('a6',7)],\n",
    "            [('aq',3),('aw',4),('ag',2),('ar',8)]]\n",
    "\n",
    "feature1=[[3,4,5,6],\n",
    "            [7,9,6,5]]\n",
    "feature2=[[1,2,7,8],\n",
    "            [9,90,4,3]]\n",
    "\n",
    "for n_doc in range(len(candidates)):\n",
    "    for n_candidate in range(len(candidates[n_doc])):\n",
    "        candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature1[n_doc][n_candidate],)\n",
    "        candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate],)\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
