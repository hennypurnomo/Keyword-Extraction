{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##basic topic rank\n",
    "#generate per document\n",
    "from pytopicrank import TopicRank\n",
    "import utils\n",
    "import preprocessing\n",
    "from datetime import datetime\n",
    "import keyphrase_extraction\n",
    "from nltk.stem.porter import *\n",
    "from itertools import chain\n",
    "import re\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "corpus = utils.open_pickle('xml train data')\n",
    "label_data = utils.open_pickle('train label')\n",
    "start = datetime.now()\n",
    "\n",
    "def topic_rank(corpus):\n",
    "    all_topics = []\n",
    "    for n_doc in corpus:\n",
    "        all_topics.append(TopicRank(n_doc).get_top_n(n = 15))\n",
    "    return all_topics\n",
    "\n",
    "topic_rank_cluster = topic_rank(corpus[:1])\n",
    "print(topic_rank_cluster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "t=[12,3,4,64]\n",
    "t=utils.create_pickle(t, './pickle/test')\n",
    "print(\"done\")\n",
    "tt=utils.open_pickle('./pickle/test')\n",
    "print(tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##feature topic rank\n",
    "#topic_rank_keyphrase = [list(chain.from_iterable(n_doc[:10])) for n_doc in topic_rank_cluster] # number depends with n_keyphrase\n",
    "#print(topic_rank_keyphrase)\n",
    "\n",
    "candidates = [[('banana',2), ('banana registri',3),('entri registri',2), ('multipl registri',3), ('registri',2), \n",
    "               ('properti servic',1), ('discov servic',4), ('servic',3)]]\n",
    "\n",
    "def feature_topic_rank(candidates, topic_rank, n_keyphrase):\n",
    "    feature = []\n",
    "    topic_rank_keyphrase = [list(chain.from_iterable(n_doc[:n_keyphrase])) for n_doc in topic_rank] \n",
    "    for n_doc in range(len(candidates)):\n",
    "        doc = []\n",
    "        matches = []\n",
    "        for n_cand, value in candidates[n_doc]:\n",
    "            for topic in topic_rank_keyphrase[n_doc]:\n",
    "                if re.findall(r'.*'+topic+'.*', n_cand):\n",
    "                    matches.append(n_cand)\n",
    "            if n_cand in matches:\n",
    "                doc.append(1)\n",
    "            else:\n",
    "                doc.append(0)\n",
    "        feature.append(doc)\n",
    "    return feature\n",
    "\n",
    "print(feature_topic_rank(candidates, topic_rank_cluster, 15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#generate per document\n",
    "from pytopicrank import TopicRank\n",
    "import utils\n",
    "import preprocessing\n",
    "from datetime import datetime\n",
    "import keyphrase_extraction\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "train_data = utils.open_pickle('xml test data')\n",
    "label_data = utils.open_pickle('test label')\n",
    "start = datetime.now()\n",
    "candidates=[]\n",
    "for n_doc in train_data:\n",
    "    #candidates.append([stemmer.stem(word) for word in TopicRank(n_doc).get_top_n(n = 15)])\n",
    "    candidates.append([' '.join(list(stemmer.stem(word) for word in candidate.split(\" \"))) for candidate in TopicRank(n_doc).get_top_n(n = 10)])\n",
    "#print(candidates)\n",
    "print(\"time=\", datetime.now() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmeasure = keyphrase_extraction.calculate_fmeasure(candidates, label_data)\n",
    "print(fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate per document\n",
    "from pytopicrank import TopicRank\n",
    "import utils\n",
    "import preprocessing\n",
    "from datetime import datetime\n",
    "import keyphrase_extraction\n",
    "from nltk.stem.porter import *\n",
    "from itertools import chain\n",
    "import re\n",
    "\n",
    "stemmer=PorterStemmer()\n",
    "\n",
    "#corpus = utils.open_pickle('txt test data')\n",
    "#train_data = utils.open_pickle('xml test data')\n",
    "label = utils.open_pickle('test label')\n",
    "start = datetime.now()\n",
    "\n",
    "topic=[['table','zoo','machine learning'], ['fried', 'chicken katsu']]\n",
    "candidates=[['chocolate', 'chicken nugget', 'table','zoo','machine learning', 'dinner table', 'colchester zoo'],\n",
    "            ['katsu', 'domino', 'fried', 'chicken katsu', 'fried frice']]\n",
    "\n",
    "#def topic_rank(corpus, candidates):\n",
    "def topic_rank(candidates):\n",
    "    #all_topics = []\n",
    "    #for n_doc in corpus:\n",
    "    #    all_topics.append([' '.join(list(stemmer.stem(word) for word in candidate.split(\" \"))) \n",
    "    #                   for candidate in list(chain.from_iterable(TopicRank(n_doc).get_top_n(n = 15)))])\n",
    "    all_topics=[['table','zoo','machine learning'], ['fried', 'chicken katsu']] \n",
    "    feature=[]\n",
    "    for n_doc in range(len(candidates)):\n",
    "        doc=[]\n",
    "        matches=[]\n",
    "        topics=all_topics[n_doc]\n",
    "        for n_cand in candidates[n_doc]:\n",
    "            for topic in topics:\n",
    "                if re.findall(r'.*'+topic+'.*', n_cand):\n",
    "                    matches.append(n_cand)\n",
    "            if n_cand in matches:\n",
    "                doc.append(1)\n",
    "            else:\n",
    "                doc.append(0)\n",
    "        feature.append(doc)\n",
    "    return feature\n",
    "print(topic_rank(candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#match value from topic rank into candidates\n",
    "import re\n",
    "\n",
    "topic=[['table','zoo','machine learning'], ['fried', 'chicken katsu']]\n",
    "candidates=[['chocolate', 'chicken nugget', 'table','zoo','machine learning', 'dinner table', 'colchester zoo'],\n",
    "            ['katsu', 'domino', 'fried', 'chicken katsu', 'fried frice']]\n",
    "al=[]\n",
    "for n_doc in range(len(candidates)):\n",
    "    doc=[]\n",
    "    matches=[]\n",
    "    topics=topic[n_doc]\n",
    "    for n_cand in candidates[n_doc]:\n",
    "        for top in topics:\n",
    "            if re.findall(r'.*'+top+'.*', n_cand):\n",
    "                matches.append(n_cand)\n",
    "        if n_cand in matches:\n",
    "            doc.append(1)\n",
    "        else:\n",
    "            doc.append(0)\n",
    "    al.append(doc)\n",
    "    \n",
    "print(al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[['table', 'zoo', 'machine learning', 'dinner table', 'colchester zoo'],\n",
    " ['fried', 'chicken katsu', 'fried frice']]\n",
    "'''\n",
    "al=[]\n",
    "for n_doc in range(len(candidates)):\n",
    "    doc=[]\n",
    "    for n_cand in candidates[n_doc]:\n",
    "        if n_cand in topic[n_doc]:\n",
    "            doc.append(1)\n",
    "        else:\n",
    "            doc.append(0)\n",
    "    al.append(doc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=[[1,2,3],[2,13,4],[5,6,17]]\n",
    "t2=[[11,2,3],[2,31,4],[15,6,7]]\n",
    "t3=[[11,2,3],[2,311,4],[15,6,7]]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
