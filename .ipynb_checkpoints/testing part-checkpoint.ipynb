{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, re, string, itertools\n",
    "#mingw_path = 'C:\\mingw-w64\\x86_64-8.1.0-win32-seh-rt_v6-rev0\\mingw64\\bin'\n",
    "#os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n",
    "import xml.etree.ElementTree as et\n",
    "import pickle, json\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "from nltk.stem.porter import *\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents, sent_tokenize\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "from nltk.chunk import tree2conlltags\n",
    "from pandas import DataFrame\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "import numpy as np\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn import svm\n",
    "#from xgboost import XGBClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Henny love machine learning', 'machine learning is the best'], ['this love is never stop', 'too long to be true'], ['high performance computing is one of course in computer science, it was taught by adrian clark', 'high performance computing is one of course in computer science']]\n",
      "[[1, 1, 0], [1, 0, 0, 1], [1, 1, 0]]\n"
     ]
    }
   ],
   "source": [
    "candidates=[['Henny love machine learning','machine learning is the best', 'hey, machine'],\n",
    "           ['this love is never stop','love', 'never stop','too long to be true'],\n",
    "           ['high performance computing is one of course in computer science',\n",
    "           'high performance computing is one of course in computer science, it was taught by adrian clark',\n",
    "           'high performance']]\n",
    "labels=[['love machine learning','the best'],\n",
    "        ['love is never stop','too long'],\n",
    "        ['high performance computing is one of course in computer science, it was taught by adrian clark',\n",
    "         'high performance in computer science']] #example 5 sentences\n",
    "\n",
    "matched_sentences = []\n",
    "for n_doc in range(len(candidates)):\n",
    "    doc=[]\n",
    "    for n_label in labels[n_doc]:\n",
    "        label = []\n",
    "        for n_sent in candidates[n_doc]:\n",
    "            n_match = len(set([x for x in n_sent.split(\" \")]).intersection([x for x in n_label.split(\" \")]))\n",
    "            label.append((n_sent, n_match))\n",
    "            order = max(label, key = lambda x:x[1])\n",
    "        doc.append(order[0])\n",
    "    matched_sentences.append(doc)\n",
    "print(matched_sentences)\n",
    "\n",
    "\n",
    "#give label\n",
    "all_labels=[]\n",
    "for n_doc in range(len(candidates)):\n",
    "    doc=[]\n",
    "    for n_sent in candidates[n_doc]:\n",
    "        if n_sent in matched_sentences[n_doc]:\n",
    "            doc.append(1)\n",
    "        else:\n",
    "            doc.append(0)\n",
    "    all_labels.append(doc)\n",
    "print(all_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_sentences = []\n",
    "for n_label in labels:\n",
    "    label = []\n",
    "    for n_sent in candidates:\n",
    "        n_match = len(set([x for x in n_sent.split(\" \")]).intersection([x for x in n_label.split(\" \")]))\n",
    "        label.append((n_sent, n_match))\n",
    "        order = max(label, key = lambda x:x[1])\n",
    "    matched_sentences.append(order[0])\n",
    "print(matched_sentences)\n",
    "\n",
    "#give label\n",
    "sent=[]\n",
    "for n_sent in candidates:\n",
    "    if n_sent in matched_sentences:\n",
    "        sent.append(1)\n",
    "    else:\n",
    "        sent.append(0)\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def create_pickle(data, name):\n",
    "    with open('%s.pickle' % name,'wb') as handle:\n",
    "        result=pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return result\n",
    "\n",
    "def open_pickle(name):\n",
    "    with open('%s.pickle' % name,'rb') as handle:\n",
    "        result=pickle.load(handle)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in xml version 's as one token\n",
    "def clean_xml(input_list): \n",
    "    result=[]\n",
    "    #remove unwanted character per line\n",
    "    for line in input_list:\n",
    "        clean=re.sub(\"(-LSB-|-LRB-|-LCB-)\",'(', line) #remove brackets\n",
    "        clean=re.sub(\"(-RSB-|-RRB-|-RCB-)\",')', clean) #remove brackets\n",
    "        clean=re.sub(\"('s)\",'', clean) #remove 's\n",
    "        clean=re.sub(\"\\[([0-9]{1,2}\\,?\\s?)+\\]\",'', clean) #remove [2]\n",
    "        clean=re.sub(\"\\(([0-9]{1,2}\\,?\\s?)+\\)\",'', clean) #remove (2)\n",
    "        clean=re.sub(r\"\\b(iv|ix|x|v?i{0,3})+\\b\",'', clean) #remove roman number\n",
    "        clean=re.sub(\"([Ff]ig.|[Ff]igures?|[Tt]ab.|[Tt]ables?)\\s?\\d{1,2}\",'', clean) #remove fig. 2 etc\n",
    "        clean=re.sub(r\"\\b(i.e.g.|e.g.|i.e.)\",'', clean) #remove i.e.g., i.e., e.g.\n",
    "        clean=re.sub(r\"\\b((https?://|www.)[^\\s]+)\",'', clean) #remove email\n",
    "        result.append(clean)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_xml(path):\n",
    "    all_files=[]\n",
    "    for file in path:\n",
    "        tree=et.parse(file)\n",
    "\n",
    "        #create a list that contain a dictionary for label per document\n",
    "        dict_doc={'doc_id': None, 'title': None, 'abstract': None, \n",
    "                 'introduction': None, 'method': None, \n",
    "                 'evaluation': None, 'related work': None, \n",
    "                 'conclusions': None, 'full_text': None, 'glabels': None}\n",
    "        \n",
    "        file_id=os.path.basename(file.rstrip('.xml'))\n",
    "        dict_doc['doc_id']=file_id\n",
    "        \n",
    "        title=[]\n",
    "        abstract=[]\n",
    "        introduction=[]\n",
    "        method=[]\n",
    "        evaluation=[]\n",
    "        related_work=[]\n",
    "        conclusions=[]\n",
    "        unknown=[]\n",
    "        full=[]\n",
    "        #loop for every sentence\n",
    "        for sentence in tree.iterfind('./document/sentences/sentence'):\n",
    "            #create dictionary\n",
    "            if sentence.attrib['section']=='title' and sentence.attrib['type']!='sectionHeader':\n",
    "                title.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "            elif sentence.attrib['section']=='abstract' and sentence.attrib['type']!='sectionHeader':\n",
    "                abstract.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "            elif sentence.attrib['section']=='introduction' and sentence.attrib['type']!='sectionHeader':\n",
    "                introduction.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "            elif sentence.attrib['section']=='method' and sentence.attrib['type']!='sectionHeader':\n",
    "                method.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "            elif sentence.attrib['section']=='evaluation' and sentence.attrib['type']!='sectionHeader':\n",
    "                evaluation.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "            elif sentence.attrib['section']=='related work' and sentence.attrib['type']!='sectionHeader':\n",
    "                related_work.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "            elif sentence.attrib['section']=='conclusions' and sentence.attrib['type']!='sectionHeader':\n",
    "                conclusions.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "            else:\n",
    "                unknown.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "        #still on list, must be convert to \n",
    "        \n",
    "        dict_doc['title']=' '.join(title)\n",
    "        dict_doc['abstract']=' '.join(clean_xml(abstract))\n",
    "        dict_doc['introduction']=' '.join(clean_xml(introduction))\n",
    "        dict_doc['method']=' '.join(clean_xml(method))\n",
    "        dict_doc['evaluation']=' '.join(clean_xml(evaluation))\n",
    "        dict_doc['related work']=' '.join(clean_xml(related_work))\n",
    "        dict_doc['conclusions']=' '.join(clean_xml(conclusions))\n",
    "        dict_doc['unknown']=' '.join(clean_xml(unknown))\n",
    "        full=title+abstract+introduction+method+evaluation+related_work+conclusions\n",
    "        dict_doc['full_text']=' '.join(clean_xml(full))\n",
    "        \n",
    "        all_files.append(dict_doc)\n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_xml_corpus(raw_data):\n",
    "    train_data=[]\n",
    "    for doc in raw_data:\n",
    "        train_data.append(doc['full_text'])\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ngram_tfidf(corpus):\n",
    "    \n",
    "    #porter stemmer\n",
    "    stemmer=PorterStemmer()\n",
    "\n",
    "    #eliminate ngram which starts or ends from stopwords\n",
    "    #from https://stackoverflow.com/questions/49746555/sklearn-tfidfvectorizer-generate-custom\n",
    "    #-ngrams-by-not-removing-stopword-in-the/49775000#49775000\n",
    "    class NewTfidfVectorizer(TfidfVectorizer):\n",
    "        def _word_ngrams(self, tokens, stop_words=None):\n",
    "            # First get tokens without stop words\n",
    "            tokens = super(TfidfVectorizer, self)._word_ngrams(tokens, None)\n",
    "            if stop_words is not None:\n",
    "                new_tokens=[]\n",
    "                for token in tokens:\n",
    "                    split_words = token.split(' ')\n",
    "                    # Only check the first and last word for stop words\n",
    "                    if len(token)>2 and split_words[0] not in stop_words and split_words[-1] not in stop_words:\n",
    "                        #stem every word in token\n",
    "                        if len(split_words)==1 and len(split_words[0])>2:\n",
    "                            new_tokens.append(stemmer.stem(token))\n",
    "                        elif len(split_words)==2 and split_words[-1]==\"'\":\n",
    "                            del(token)\n",
    "                        elif len(split_words[0])<3 and len(split_words[1])<3:\n",
    "                            del(token)\n",
    "                        elif split_words[1]==\"'\" and split_words[2]==\"s\":\n",
    "                            new_tokens.append(stemmer.stem(split_words[0])+split_words[1]+split_words[2])\n",
    "                        else:\n",
    "                            new_tokens.append(' '.join(list(stemmer.stem(word) for word in word_tokenize(token))))\n",
    "                return new_tokens\n",
    "            return tokens\n",
    "    \n",
    "    stop_words=text.ENGLISH_STOP_WORDS\n",
    "    \n",
    "    #pattern=\"(?u)\\\\b[\\\\w-]+\\\\b\"\n",
    "    tfidf=NewTfidfVectorizer(ngram_range=(1,5), stop_words=stop_words,\n",
    "                                token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    \n",
    "    matrix=tfidf.fit_transform(corpus)\n",
    "    feature_names=tfidf.get_feature_names()\n",
    "\n",
    "    #how to print tf-idf from https://stackoverflow.com/questions/34449127/\n",
    "    #sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-documen\n",
    "    candidates=[]\n",
    "    for doc in range(0,len(corpus)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        tfidf_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_tfidf=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in tfidf_doc]]\n",
    "        candidates.append(names_tfidf)\n",
    "    \n",
    "    #this is the candidates per document\n",
    "    #vocab_perdoc=tfidf.inverse_transform(matrix)\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate noun phrases based on corpus\n",
    "def create_phrase_vocabulary(raw_data):\n",
    "     \n",
    "    #porter stemmer\n",
    "    stemmer=PorterStemmer()\n",
    "    \n",
    "    #from http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\n",
    "    #grammar=r'NP: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}' #only detect noun phrases that contain specific pattern, hypen word is counted as one NN\n",
    "    \n",
    "    grammar=r'NP: {(<JJ.*>* <VBN>? <NN.*>+ <IN>)? <JJ.*>* <VBG>? <NN.*>+}' \n",
    "    \n",
    "    punct = set(string.punctuation) #list of punctuation\n",
    "    chunker = RegexpParser(grammar) #chunker from nltk\n",
    "    \n",
    "    def lambda_unpack(f):\n",
    "        return lambda args:f(*args)\n",
    "    \n",
    "    postag_sents = pos_tag_sents(word_tokenize(sent) for sent in raw_data) #tokenize and create pos tag per sentence\n",
    "    #list of IOB of noun phrases based on the specific grammar\n",
    "    noun_phrases = list(chain.from_iterable(tree2conlltags(chunker.parse(tagged_sent)) for tagged_sent in postag_sents)) \n",
    "    \n",
    "    #join B-NP and I-NP tags as one noun phrase excluding O tags    \n",
    "    merged_nounphrase = [' '.join(stemmer.stem(word) for word, pos, chunk in group).lower() for key, group in\n",
    "                    itertools.groupby(noun_phrases, lambda_unpack(lambda word, pos, chunk: chunk != 'O')) if key]\n",
    "    \n",
    "    #filter noun phrases from stopwords and punctuation\n",
    "    all_nounphrases=[cand for cand in merged_nounphrase\n",
    "            if len(cand)>2 and not all(char in punct for char in cand)]\n",
    "    \n",
    "    #select distinct noun phrases\n",
    "    vocabulary=(list(set(all_nounphrases)))\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nounphrase_tfidf(corpus, voc):\n",
    "    \n",
    "    stemmer=PorterStemmer()\n",
    "    \n",
    "    class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "        def build_tokenizer(self):\n",
    "            tokenizer=super(TfidfVectorizer, self).build_tokenizer()\n",
    "            return lambda doc: (stemmer.stem(token) for token in tokenizer(doc) if token not in stop_words)\n",
    "\n",
    "    stop_words=set(text.ENGLISH_STOP_WORDS)\n",
    "    s=['of','in','on','for']\n",
    "    stop_words=stop_words.difference(s)\n",
    "    tfidf=StemmedTfidfVectorizer(ngram_range=(1,5), stop_words=stop_words, vocabulary=voc, token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    \n",
    "    matrix=tfidf.fit_transform(corpus)\n",
    "    feature_names=tfidf.get_feature_names()\n",
    "\n",
    "    #how to print tf-idf from https://stackoverflow.com/questions/34449127/\n",
    "    #sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-documen\n",
    "    candidates=[]\n",
    "    for doc in range(0,len(corpus)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        tfidf_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_tfidf=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in tfidf_doc]]\n",
    "        candidates.append(names_tfidf)\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###sorting candidates based on 15 keywords\n",
    "def get_top_candidates(candidates_list, number_keyphrases):\n",
    "    best_candidates=[]\n",
    "    for doc in candidates_list:\n",
    "        #sort candidates by tf-idf value\n",
    "        sorted_candidates=sorted(doc, key=lambda x: x[1], reverse=True)[:number_keyphrases]\n",
    "        #best_candidates.append(sorted_candidates)\n",
    "        best_candidates.append([x for x,_ in sorted_candidates])\n",
    "        #remove overlapping keywords\n",
    "    return best_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###compare candidates to goldstandard\n",
    "def extract_goldkeyphrase(gold_data):\n",
    "    \n",
    "    r_plus=re.compile(\"^.*\\+.*$\")\n",
    "    r_slash=re.compile(\"^.*\\s.*\\/.*$\")\n",
    "    \n",
    "    gold_standard=[]\n",
    "    for line in gold_data.split('\\n'):\n",
    "        doc=[]      \n",
    "        for key in line[6:].split(','):\n",
    "            if key[0]==' ':\n",
    "                doc.append(key[1:])\n",
    "            elif re.search(r_plus, key):\n",
    "                split=[]\n",
    "                for element in key.split('+'):\n",
    "                    doc.append(element)\n",
    "            elif re.search(r_slash, key):\n",
    "                split=[]\n",
    "                for element in key.split('/'):\n",
    "                    doc.append(element)\n",
    "            else:\n",
    "                doc.append(key)\n",
    "        gold_standard.append(doc)\n",
    "    return gold_standard\n",
    "\n",
    "def calculate_fmeasure(candidates_list, gold_data):\n",
    "    #true positive\n",
    "    all_matches=[]\n",
    "    for index in range(len(candidates_list)):\n",
    "        #store all measure per document in dic\n",
    "        value={'tp': None, 'fp': None, 'fn': None, 'gold': None}\n",
    "        value['gold']=len(gold_data[index])\n",
    "        #counter true positive per document\n",
    "        true_positive=0\n",
    "        #loop between elements\n",
    "        for element_candidate in candidates_list[index]:                    \n",
    "            for element_goldkeyphrase in gold_data[index]:\n",
    "                #matched predicted keyword in gold keyphrase\n",
    "                if element_candidate==element_goldkeyphrase:\n",
    "                    #matches_perdoc.append(element_candidate)\n",
    "                    true_positive+=1\n",
    "            #if need the detail of evaluation\n",
    "            value['tp']=int(true_positive) #matched pair\n",
    "            value['fp']=int(15-true_positive) #depend how many keyword should we use\n",
    "            value['fn']=int(value['gold']-value['tp'])\n",
    "        #return all metrics per document\n",
    "        all_matches.append(value)\n",
    "\n",
    "    true_positive=sum(doc['tp'] for doc in all_matches)\n",
    "    false_positive=sum(doc['fp'] for doc in all_matches)\n",
    "    false_negative=sum(doc['fn'] for doc in all_matches)\n",
    "    \n",
    "    #matched/total top n\n",
    "    precision=float(true_positive/(false_positive+true_positive))\n",
    "    #matched/total gold standard\n",
    "    recall=float(true_positive/(false_negative+true_positive))\n",
    "    # calculate with micro averagedprecision\n",
    "    #f_measure=2*(precision*recall)/(precision+recall)\n",
    "    f_measure=float(\"{0:.2F}\".format(2*(precision*recall)/(precision+recall)*100))\n",
    "    return f_measure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_frequency(section):\n",
    "     #porter stemmer\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    stemmer=PorterStemmer()\n",
    "    \n",
    "    #eliminate ngram which starts or ends from stopwords\n",
    "    class NewCountVectorizer(CountVectorizer):\n",
    "        def _word_ngrams(self, tokens, stop_words=None):\n",
    "            # First get tokens without stop words\n",
    "            tokens = super(CountVectorizer, self)._word_ngrams(tokens, None)\n",
    "            if stop_words is not None:\n",
    "                new_tokens=[]\n",
    "                for token in tokens:\n",
    "                    split_words = token.split(' ')\n",
    "                    # Only check the first and last word for stop words\n",
    "                    if len(token)>2 and split_words[0] not in stop_words and split_words[-1] not in stop_words:\n",
    "                        #stem every word in token\n",
    "                        if len(split_words)==1 and len(split_words[0])>2:\n",
    "                            new_tokens.append(stemmer.stem(token))\n",
    "                        elif len(split_words)==2 and split_words[-1]==\"'\":\n",
    "                            del(token)\n",
    "                        elif len(split_words[0])<3 and len(split_words[1])<3:\n",
    "                            del(token)\n",
    "                        elif split_words[1]==\"'\" and split_words[2]==\"s\":\n",
    "                            new_tokens.append(stemmer.stem(split_words[0])+split_words[1]+split_words[2])\n",
    "                        else:\n",
    "                            new_tokens.append(' '.join(list(stemmer.stem(word) for word in word_tokenize(token))))\n",
    "                return new_tokens\n",
    "            return tokens\n",
    "    \n",
    "    stop_words=text.ENGLISH_STOP_WORDS\n",
    "    \n",
    "    count_vect=NewCountVectorizer(ngram_range=(1,5), stop_words=stop_words,\n",
    "                                token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    \n",
    "    matrix=count_vect.fit_transform(section)\n",
    "    feature_names=count_vect.get_feature_names()\n",
    "\n",
    "    #how to print tf-idf from https://stackoverflow.com/questions/34449127/\n",
    "    #sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-document\n",
    "    ngrams=[]\n",
    "    for doc in range(0,len(section)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        count_vect_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_count_vect=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in count_vect_doc]]\n",
    "        ngrams.append(names_count_vect)\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tf_nounphrase(corpus, voc):\n",
    "    \n",
    "    stemmer=PorterStemmer()\n",
    "    \n",
    "    class NewCountVectorizer(CountVectorizer):\n",
    "        def build_tokenizer(self):\n",
    "            tokenizer=super(CountVectorizer, self).build_tokenizer()\n",
    "            return lambda doc: (stemmer.stem(token) for token in tokenizer(doc) if token not in stop_words)\n",
    "\n",
    "    stop_words=set(text.ENGLISH_STOP_WORDS)\n",
    "    s=['of','in','on','for']\n",
    "    stop_words=stop_words.difference(s)\n",
    "    tfidf=NewCountVectorizer(ngram_range=(1,5), stop_words=stop_words, vocabulary=voc, token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    \n",
    "    matrix=tfidf.fit_transform(corpus)\n",
    "    feature_names=tfidf.get_feature_names()\n",
    "\n",
    "    #how to print tf-idf from https://stackoverflow.com/questions/34449127/\n",
    "    #sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-documen\n",
    "    candidates=[]\n",
    "    for doc in range(0,len(corpus)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        tfidf_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_tfidf=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in tfidf_doc]]\n",
    "        candidates.append(names_tfidf)\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#load directory of training data to feed to scikit learn\n",
    "train_directory=glob.glob('./se_xml/train/*.xml')\n",
    "train_raw=load_xml(train_directory)\n",
    "pickle_train_raw=create_pickle(train_raw,'xml train raw')\n",
    "train_data=create_xml_corpus(train_raw)\n",
    "pickle_train_data=create_pickle(train_data,'xml train data')\n",
    "train_tf_corpus=calculate_term_frequency(train_data)\n",
    "pickle_train_tf_corpus=create_pickle(train_tf_corpus,'xml train tf corpus')\n",
    "\n",
    "#label data\n",
    "train_label_directory=open('./se_txt/train/train.combined.stem.final', encoding='utf-8').read()\n",
    "train_label=extract_goldkeyphrase(train_label_directory)\n",
    "pickle_train_label=create_pickle(train_label, 'xml train label')\n",
    "\n",
    "###load testing data\n",
    "test_directory=glob.glob('./se_xml/test/*.xml')\n",
    "test_raw=load_xml(test_directory)\n",
    "pickle_test_raw=create_pickle(test_raw,'xml test raw')\n",
    "test_data=create_xml_corpus(test_raw)\n",
    "pickle_test_data=create_pickle(test_data,'xml test data')\n",
    "test_tf_corpus=calculate_term_frequency(test_data)\n",
    "pickle_test_tf_corpus=create_pickle(test_tf_corpus,'xml test tf corpus')\n",
    "\n",
    "#label\n",
    "test_label_directory=open('./se_txt/test_answer/test.combined.stem.final', encoding='utf-8').read()\n",
    "test_label=extract_goldkeyphrase(test_label_directory)\n",
    "pickle_test_label=create_pickle(test_label, 'xml test label')\n",
    "\n",
    "\n",
    "###Ngram version\n",
    "print(\"N-gram TF-IDF version\")\n",
    "ngram_candidates=calculate_ngram_tfidf(train_data) \n",
    "pickle_ngram_candidates=create_pickle(ngram_candidates, 'xml ngram candidates')\n",
    "#ngram_top_keyphrases=get_top_candidates(ngram_candidates, 15)\n",
    "#ngram_fmeasure=calculate_fmeasure(ngram_top_keyphrases, train_label)\n",
    "#print(\"F-measure on training:\", ngram_fmeasure)\n",
    "\n",
    "test_ngram_candidates=calculate_ngram_tfidf(test_data) \n",
    "pickle_test_ngram_candidates=create_pickle(test_ngram_candidates, 'xml test ngram candidates')\n",
    "#test_ngram_top_candidates=get_top_candidates(test_ngram_candidates, 15)\n",
    "#test_ngram_fmeasure=calculate_fmeasure(test_ngram_top_candidates, test_label)\n",
    "#print(\"F-measure on testing:\", test_ngram_fmeasure)\n",
    "\n",
    "\n",
    "#### Noun phrase version\n",
    "print(\"Noun phrase TF-IDF version\")\n",
    "nounphrase_vocabulary=create_phrase_vocabulary(train_data)\n",
    "train_tf_nounphrase_corpus=calculate_tf_nounphrase(train_data, nounphrase_vocabulary)\n",
    "pickle_train_tf_nounphrase_corpus=create_pickle(train_tf_nounphrase_corpus,'xml train tf nounphrase corpus')\n",
    "nounphrase_candidates=calculate_nounphrase_tfidf(train_data, nounphrase_vocabulary)\n",
    "pickle_nounphrase_candidates=create_pickle(nounphrase_candidates, 'xml nounphrase candidates')\n",
    "#nounphrase_top_keyphrases=get_top_candidates(nounphrase_candidates, 15)\n",
    "#nounphrase_fmeasure=calculate_fmeasure(nounphrase_top_keyphrases, train_label)\n",
    "#print(\"F-measure on training:\", nounphrase_fmeasure)\n",
    "\n",
    "test_nounphrase_vocabulary=create_phrase_vocabulary(test_data)\n",
    "test_tf_nounphrase_corpus=calculate_tf_nounphrase(test_data, test_nounphrase_vocabulary)\n",
    "pickle_test_tf_nounphrase_corpus=create_pickle(test_tf_nounphrase_corpus,'xml test tf nounphrase corpus')\n",
    "test_nounphrase_candidates=calculate_nounphrase_tfidf(test_data, test_nounphrase_vocabulary)\n",
    "pickle_test_nounphrase_candidates=create_pickle(test_nounphrase_candidates, 'xml test nounphrase candidates')\n",
    "#test_nounphrase_top_candidates=get_top_candidates(test_nounphrase_candidates, 15)\n",
    "#test_nounphrase_fmeasure=calculate_fmeasure(test_nounphrase_top_candidates, test_label)\n",
    "#print(\"F-measure on testing:\", test_nounphrase_fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus):\n",
    "    clean=[]\n",
    "    stemmer=PorterStemmer()\n",
    "    for doc in corpus:\n",
    "        cleaned_words=\" \".join([word for word in word_tokenize(doc.lower()) if re.search(r\"\\b[A-Za-z-]+\\b\", word) and len(word)>2])\n",
    "        stemmed_words=[stemmer.stem(word) for word in cleaned_words.split()]\n",
    "        clean.append(\" \".join([word for word in stemmed_words]))\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(raw_data, corpus, candidates, label, tf_corpus):\n",
    "    \n",
    "    #binary_title, frequency_title, binary_abstract, frequency_abstract, binary_introduction, frequency_introduction\n",
    "    def feature_structure(candidates, raw_data):\n",
    "        title_raw=[doc['title'] for doc in raw_data]\n",
    "        #xml version doesnt need ' '.join\n",
    "        abstract_raw=[doc['abstract'] for doc in raw_data]\n",
    "        introduction_raw=[doc['introduction'] for doc in raw_data]  \n",
    "        title=calculate_term_frequency(title_raw)\n",
    "        abstract=calculate_term_frequency(abstract_raw)\n",
    "        introduction=calculate_term_frequency(introduction_raw) \n",
    "        feature=[]\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                title_perdoc=[feature for (feature, value) in title[n_doc]]\n",
    "                abstract_perdoc=[feature for (feature, value) in abstract[n_doc]]\n",
    "                introduction_perdoc=[feature for (feature, value) in introduction[n_doc]]\n",
    "                if candidates[n_doc][n_cand][0] in title_perdoc:\n",
    "                    binary_title=1\n",
    "                    value=[value for (feature, value) in title[n_doc] if feature in candidates[n_doc][n_cand][0]]\n",
    "                    frequency_title=value[0]\n",
    "                else:\n",
    "                    binary_title=0\n",
    "                    frequency_title=0\n",
    "                if candidates[n_doc][n_cand][0] in abstract_perdoc:\n",
    "                    binary_abstract=1\n",
    "                    value=[value for (feature, value) in abstract[n_doc] if feature in candidates[n_doc][n_cand][0]]\n",
    "                    frequency_abstract=value[0]\n",
    "                else:\n",
    "                    binary_abstract=0\n",
    "                    frequency_abstract=0\n",
    "                if candidates[n_doc][n_cand][0] in introduction_perdoc:\n",
    "                    binary_introduction=1\n",
    "                    value=[value for (feature, value) in introduction[n_doc] if feature in candidates[n_doc][n_cand][0]]\n",
    "                    frequency_introduction=value[0]\n",
    "                else:\n",
    "                    binary_introduction=0\n",
    "                    frequency_introduction=0\n",
    "                doc.append(((binary_title, frequency_title, binary_abstract, frequency_abstract, binary_introduction, frequency_introduction)))\n",
    "            feature.append(doc)\n",
    "        return feature\n",
    "    \n",
    "    def feature_candidate_length(candidates):\n",
    "        feature=[]\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_feature in range(len(candidates[n_doc])):\n",
    "                doc.append(len(candidates[n_doc][n_feature][0]))\n",
    "            feature.append(doc)\n",
    "        return feature\n",
    "        \n",
    "    def feature_frequency(label, tf_corpus):\n",
    "        merged_labels=list(chain.from_iterable(label))\n",
    "        feature=[]\n",
    "        for n_doc in range(len(tf_corpus)):\n",
    "            doc=[]\n",
    "            for n_cand in range(len(tf_corpus[n_doc])):\n",
    "                cand_freq=tf_corpus[n_doc][n_cand][1]\n",
    "                if tf_corpus[n_doc][n_cand][0] not in merged_labels:\n",
    "                    supervised=0\n",
    "                else:\n",
    "                    supervised=tf_corpus[n_doc][n_cand][1]\n",
    "                doc.append(((cand_freq, supervised)))\n",
    "            feature.append(doc)\n",
    "        return feature\n",
    "    \n",
    "    #create first, last occurence, distance from first occurence, spread from first and last occurence\n",
    "    def feature_occurence(candidates, corpus):\n",
    "        feature=[]\n",
    "        cleaned_corpus=clean_corpus(corpus)\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            corpus_words=len(cleaned_corpus[n_doc].split(\" \"))\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                first_index=cleaned_corpus[n_doc].lower().find(candidates[n_doc][n_cand][0])\n",
    "                last_index=cleaned_corpus[n_doc].lower().rfind(candidates[n_doc][n_cand][0])\n",
    "                preceding_words=len(cleaned_corpus[n_doc][:first_index].split(\" \"))-1\n",
    "                following_words=len(cleaned_corpus[n_doc][:last_index].split(\" \"))-1\n",
    "                distance=float(\"{0:.2F}\".format(preceding_words/corpus_words))\n",
    "                spread=len(cleaned_corpus[n_doc][first_index:last_index].split(\" \"))-1\n",
    "                doc.append(((preceding_words, following_words, distance, spread)))\n",
    "            feature.append(doc)\n",
    "        return feature\n",
    "    \n",
    "    \n",
    "    #lists of feature\n",
    "    feature2=feature_structure(candidates, raw_data)\n",
    "    feature3=feature_candidate_length(candidates)\n",
    "    feature4=feature_frequency(label, tf_corpus)\n",
    "    feature5=feature_occurence(candidates, corpus) #important feature there is 3 feature\n",
    "    \n",
    "    #add values of all features into candidate list\n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_candidate in range(len(candidates[n_doc])):\n",
    "            #binary_title\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate][0],)\n",
    "            #frequency_title\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate][1],)\n",
    "            #binary_abstract\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate][2],)\n",
    "            #frequency_abstract\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate][3],)\n",
    "            #binary_introduction\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate][4],)\n",
    "            #frequency_introduction\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate][5],)            \n",
    "            #length\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature3[n_doc][n_candidate],)\n",
    "            #term frequency\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature4[n_doc][n_candidate][0],)\n",
    "            #supervised\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature4[n_doc][n_candidate][1],)\n",
    "            #first occurence\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature5[n_doc][n_candidate][0],)\n",
    "            #last occurence\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature5[n_doc][n_candidate][1],)\n",
    "            #distance from first occurence\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature5[n_doc][n_candidate][2],)\n",
    "            #spread\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature5[n_doc][n_candidate][3],)\n",
    "            \n",
    "    #convert the format from candidate from tuple to list\n",
    "    x_data=[]\n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_candidate in range(len(candidates[n_doc])):\n",
    "            #append only values of features. without word\n",
    "            x_data.append(list(candidates[n_doc][n_candidate][1:]))\n",
    "    return x_data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create label for training or testing\n",
    "def create_label(candidates, label):\n",
    "    y_label=[]\n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_cand in range(len(candidates[n_doc])):\n",
    "            keyphrase_document=list(label[n_doc])\n",
    "            if candidates[n_doc][n_cand][0] not in keyphrase_document:\n",
    "                y_label.append(0)\n",
    "            else:\n",
    "                y_label.append(1)\n",
    "    return y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_to_fmeasure(predict_proba, candidates, labels, models):\n",
    "    #all_fmeasure=[]\n",
    "    for model in range(0, len(predict_proba)):\n",
    "        probability=[]\n",
    "        counter=0\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                doc.append((candidates[n_doc][n_cand][0], predict_proba[model][counter]))\n",
    "                counter+=1\n",
    "            probability.append(doc)\n",
    "        fmeasure=calculate_fmeasure(get_top_candidates(probability, 15), labels)\n",
    "        print(\"Model %s: %.3f\" % (models[model][0], fmeasure))\n",
    "        #all_fmeasure.append((models[model][0], fmeasure))\n",
    "    return 'finish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(x_train, y_train, x_test, y_test, candidates, labels):\n",
    "    seed = 7 #just randomly select the number\n",
    "    models = []\n",
    "    #models.append(('LR', LogisticRegression(C=1)))\n",
    "    #models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    #models.append(('XGB', XGBClassifier()))\n",
    "    #models.append(('NB', GaussianNB()))\n",
    "    #models.append(('DT', DecisionTreeClassifier()))\n",
    "    #models.append(('SVM', SVC(probability=True)))\n",
    "    models.append(('RF', RF(n_estimators=20, max_depth=11)))#highest\n",
    "    #models.append(('AdaBoost', AdaBoostClassifier(n_estimators=70, learning_rate=1.0)))#highest, learning rate must be 1\n",
    "    #models.append(('Bagging', BaggingClassifier(n_estimators=30)))#highest\n",
    "    #models.append(('GradientBoosting', (GradientBoostingClassifier(n_estimators=85, learning_rate=0.2))))\n",
    "    #models.append(('MLP', (MLPClassifier(learning_rate_init=0.0001)))) #learning_rate_init=0.002))))\n",
    "    #models.append(('Multinomial', (MultinomialNB(alpha=2.0))))\n",
    "    \n",
    "    \n",
    "    #loop as many as features\n",
    "    print(\"Take one feature out\")\n",
    "    for feature in range(len(x_train[0])):\n",
    "        print(\"Remove feature number\", feature+1)\n",
    "     \n",
    "        modified_train=[doc[:feature]+doc[feature+1:] for doc in x_train] \n",
    "        modified_test=[doc[:feature]+doc[feature+1:] for doc in x_test]\n",
    "        \n",
    "        predict_proba=[]\n",
    "        for name, model in models:\n",
    "        #calculate F-score, recall and precision\n",
    "            #print(\"%s: %.3f\" % (name, accuracy_score(model.fit(modified_train, train_label).predict(modified_test), test_label)))\n",
    "            predict_proba.append(model.fit(x_train, y_train).predict_proba(x_test)[:,1])\n",
    "            \n",
    "        #calculate f-measure\n",
    "        fmeasure=probability_to_fmeasure(predict_proba, candidates, labels, models)\n",
    "    \n",
    "    \n",
    "    #results = []\n",
    "    #names = []\n",
    "    #scoring='accuracy'\n",
    "    #print(\"\\nAccuracy on testing data:\")\n",
    "    all_predict_proba=[]\n",
    "    for name, model in models:\n",
    "        #accuracy\n",
    "        #print(\"%s: %.3f\" % (name, accuracy_score(model.fit(x_train, y_train).predict(x_test), y_test)))\n",
    "        all_predict_proba.append(model.fit(x_train, y_train).predict_proba(x_test)[:,1])\n",
    "    \n",
    "    print(\"Fmeasure on full features:\")\n",
    "    all_fmeasure=[]\n",
    "    for model in range(0, len(all_predict_proba)):\n",
    "        probability=[]\n",
    "        counter=0\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                doc.append((candidates[n_doc][n_cand][0], all_predict_proba[model][counter]))\n",
    "                counter+=1\n",
    "            probability.append(doc)\n",
    "        fmeasure=calculate_fmeasure(get_top_candidates(probability, 15), labels)\n",
    "        all_fmeasure.append((models[model][0], fmeasure))\n",
    "    return all_fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####with machine learning\n",
    "##NGRAM\n",
    "#open all pickle\n",
    "print(\"Opening all pickles\")\n",
    "train_raw=open_pickle('xml train raw')\n",
    "train_data=open_pickle('xml train data')\n",
    "\n",
    "train_label=open_pickle('xml train label')\n",
    "train_tf_corpus=open_pickle('xml train tf corpus')\n",
    "train_tf_nounphrase_corpus=open_pickle('xml train tf new nounphrase corpus')\n",
    "\n",
    "\n",
    "test_raw=open_pickle('xml test raw')\n",
    "test_data=open_pickle('xml test data')\n",
    "\n",
    "test_label=open_pickle('xml test label')\n",
    "test_tf_corpus=open_pickle('xml test tf corpus')\n",
    "test_tf_nounphrase_corpus=open_pickle('xml test tf new nounphrase corpus')\n",
    "\n",
    "'''\n",
    "ngram_candidates=open_pickle('xml ngram candidates')\n",
    "test_ngram_candidates=open_pickle('xml test ngram candidates')\n",
    "'''\n",
    "\n",
    "nounphrase_candidates=open_pickle('xml new nounphrase candidates')\n",
    "test_nounphrase_candidates=open_pickle('xml test new nounphrase candidates')\n",
    "\n",
    "'''\n",
    "print(\"creating ngram example on training\")\n",
    "ngram_x_train=create_example(train_raw, train_data, ngram_candidates, train_label, train_tf_corpus)\n",
    "print(\"creating ngram label on training\")\n",
    "ngram_y_train=create_label(ngram_candidates, train_label)\n",
    "print(\"creating ngram example on testing\")\n",
    "ngram_x_test=create_example(test_raw, test_data, test_ngram_candidates, test_label, test_tf_corpus)\n",
    "print(\"creating ngram label on testing\")\n",
    "ngram_y_test=create_label(test_ngram_candidates, test_label)\n",
    "'''\n",
    "\n",
    "print(\"creating phrase example on training\")\n",
    "nounphrase_x_train=create_example(train_raw, train_data, nounphrase_candidates, train_label, train_tf_corpus) \n",
    "print(\"creating phrase label on training\")\n",
    "nounphrase_y_train=create_label(nounphrase_candidates, train_label)\n",
    "print(\"creating phrase example on testing\")\n",
    "nounphrase_x_test=create_example(test_raw, test_data, test_nounphrase_candidates, test_label, test_tf_corpus)\n",
    "print(\"creating phrase label on testing\")\n",
    "nounphrase_y_test=create_label(test_nounphrase_candidates, test_label)\n",
    "\n",
    "\n",
    "print(\"F-measure with machine learning (testing)\")\n",
    "#ngram_prediction=predict_data(ngram_x_train, ngram_y_train, ngram_x_test, ngram_y_test, test_ngram_candidates, test_label)\n",
    "#print('F-measure on ngram', ngram_prediction)\n",
    "\n",
    "nounphrase_prediction=predict_data(nounphrase_x_train, nounphrase_y_train, nounphrase_x_test, nounphrase_y_test, test_nounphrase_candidates, test_label)\n",
    "print('F-measure on noun phrase', nounphrase_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####testing feature\n",
    "from nltk import sent_tokenize\n",
    "print(\"Opening all pickles\")\n",
    "train_raw=open_pickle('xml train raw')\n",
    "ngram_candidates=open_pickle('xml ngram candidates')\n",
    "\n",
    "def feature_(candidates, raw_data):\n",
    "    doc_tfidf=[]\n",
    "    for doc in raw_data:\n",
    "        text=[doc['full_text']]\n",
    "        for n_doc in text:\n",
    "            sentences=[sent for sent in sent_tokenize(n_doc)]\n",
    "            tf_idf=calculate_ngram_tfidf(sentences)\n",
    "        doc_tfidf.append(tf_idf)\n",
    "        \n",
    "    feature=[]\n",
    "    for n_doc in range(len(candidates)):\n",
    "        doc=[]\n",
    "        for n_cand in range(len(candidates[n_doc])):\n",
    "            first_index=cleaned_corpus[n_doc].lower().find(candidates[n_doc][n_cand][0])\n",
    "            last_index=cleaned_corpus[n_doc].lower().rfind(candidates[n_doc][n_cand][0])\n",
    "            preceding_words=len(cleaned_corpus[n_doc][:first_index].split(\" \"))-1\n",
    "            following_words=len(cleaned_corpus[n_doc][:last_index].split(\" \"))-1\n",
    "            distance=float(\"{0:.2F}\".format(preceding_words/corpus_words))\n",
    "            spread=len(cleaned_corpus[n_doc][first_index:last_index].split(\" \"))-1\n",
    "            doc.append(((preceding_words, following_words, distance, spread)))\n",
    "        feature.append(doc)    \n",
    "    \n",
    "    return corpus[:1]\n",
    "\n",
    "print(feature_(ngram_candidates, train_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
