{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, re, string, itertools\n",
    "import logging\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "from nltk.stem.porter import *\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "from nltk.chunk import tree2conlltags\n",
    "from pandas import DataFrame\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn import svm                                       #library for creating the classifier, SVM\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def create_pickle(data, name):\n",
    "    with open('%s.pickle' % name,'wb') as handle:\n",
    "        result=pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return result\n",
    "\n",
    "def open_pickle(name):\n",
    "    with open('%s.pickle' % name,'rb') as handle:\n",
    "        result=pickle.load(handle)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(input_list):\n",
    "    result=[]\n",
    "    #remove unwanted character per line\n",
    "    for line in input_list:\n",
    "        clean=re.sub(\"(\\.)?\\n\",'', line) #remove \\n\n",
    "        clean=re.sub(\"('s)\",'', clean) #remove 's\n",
    "        clean=re.sub(\"\\[([0-9]{1,2}\\,?\\s?)+\\]\",'', clean) #remove [2]\n",
    "        clean=re.sub(\"\\(([0-9]{1,2}\\,?\\s?)+\\)\",'', clean) #remove (2)\n",
    "        #clean=re.sub(r\"\\b(iv|ix|x|v?i{0,3})+\\b\",'', clean) #remove roman number\n",
    "        #remove fig. 2 etc, need improvement to catch the sentence after it\n",
    "        #clean=re.sub(r\"\\b(i.e.g.|e.g.|i.e.)\",'', clean) #remove i.e.g., i.e., e.g.\n",
    "        clean=re.sub(\"([Ff]ig.|[Ff]igure|[Tt]ab.|[Tt]able)\\s?[0-9]{1,2}\",'', clean) #remove fig. 2 etc\n",
    "        clean=re.sub(r\"\\b((https?://|www.)[^\\s]+)\",'', clean) #remove email\n",
    "        result.append(clean)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(path):\n",
    "    raw=[]\n",
    "    for file in path:\n",
    "        dict_doc={'doc_id': None, 'title': None, 'abstract': None, 'introduction': None, 'full-text': None}\n",
    "        file_id=os.path.basename(file).rstrip('.txt.final') #catch only file name  \n",
    "        dict_doc['doc_id']=file_id\n",
    "        \n",
    "        source=open(file,encoding='utf-8').readlines()\n",
    "        source=clean(source)\n",
    "        \n",
    "        ##########detect title\n",
    "        beginning=re.sub(\"\\n\", \"\", source[0]) #retrieve title\n",
    "        candidate=re.sub(\"\\n\", \"\", source[1]) # retrieve title candidate\n",
    "        h_candidate=word_tokenize(re.sub(\"-\",' ',candidate)) #tokenize the candidate\n",
    "        \n",
    "        title=[]\n",
    "        name=[]\n",
    "        for word in h_candidate:\n",
    "            if wordnet.synsets(word): #check if title candidate exist on wordnet\n",
    "                title.append(word)\n",
    "            else:\n",
    "                name.append(word)\n",
    "            #if title>\n",
    "            if len(title)>len(name): \n",
    "                newtitle=beginning+' '+candidate\n",
    "            elif len(title)==len(name):\n",
    "                newtitle=beginning\n",
    "            else:\n",
    "                newtitle=beginning\n",
    "\n",
    "        dict_doc['title']=newtitle\n",
    "        \n",
    "        content=source[2:]\n",
    "        ######check header, inconsistency all file\n",
    "        r_intro=re.compile(\"^1\\.?\\s[A-Z]+\")\n",
    "        r_after_intro=re.compile(\"^2\\.?\\s[A-Z]+\")\n",
    "        r_ref=re.compile(\"[0-9]{1,2}?\\.?\\s?R[EFERENCES|eferences]\") #detect reference\n",
    "        #r_header=re.compile(\"[0-9]{1,2}?\\.?\\s?[A-Z]\")\n",
    "        \n",
    "        in_abstract=content.index('ABSTRACT')\n",
    "        in_authorkey=content.index('Categories and Subject Descriptors')\n",
    "        \n",
    "        list_intro=[i for i, item in enumerate(content) if re.search(r_intro, item)]\n",
    "        in_intro=list_intro[0]\n",
    "        list_after_intro=[i for i, item in enumerate(content) if re.search(r_after_intro, item)]\n",
    "        in_after_intro=list_after_intro[0]\n",
    "        list_ref=[i for i, item in enumerate(content) if re.search(r_ref, item)]\n",
    "        in_ref=list_ref[0]\n",
    "        \n",
    "        abstract=content[in_abstract+1:in_authorkey] #eliminate keyword and category\n",
    "        intro=content[in_intro+1:in_after_intro]\n",
    "        body=content[in_after_intro+1:in_ref]      \n",
    "        \n",
    "        list_title=[]\n",
    "        list_title.append(newtitle)\n",
    "        \n",
    "        full_text=list(chain(list_title, abstract, intro, body))\n",
    "        dict_doc['abstract']=abstract\n",
    "        dict_doc['introduction']=intro\n",
    "        dict_doc['body']=body\n",
    "        dict_doc['full_text']=full_text\n",
    "        \n",
    "        #per sentence in a document\n",
    "        raw.append(dict_doc)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data to tfidfvectorizer format\n",
    "#corpus=['a','b','c']\n",
    "#RENAME TO CREATE CORPUS\n",
    "def create_corpus(raw_data):\n",
    "    #add to list and join all element in full text into a text\n",
    "    train_data=[' '.join(doc['full_text']) for doc in raw_data]\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate noun phrases based on corpus\n",
    "def extract_candidate(raw_data):\n",
    "    \n",
    "    #porter stemmer\n",
    "    #stemmer=PorterStemmer()\n",
    "    \n",
    "    #from http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\n",
    "    grammar=r'NP: {(<JJ.*>* <NN.*>+ <IN>)? (<JJ.*>* <NN.*>+)+}' #only detect noun phrases that contain specific pattern, hypen word is counted as one NN\n",
    "    \n",
    "    #test new grammar\n",
    "    #grammar=r'NP: {(<JJ>* <VBN>? <NN.*>+ <IN>)? <JJ>* <VBG>? <NN.*>+}' \n",
    "    \n",
    "    punct = set(string.punctuation) #list of punctuation\n",
    "    chunker = RegexpParser(grammar) #chunker from nltk\n",
    "    \n",
    "    def lambda_unpack(f):\n",
    "        return lambda args:f(*args)\n",
    "    \n",
    "    postag_sents = pos_tag_sents(word_tokenize(sent) for sent in raw_data) #tokenize and create pos tag per sentence\n",
    "    #list of IOB of noun phrases based on the specific grammar\n",
    "    noun_phrases = list(chain.from_iterable(tree2conlltags(chunker.parse(tagged_sent)) for tagged_sent in postag_sents)) \n",
    "    \n",
    "    #join B-NP and I-NP tags as one noun phrase excluding O tags    \n",
    "    merged_nounphrase = [' '.join(stemmer.stem(word) for word, pos, chunk in group).lower() for key, group in\n",
    "                    itertools.groupby(noun_phrases, lambda_unpack(lambda word, pos, chunk: chunk != 'O')) if key]\n",
    "    \n",
    "    #filter noun phrases from stopwords and punctuation\n",
    "    all_nounphrases=[cand for cand in merged_nounphrase\n",
    "            if len(cand)>2 and not all(char in punct for char in cand)]\n",
    "    \n",
    "    #select distinct noun phrases\n",
    "    vocabulary=(list(set(all_nounphrases)))\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ngram_tfidf(corpus):\n",
    "    \n",
    "    #porter stemmer\n",
    "    stemmer=PorterStemmer()\n",
    "\n",
    "    #eliminate ngram which starts or ends from stopwords\n",
    "    #from https://stackoverflow.com/questions/49746555/sklearn-tfidfvectorizer-generate-custom\n",
    "    #-ngrams-by-not-removing-stopword-in-the/49775000#49775000\n",
    "    class NewTfidfVectorizer(TfidfVectorizer):\n",
    "        def _word_ngrams(self, tokens, stop_words=None):\n",
    "            # First get tokens without stop words\n",
    "            tokens = super(TfidfVectorizer, self)._word_ngrams(tokens, None)\n",
    "            if stop_words is not None:\n",
    "                new_tokens=[]\n",
    "                for token in tokens:\n",
    "                    split_words = token.split(' ')\n",
    "                    # Only check the first and last word for stop words\n",
    "                    if len(token)>2 and split_words[0] not in stop_words and split_words[-1] not in stop_words:\n",
    "                        #stem every word in token\n",
    "                        if len(split_words)==1 and len(split_words[0])>2:\n",
    "                            new_tokens.append(stemmer.stem(token))\n",
    "                        elif len(split_words)==2 and split_words[-1]==\"'\":\n",
    "                            del(token)\n",
    "                        elif len(split_words[0])<3 and len(split_words[1])<3:\n",
    "                            del(token)\n",
    "                        elif split_words[1]==\"'\" and split_words[2]==\"s\":\n",
    "                            new_tokens.append(stemmer.stem(split_words[0])+split_words[1]+split_words[2])\n",
    "                        else:\n",
    "                            new_tokens.append(' '.join(list(stemmer.stem(word) for word in word_tokenize(token))))\n",
    "                return new_tokens\n",
    "            return tokens\n",
    "    \n",
    "    stop_words=text.ENGLISH_STOP_WORDS\n",
    "    \n",
    "    tfidf=NewTfidfVectorizer(ngram_range=(1,5), stop_words=stop_words,\n",
    "                                token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    \n",
    "    matrix=tfidf.fit_transform(corpus)\n",
    "    feature_names=tfidf.get_feature_names()\n",
    "\n",
    "    #how to print tf-idf from https://stackoverflow.com/questions/34449127/\n",
    "    #sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-documen\n",
    "    candidates=[]\n",
    "    for doc in range(0,len(corpus)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        tfidf_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_tfidf=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in tfidf_doc]]\n",
    "        candidates.append(names_tfidf)\n",
    "    \n",
    "    #this is the candidates per document\n",
    "    #vocab_perdoc=tfidf.inverse_transform(matrix)\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nounphrase_tfidf(corpus, voc):\n",
    "    \n",
    "    stemmer=PorterStemmer()\n",
    "    \n",
    "    class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "        def build_tokenizer(self):\n",
    "            tokenizer=super(TfidfVectorizer, self).build_tokenizer()\n",
    "            return lambda doc: (stemmer.stem(token) for token in tokenizer(doc) if token not in stop_words)\n",
    "\n",
    "    stop_words=set(text.ENGLISH_STOP_WORDS)\n",
    "    s=['of','in','on','for']\n",
    "    stop_words=stop_words.difference(s)\n",
    "    tfidf=StemmedTfidfVectorizer(ngram_range=(1,5), stop_words=stop_words, vocabulary=voc, token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    \n",
    "    matrix=tfidf.fit_transform(corpus)\n",
    "    feature_names=tfidf.get_feature_names()\n",
    "\n",
    "    #how to print tf-idf from https://stackoverflow.com/questions/34449127/\n",
    "    #sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-documen\n",
    "    candidates=[]\n",
    "    for doc in range(0,len(corpus)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        tfidf_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_tfidf=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in tfidf_doc]]\n",
    "        candidates.append(names_tfidf)\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###sorting candidates based on 15 keywords\n",
    "def get_top_candidates(candidates_list, number_keyphrases):\n",
    "    best_candidates=[]\n",
    "    for doc in candidates_list:\n",
    "        #sort candidates by tf-idf value\n",
    "        sorted_candidates=sorted(doc, key=lambda x: x[1], reverse=True)[:number_keyphrases]\n",
    "        #best_candidates.append(sorted_candidates)\n",
    "        best_candidates.append([x for x,_ in sorted_candidates])\n",
    "        #remove overlapping keywords\n",
    "    return best_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###compare candidates to goldstandard\n",
    "def extract_goldkeyphrase(gold_data):\n",
    "    \n",
    "    r_plus=re.compile(\"^.*\\+.*$\")\n",
    "    r_slash=re.compile(\"^.*\\s.*\\/.*$\")\n",
    "    \n",
    "    gold_standard=[]\n",
    "    for line in gold_data.split('\\n'):\n",
    "        doc=[]      \n",
    "        for key in line[6:].split(','):\n",
    "            if key[0]==' ':\n",
    "                doc.append(key[1:])\n",
    "            elif re.search(r_plus, key):\n",
    "                split=[]\n",
    "                for element in key.split('+'):\n",
    "                    doc.append(element)\n",
    "            elif re.search(r_slash, key):\n",
    "                split=[]\n",
    "                for element in key.split('/'):\n",
    "                    doc.append(element)\n",
    "            else:\n",
    "                doc.append(key)\n",
    "        gold_standard.append(doc)\n",
    "    return gold_standard\n",
    "\n",
    "def calculate_fmeasure(candidates_list, gold_data):\n",
    "    #true positive\n",
    "    all_matches=[]\n",
    "    for index in range(len(candidates_list)):\n",
    "        #store all measure per document in dic\n",
    "        value={'tp': None, 'fp': None, 'fn': None, 'gold': None}\n",
    "        value['gold']=len(gold_data[index])\n",
    "        #counter true positive per document\n",
    "        true_positive=0\n",
    "        #loop between elements\n",
    "        for element_candidate in candidates_list[index]:                    \n",
    "            for element_goldkeyphrase in gold_data[index]:\n",
    "                #matched predicted keyword in gold keyphrase\n",
    "                if element_candidate==element_goldkeyphrase:\n",
    "                    #matches_perdoc.append(element_candidate)\n",
    "                    true_positive+=1\n",
    "            #if need the detail of evaluation\n",
    "            value['tp']=int(true_positive) #matched pair\n",
    "            value['fp']=int(15-true_positive) #depend how many keyword should we use\n",
    "            value['fn']=int(value['gold']-value['tp'])\n",
    "        #return all metrics per document\n",
    "        all_matches.append(value)\n",
    "\n",
    "    true_positive=sum(doc['tp'] for doc in all_matches)\n",
    "    false_positive=sum(doc['fp'] for doc in all_matches)\n",
    "    false_negative=sum(doc['fn'] for doc in all_matches)\n",
    "    \n",
    "    #matched/total top n\n",
    "    precision=float(true_positive/(false_positive+true_positive))\n",
    "    #matched/total gold standard\n",
    "    recall=float(true_positive/(false_negative+true_positive))\n",
    "    # calculate with micro averagedprecision\n",
    "    f_measure=float(\"{0:.2F}\".format(2*(precision*recall)/(precision+recall)*100))\n",
    "    return f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_frequency(section):\n",
    "     #porter stemmer\n",
    "    stemmer=PorterStemmer()\n",
    "    \n",
    "    #eliminate ngram which starts or ends from stopwords\n",
    "    class NewCountVectorizer(CountVectorizer):\n",
    "        def _word_ngrams(self, tokens, stop_words=None):\n",
    "            # First get tokens without stop words\n",
    "            tokens = super(CountVectorizer, self)._word_ngrams(tokens, None)\n",
    "            if stop_words is not None:\n",
    "                new_tokens=[]\n",
    "                for token in tokens:\n",
    "                    split_words = token.split(' ')\n",
    "                    # Only check the first and last word for stop words\n",
    "                    if len(token)>2 and split_words[0] not in stop_words and split_words[-1] not in stop_words:\n",
    "                        #stem every word in token\n",
    "                        if len(split_words)==1 and len(split_words[0])>2:\n",
    "                            new_tokens.append(stemmer.stem(token))\n",
    "                        elif len(split_words)==2 and split_words[-1]==\"'\":\n",
    "                            del(token)\n",
    "                        elif len(split_words[0])<3 and len(split_words[1])<3:\n",
    "                            del(token)\n",
    "                        elif split_words[1]==\"'\" and split_words[2]==\"s\":\n",
    "                            new_tokens.append(stemmer.stem(split_words[0])+split_words[1]+split_words[2])\n",
    "                        else:\n",
    "                            new_tokens.append(' '.join(list(stemmer.stem(word) for word in word_tokenize(token))))\n",
    "                return new_tokens\n",
    "            return tokens\n",
    "    \n",
    "    stop_words=text.ENGLISH_STOP_WORDS\n",
    "    \n",
    "    count_vect=NewCountVectorizer(ngram_range=(1,5), stop_words=stop_words,\n",
    "                                token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    \n",
    "    matrix=count_vect.fit_transform(section)\n",
    "    feature_names=count_vect.get_feature_names()\n",
    "\n",
    "    #how to print tf-idf from https://stackoverflow.com/questions/34449127/\n",
    "    #sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-document\n",
    "    ngrams=[]\n",
    "    for doc in range(0,len(section)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        count_vect_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_count_vect=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in count_vect_doc]]\n",
    "        ngrams.append(names_count_vect)\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram TF-IDF version\n",
      "Noun phrase TF-IDF version\n"
     ]
    }
   ],
   "source": [
    "###Dont run it twice\n",
    "#----------------------------------------------------TF-IDF version\n",
    "###load training data\n",
    "train_directory=glob.glob('./se_txt/train/*.txt.final')\n",
    "train_raw=load_files(train_directory)\n",
    "pickle_train_raw=create_pickle(train_raw,'txt train raw')\n",
    "train_data=create_corpus(train_raw)\n",
    "pickle_train_data=create_pickle(train_data,'txt train data')\n",
    "train_tf_corpus=calculate_term_frequency(train_data)\n",
    "pickle_train_tf_corpus=create_pickle(train_tf_corpus,'txt train tf corpus')\n",
    "\n",
    "\n",
    "#load gold keyphrase\n",
    "train_label_directory=open('./se_txt/train/train.combined.stem.final', encoding='utf-8').read()\n",
    "train_label=extract_goldkeyphrase(train_label_directory)\n",
    "pickle_train_label=create_pickle(train_label, 'txt train label')\n",
    "\n",
    "###Load testing data\n",
    "test_directory=glob.glob('./se_txt/test/*.txt.final')\n",
    "test_raw=load_files(test_directory)\n",
    "pickle_test_raw=create_pickle(test_raw,'txt test raw')\n",
    "test_data=create_corpus(test_raw)\n",
    "pickle_test_data=create_pickle(test_data,'txt test data')\n",
    "test_tf_corpus=calculate_term_frequency(test_data)\n",
    "pickle_test_tf_corpus=create_pickle(test_tf_corpus,'txt test tf corpus')\n",
    "\n",
    "test_label_directory=open('./se_txt/test_answer/test.combined.stem.final', encoding='utf-8').read()\n",
    "test_label=extract_goldkeyphrase(test_label_directory)\n",
    "pickle_test_label=create_pickle(test_label, 'txt test label')\n",
    "\n",
    "#### Ngram version\n",
    "print(\"N-gram TF-IDF version\")\n",
    "ngram_candidates=calculate_ngram_tfidf(train_data) \n",
    "pickle_ngram_candidates=create_pickle(ngram_candidates, 'txt ngram candidates')\n",
    "#ngram_top_keyphrases=get_top_candidates(ngram_candidates, 15)\n",
    "#ngram_fmeasure=calculate_fmeasure(ngram_top_keyphrases, train_label)\n",
    "#print(\"F-measure on training:\", ngram_fmeasure)\n",
    "\n",
    "test_ngram_candidates=calculate_ngram_tfidf(test_data)\n",
    "pickle_test_ngram_candidates=create_pickle(test_ngram_candidates, 'txt test ngram candidates')\n",
    "#test_ngram_top_candidates=get_top_candidates(test_ngram_candidates, 15)\n",
    "#test_ngram_fmeasure=calculate_fmeasure(test_ngram_top_candidates, test_label)\n",
    "#print(\"F-measure on testing:\", test_ngram_fmeasure)\n",
    "\n",
    "\n",
    "#### Noun phrase version\n",
    "print(\"Noun phrase TF-IDF version\")\n",
    "nounphrase_vocabulary=create_phrase_vocabulary(train_data)\n",
    "nounphrase_candidates=calculate_nounphrase_tfidf(train_data, nounphrase_vocabulary)\n",
    "pickle_nounphrase_candidates=create_pickle(nounphrase_candidates, 'txt nounphrase candidates')\n",
    "#nounphrase_top_keyphrases=get_top_candidates(nounphrase_candidates, 15)\n",
    "#nounphrase_fmeasure=calculate_fmeasure(nounphrase_top_keyphrases, train_label)\n",
    "#print(\"F-measure on training:\", nounphrase_fmeasure)\n",
    "\n",
    "test_nounphrase_vocabulary=create_phrase_vocabulary(test_data)\n",
    "test_nounphrase_candidates=calculate_nounphrase_tfidf(test_data, test_nounphrase_vocabulary)\n",
    "pickle_test_nounphrase_candidates=create_pickle(test_nounphrase_candidates, 'txt test nounphrase candidates')\n",
    "#test_nounphrase_top_candidates=get_top_candidates(test_nounphrase_candidates, 15)\n",
    "#test_nounphrase_fmeasure=calculate_fmeasure(test_nounphrase_top_candidates, test_label)\n",
    "#print(\"F-measure on testing:\", test_nounphrase_fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus):\n",
    "    clean=[]\n",
    "    stemmer=PorterStemmer()\n",
    "    for doc in corpus:\n",
    "        cleaned_words=\" \".join([word for word in word_tokenize(doc.lower()) if re.search(r\"\\b[A-Za-z-]+\\b\", word) and len(word)>2])\n",
    "        stemmed_words=[stemmer.stem(word) for word in cleaned_words.split()]\n",
    "        clean.append(\" \".join([word for word in stemmed_words]))\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(raw_data, corpus, candidates, label, tf_corpus):\n",
    "    \n",
    "    #binary_title, frequency_title, binary_abstract, frequency_abstract, binary_introduction, frequency_introduction\n",
    "    def feature_structure(candidates, raw_data):\n",
    "        title_raw=[doc['title'] for doc in raw_data]\n",
    "        abstract_raw=[' '.join(doc['abstract']) for doc in raw_data]\n",
    "        introduction_raw=[' '.join(doc['introduction']) for doc in raw_data]  \n",
    "        title=calculate_term_frequency(title_raw)\n",
    "        abstract=calculate_term_frequency(abstract_raw)\n",
    "        introduction=calculate_term_frequency(introduction_raw) \n",
    "        feature=[]\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                title_perdoc=[feature for (feature, value) in title[n_doc]]\n",
    "                abstract_perdoc=[feature for (feature, value) in abstract[n_doc]]\n",
    "                introduction_perdoc=[feature for (feature, value) in introduction[n_doc]]\n",
    "                if candidates[n_doc][n_cand][0] in title_perdoc:\n",
    "                    binary_title=1\n",
    "                    value=[value for (feature, value) in title[n_doc] if feature in candidates[n_doc][n_cand][0]]\n",
    "                    frequency_title=value[0]\n",
    "                else:\n",
    "                    binary_title=0\n",
    "                    frequency_title=0\n",
    "                if candidates[n_doc][n_cand][0] in abstract_perdoc:\n",
    "                    binary_abstract=1\n",
    "                    value=[value for (feature, value) in abstract[n_doc] if feature in candidates[n_doc][n_cand][0]]\n",
    "                    frequency_abstract=value[0]\n",
    "                else:\n",
    "                    binary_abstract=0\n",
    "                    frequency_abstract=0\n",
    "                if candidates[n_doc][n_cand][0] in introduction_perdoc:\n",
    "                    binary_introduction=1\n",
    "                    value=[value for (feature, value) in introduction[n_doc] if feature in candidates[n_doc][n_cand][0]]\n",
    "                    frequency_introduction=value[0]\n",
    "                else:\n",
    "                    binary_introduction=0\n",
    "                    frequency_introduction=0\n",
    "                doc.append(((binary_title, frequency_title, binary_abstract, frequency_abstract, binary_introduction, frequency_introduction)))\n",
    "            feature.append(doc)\n",
    "        return feature\n",
    "    \n",
    "    def feature_candidate_length(candidates):\n",
    "        feature=[]\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_feature in range(len(candidates[n_doc])):\n",
    "                doc.append(len(candidates[n_doc][n_feature][0]))\n",
    "            feature.append(doc)\n",
    "        return feature\n",
    "        \n",
    "    def feature_frequency(label, tf_corpus):\n",
    "        merged_labels=list(chain.from_iterable(label))\n",
    "        feature=[]\n",
    "        for n_doc in range(len(tf_corpus)):\n",
    "            doc=[]\n",
    "            for n_cand in range(len(tf_corpus[n_doc])):\n",
    "                cand_freq=tf_corpus[n_doc][n_cand][1]\n",
    "                if tf_corpus[n_doc][n_cand][0] not in merged_labels:\n",
    "                    supervised=0\n",
    "                else:\n",
    "                    supervised=tf_corpus[n_doc][n_cand][1]\n",
    "                doc.append(((cand_freq, supervised)))\n",
    "            feature.append(doc)\n",
    "        return feature\n",
    "    \n",
    "    #create first, last occurence, distance from first occurence, spread from first and last occurence\n",
    "    def feature_occurence(candidates, corpus):\n",
    "        feature=[]\n",
    "        cleaned_corpus=clean_corpus(corpus)\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            corpus_words=len(cleaned_corpus[n_doc].split(\" \"))\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                first_index=cleaned_corpus[n_doc].lower().find(candidates[n_doc][n_cand][0])\n",
    "                last_index=cleaned_corpus[n_doc].lower().rfind(candidates[n_doc][n_cand][0])\n",
    "                preceding_words=len(cleaned_corpus[n_doc][:first_index].split(\" \"))-1\n",
    "                following_words=len(cleaned_corpus[n_doc][:last_index].split(\" \"))-1\n",
    "                distance=float(\"{0:.2F}\".format(preceding_words/corpus_words))\n",
    "                spread=len(cleaned_corpus[n_doc][first_index:last_index].split(\" \"))-1\n",
    "                doc.append(((preceding_words, following_words, distance, spread)))\n",
    "            feature.append(doc)\n",
    "        return feature\n",
    "    \n",
    "    \n",
    "    #lists of feature\n",
    "    feature2=feature_structure(candidates, raw_data)\n",
    "    feature3=feature_candidate_length(candidates)\n",
    "    feature4=feature_frequency(label, tf_corpus)\n",
    "    feature5=feature_occurence(candidates, corpus) #important feature there is 3 feature\n",
    "    \n",
    "    #add values of all features into candidate list\n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_candidate in range(len(candidates[n_doc])):\n",
    "            #binary_title\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate][0],)\n",
    "            #frequency_title\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate][1],)\n",
    "            #binary_abstract\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate][2],)\n",
    "            #frequency_abstract\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate][3],)\n",
    "            #binary_introduction\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate][4],)\n",
    "            #frequency_introduction\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate][5],)            \n",
    "            #length\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature3[n_doc][n_candidate],)\n",
    "            #term frequency\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature4[n_doc][n_candidate][0],)\n",
    "            #supervised\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature4[n_doc][n_candidate][1],)\n",
    "            #first occurence\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature5[n_doc][n_candidate][0],)\n",
    "            #last occurence\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature5[n_doc][n_candidate][1],)\n",
    "            #distance from first occurence\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature5[n_doc][n_candidate][2],)\n",
    "            #spread\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature5[n_doc][n_candidate][3],)\n",
    "            \n",
    "    #convert the format from candidate from tuple to list\n",
    "    x_data=[]\n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_candidate in range(len(candidates[n_doc])):\n",
    "            #append only values of features. without word\n",
    "            x_data.append(list(candidates[n_doc][n_candidate][1:]))\n",
    "    return x_data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create label for training or testing\n",
    "def create_label(candidates, label):\n",
    "    y_label=[]\n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_cand in range(len(candidates[n_doc])):\n",
    "            keyphrase_document=list(label[n_doc])\n",
    "            if candidates[n_doc][n_cand][0] not in keyphrase_document:\n",
    "                y_label.append(0)\n",
    "            else:\n",
    "                y_label.append(1)\n",
    "    return y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_to_fmeasure(predict_proba, candidates, labels, models):\n",
    "    #all_fmeasure=[]\n",
    "    for model in range(0, len(predict_proba)):\n",
    "        probability=[]\n",
    "        counter=0\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                doc.append((candidates[n_doc][n_cand][0], predict_proba[model][counter]))\n",
    "                counter+=1\n",
    "            probability.append(doc)\n",
    "        fmeasure=calculate_fmeasure(get_top_candidates(probability, 15), labels)\n",
    "        print(\"Model %s: %.3f\" % (models[model][0], fmeasure))\n",
    "        #all_fmeasure.append((models[model][0], fmeasure))\n",
    "    return 'finish'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_data(x_train, y_train, x_test, y_test, candidates, labels):\n",
    "    seed = 7 #just randomly select the number\n",
    "    models = []\n",
    "    #models.append(('LR', LogisticRegression()))\n",
    "    #models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    #models.append(('NB', GaussianNB()))\n",
    "    #models.append(('SVM', SVC(probability=True)))\n",
    "    models.append(('RF', RF(n_estimators=20, max_depth=10)))\n",
    "    #models.append(('AdaBoost', AdaBoostClassifier()))\n",
    "    #models.append(('Bagging', BaggingClassifier()))\n",
    "    #models.append(('GradientBoosting', (GradientBoostingClassifier())))\n",
    "    #models.append(('MLP', (MLPClassifier()))) #learning_rate_init=0.002))))\n",
    "    #models.append(('Multinomial', (MultinomialNB())))\n",
    "    \n",
    "\n",
    "    #loop as many as features\n",
    "    print(\"Take one feature out\")\n",
    "    for feature in range(len(x_train[0])):\n",
    "        print(\"Remove feature number\", feature+1)\n",
    "     \n",
    "        modified_train=[doc[:feature]+doc[feature+1:] for doc in x_train] \n",
    "        modified_test=[doc[:feature]+doc[feature+1:] for doc in x_test]\n",
    "        \n",
    "        predict_proba=[]\n",
    "        for name, model in models:\n",
    "        #calculate F-score, recall and precision\n",
    "            #print(\"%s: %.3f\" % (name, accuracy_score(model.fit(modified_train, train_label).predict(modified_test), test_label)))\n",
    "            predict_proba.append(model.fit(x_train, y_train).predict_proba(x_test)[:,1])\n",
    "            \n",
    "        #calculate f-measure\n",
    "        fmeasure=probability_to_fmeasure(predict_proba, candidates, labels, models)\n",
    "    \n",
    "    #results = []\n",
    "    #names = []\n",
    "    #scoring='accuracy'\n",
    "    #print(\"\\nAccuracy on testing data:\")\n",
    "    all_predict_proba=[]\n",
    "    for name, model in models:\n",
    "        #accuracy\n",
    "        #print(\"%s: %.3f\" % (name, accuracy_score(model.fit(x_train, y_train).predict(x_test), y_test)))\n",
    "        all_predict_proba.append(model.fit(x_train, y_train).predict_proba(x_test)[:,1])\n",
    "    \n",
    "    print(\"Fmeasure on full features:\")\n",
    "    all_fmeasure=[]\n",
    "    for model in range(0, len(all_predict_proba)):\n",
    "        probability=[]\n",
    "        counter=0\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                doc.append((candidates[n_doc][n_cand][0], all_predict_proba[model][counter]))\n",
    "                counter+=1\n",
    "            probability.append(doc)\n",
    "        fmeasure=calculate_fmeasure(get_top_candidates(probability, 15), labels)\n",
    "        all_fmeasure.append((models[model][0], fmeasure))\n",
    "    return all_fmeasure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_directory=glob.glob('./se_txt/train/dummy/dump/*.txt.final')\n",
    "train_raw=load_files(train_directory)\n",
    "train_data=create_corpus(train_raw)\n",
    "print(extract_candidate(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['approach', 'power budget', 'ip core', 'point pi', 'direct of arriv differ', 'wireless network', 'from resourc monitor', 'weapon classif in addit', 'a/v', 'voltag requir', 'simpl high resolut grid search', 'combin', 'power at base station', 'sensor pair', 'true for constant projectil speed', 'signal process core', 'sensor platform', 'as corba servant', 'address', 'system perform', 'bound on applic resourc util', 'video captur', 'recommend frame rate', 'shockwav', 'system resourc util', 'jitter', '|ti − tj| < = |pi − pj |/c + ε where c', 'detect logic', 'tao', 'task of adapt resourc manag', 'second zc', 'differ sensor', 't_start', 'discret dynam', 'desir upper bound on resourc util', 'in pure java', 'thin adapt layer', 'ak-47 m240', 'v. sinc p2', 'within b', '+ tao', 'if calib avail comput weapon type', 'with ieee', 'resembl hardwar build block', 'by time of arriv t1 < t2 < t3', 'central scenario', 'hard qo requir', 'set point', 'line', 'at posit p1 with time t1', 'measur direct', 'avail observ', 'situat', 'enabl function subset', 'resourc util with hyarm', 'continuum', 'dedic connector-contain', 'wireless network bandwidth', 'sensor network', 'sens', 'for muzzl blast', 'best-effort', 'remot corba', 'uv−1', 'same link', 'reason timeout', 'slice', 'corba a/v stream servic specif', 'perspect of hybrid control theoret techniqu', 's a/v', 'dfrf', 'video signal', 'oper system', 'wireless connect', 'occasionally.2', 'appli hyarm', 'zc segment', 'residu cpu', 'mpeg4', 'chang', 'microphon array', 'signal characterist', 'raw sampl of sever acoust event', 'network link', 'deploy', 'capit n.', 'rs232 port', 'cone surfac travel', 'current node', 'sensorboard for precis time synchron', 'packet loss', 'interrupt usb psram u a r t u a r t mb det sw det rec bluetooth link user interfac sensor fusion locat engin gp messag', 'desir precis', 'record signal', 'from loss of resourc', '− p2', 'use', 'power', 'power xilinx xc3s1000 fpga chip with variou standard peripher ip core', 'util under-util of system resourc', 'resourc util per applic', 'end receiv', 'ieee', 'such as latenc', 'weapon specif inform', 'n n e l s compass picoblaz comm interfac picoblaz wt12 bluetooth radio mote if', 'lavish broadcast polici', 'class resolut frame rate latenc', 'discret variabl of applic execut', 'data fusion', 'residu system resourc', 'environ', 'wireless link with virtual uart emul', 'logic', 'user interfac', 'singl trajectori', 'definit', 'extern ft245r usb devic control', 'number', 'for futur work', 'network bandwidth and/or', 'dre multimedia system architectur', 'good standoff distanc', 'more than adequ', 'threshold level', 'face', 'time of detect', 'blast detector', 'raw sampl', 'bluetooth channel', 'end-to-end applic qo requir', 'sphere', 'from place', 'self orient', 'shoter posit p let', 'shooter', 'hybrid adapt resourcemanag middlewar', 'bullet result', 'resourc constraint', 'in circular buffer', 'bullet deceler', 'desir set point', 'ghz wireless link radio control messag rout acoust event encod sensor time synch network time synch.remot control time', 'result of over-util of resourc', 'neg zc valid full period max garbag wrong sign garbag', 'smaller time differ', 'resourc requir', 'research in adapt resourc manag for dre system', 'by vhdl test bench', 'idl', 'qos-rel properti', 'correspond bearing-rang pair', 'face of fluctuat in workload', 'accuraci for shot', 'observ pattern', 'network util', 'shock wave event', 'offset', 'essenti middlewar servic', 'toa estim', 'kb ram', 'other trajectori', 'robust algorithm', 'estim', 'subsystem', 'domain specif task', 'under combat condit', 'function of hyarm', 'error sensit problem', 'shot time', 'in whitham', 'qo of applic', 'fpga configur', 'fluctuat', 'first sensor', 'more resourc end-to-end sinc resourc avail', 'assemblersensor control', 'by retransmiss', 'dure sudden fluctuat in applic workload', 'valu of latenc', 'hypothesi', 'bluetooth capabl for commun', 'real world applic', 't = uvt1−t2 uv−1 sinc u', 'in such reverber environ as clutter urban terrain', 'record block', 'second half period', 'predefin delay', 'hyarm', 'wavefront', 'lan', 'zoom', 'cutoff', 'differ resourc', 'box', 'better separ', 'compar latenc', 's [ t-d ] < e len', 'such as mpeg1', 'delay', 'same set of regist', 'signal process task', 'leverag capabl', 'same time', 'sensor prototyp', 'signific challeng', 'longer time window', 'dismount soldier', 'reliabl wirelin link', 't =', 'level', 'desir lower bound on resourc util', 'j|d', 'sensorboard', 'same calib data', 'vector from p1', 'complet configur', 'obviou disadvantag', 'program', 'consist function for b', 'multimedia system', 'interact', 'data sourc', 'same set of pre-record signal', 'on hybrid control theoret techniqu', 'speed', 'music', 'in previou step', 'commun interfac', 'temporari over-util of resourc', 'ace orb', 'execut', 'although rudimentari acoust signal process', 'sensor board', 'softwar program', 'resourc avail and/or demand', 'servic', 'oper condit', 'on-board digit compass line', 'multimedia system case studi hyarm', 'place', 'if poor qualiti video', 'type of resourc', 'helmetmount microphon array', 'maximum number', 'best-effort applic', 'static natur', 'system condit', 'in assembl', 'projectil', 'remaind', 'base station', 'user via graphic display signific improv in video encoding/decod', 'natur', 'first period', 'shot', 'radiu', 'refer', 'projectil diamet d', 'result in packet loss', 'mpeg1', 'by uav', 'note that pp2', 'first static version', 'over-util', 'command', '− cv', 'm240 bullet deceler measur', 'hybrid control techniqu', 'accur mac-lay time-stamp', 'nois', 'distanc', 'normal vector vi', 'from uav', 'blast', 'pda screen with low latenc', 'qos-en applic', 'shockwav length estim', 'effici buffer manag', 'divers', 'small microphon array', 'node', 'access', 'neighbor', 'range-spe pair', 'network bandwidth', 'resourc monitor', 'enforc', 'network', 'experi configur', 'continu stream of sensor', 'portabl across differ pda', 'differ weapon', 'half', 'toa-', 'footprint', 'gateway', 'pictur resolut as softer qo requir', 'direct differ threshold', 'empir result', 'specifi bound', 'from p1', 'emerg respons', 'fire', 'own locat', 'at p2', 'signific differ in system perform', 'intel etherexpress pro', 'picoblaz program', 'physic link physic link physic link base station end receiv end receiv end receiver` physic link end receiv uav camera video encod camera video encod camera video encod uav camera video encod camera video encod camera video encod uav camera video encod camera video encod camera video encod', 'few practic shot per weapon', 'share', 'compon interconnect', 'bluetooth interfac', 'by discret action', 'radio modul', 'observ', 'structur', 'follow entiti', 'mani other video format', 'amplitud threshold', 'uniqu sensor fusion approach', 'custom logic', 'observ muzzl blast signatur', 'cpu monitor', 'adapt middlewar via hybrid control', 'system util', 'import', 'other signal process modul', 'as end receiv', 'gin', 'dynam rang', 'experi', 'time stamp', 'gb hard drive', 'xx1 + yy1 + zz1 = c', 'on top of tinyo', 'residu resourc', 'muzzl', 'featur vector', 'mac', 'initi trigger', 'comput rang estim', 'i2 c', 'applic', 'from automat messag aggreg', 'applic with increased/decreas qo', 'open-sourc implement of real-tim corba', 'interact of variou part', 'serial a/d core', 'kevlar helmet', 'that hyarm yield', 'multipl sensor', 'yield', 'extend shockwav cone surfac', 'own shockwav', 'month', 'dre', 'plane', 'singl well-aim shot', 'orient need', 'polici', 'result of recent advanc in video encod', 'first edg done', 'for r2', 'modul', 'by high precis', 'dre multimedia system rresourc', 'p = p1 + cu', 'on-board pseudo sram', 'compress scheme', 'in group', 'estim algorithm', 'superson speed rang', 'case', 'receiv articl', 'number of measur', 'few meter', 'protocol reli', 'side', 'exampl polici', 'follow manner', 'first step', 'latest gener sensorboard', 'surveil', 'record compon', 'accur time convers', 'result in less noisi measur', 'same shockwav time of arriv', 'interrupt virtual regist interfac c o o r d', 'abov formula', 'further research', 'frame rate', 'autom way', 'possibl unord pair', 'head', 'individu measur', 'on-board jtag chain-directli', 'real-tim', 'addit requir', 'for commun', 's pda', 'signal', 'effici system workload manag', 'resourc reserv mechan such as differenti servic', 'audio sampl', 'peripher', 'requir comput perform for shockwav detect', 'rout integr time synchron', 'without side effect', 'subject of interest', 'microsecond varianc', 'multimedia system case studi', 'first half period', 'such as speed', 'comput power', 'adapt resourc manag capabl of hyarm', 'content', 's [ t-d ] < e', 'shooter posit', 'applic qo comparison of system perform', 'evalu', 'current timer valu', 'higher prioriti over best-effort class of applic', 'form p1', 'cot micaz mote', 'store', 'modif', 'by low current ldo', 'plastic box', 'real challeng', 'by xilinx picoblaz microcontrol core', 'american footbal', 'network congest', 'ad-hoc internod commun', 'by rapid oscil', 'in resourc avail and/or demand by constant monitor of resourc util', 'design', 'end', 'number of acoust channel', 'amplitud muzzl blast', 'board', 'microcontrol', 'scheme', 'tstart', 'effici buck', 'promis result', 'show that hyarm', 'multipl toa detect', 'debug purpos', 'such as real video', 'frequenc', 'http', 'relationship between miss distanc', 'weapon classif with soldier-wear network sensor', 'shooter local', 'uc berkeley/crossbow mica product line', 'uart modul', 'in specif platform', 'self local servic', 'follow endto-end real-tim qo requir', 'exampl of heterogen input data', 'posit amplitud', 'locat', 'bullet trajectori shooter posit', 'pipelin', 'clear symmetri', 'amplifi with control gain', 'softwar testb', 'busi citi street', 'end-to-end qo requir of higher prioriti applic', 'mhz intel pentium iii processor', 'video into mpeg-2', 'on militari kevlar helmet', 'as option uart', 'us militari personnel', 's time', 'full-spe usb transfer', 'reason timeout after partial detect', 'adapt', 'shockwav detect', 'quad pack of recharg aa batteri', 'line segment', 'network bandwidth avail', 's formula', 'qo requir such as video frame rate', 'though applic qo', 'data regist', 'v power net', 'backtrack', 'on pda', 'adapt resourc manag', 'shockwavelength', 'sensor fusion modul receiv', 'point dure fluctuat in input work load', 'possibl orient error', 'such as frame rate', 'detect time', 'shockwav signal characterist', 'm16 m249 m4', 'singl soldier final', 'frame-r determin', 'sound', 'much higher impact', 'bear estim support b', 'core', 't2 − t', 'order of magnitud', 'usb modul implement', 'mote applic', 'actual devic', 'differ period', 'avail of resourc', 'detail descript', 'found', 'project', 'monitor of system resourc util', 'object of dynam modif', 'singl detect messag', 'upper bound', 'without hyarm', '[ t ]', 'enough for onlin process', '+ yy2 + zz2 = c', 'ad-hoc mote network', 'compress video', 'acoust event featur', 'sensorboard time stamp', 'trajectori estim', 'independ evalu', 'in section', 'transitori oscil', 'posit', 'mb physic memori', 'outlier filter', 'improv', 'classif result', 'melt', 'effect util', 'dedic gpio line', 'kb program space', 'unstabl behavior', 'in practic', 'problem', 'time', 'immedi shooter locat', 'frame rate of best effort applic', 'blast aoa', 'reverber', 'analog circuitri', 'partit', 'n−1', 'such as resolut', 'singl sensor', 'resourc util without hyarm rate', 'hyarm architectur hyarm', 's [ t-d ] > e tstart', 'mach number m', 'trajectori precis', 'measur data', 'shockwav direct', 'sensor suppli', 'experiment setup', 'actual test', 'with new one', 'execut model', 'in other word', 'continu variabl applic adapt', 'begin', 'subject', 'result', 'limit number of practic shot', 'di115', 'event', 'sound propag time', 'miss distanc', 'shockwav gener', 'laptop comput', 'distinct acoust phenomena', 'at most n', 'time of arriv', 'end-to-end qo requir', 'qo requir', 'miss distanc relationship', 'mhz serial clock', 'dedic connector', 'sensor node', 'p1−p2', 'smaller rang', 'run-tim paramet valu', 'follow section in detail', 'present', 'common video compress scheme', 'm16', 'differ events-th', 'mb bandwidth', 'frame rate of lower prioriti applic', 'paramet valu for new type of weapon', 'direct flood-rout framework', 'trajectori measur', 'e.g.', 'techniqu', 'variou compress scheme', 'corba audio/video', 'video stream', 'independ analog channel', 'shockwav measur', 'better predict', 'at uav', 't1 − t', 'local area network', 'shooter posit estim', 'ip cores-impl in hardwar descript languag', 'aoa-', 'in java', 'overal accuraci', 'power sourc', 'comput', 'ballist shockwav', 'simpl analyt solut', 'interfram delay', 'time inform', 'muzzl blast toa', 'rate', 'azimuth', 'earlier work', 'hundr of inexpens sensor node', 'next stage', 'block diagram', 'video in real-tim', 'individu sensor', 'architectur', 'remot oper call', 'capabl', 'e.g', 'amplitud', 'angl of arriv', 'sensor', 'messag', 'pair of shockwav', 'static', 'sensor at p2', 'cumul delay', 'unpack raw measur', 'differ bear', 'natur extens', 'deploy system', 'bullet trajectori', 'correspond mote local time stamp', 'degre bear precis', 'primari indic of system perform', 'acoust detector', 'muzzl blast detect', 'commun bandwidth between uav', 'encod', 'd l1/4', 'darpa program', 'sever time in us armi mout', 'on redhat linux', 'entiti of receiv uav tao resourc util hyarm central control a/v stream servic', 'resourc', 'bandwidth util', 'calib estim', 'higher level protocol', 'local maximum', 'applic that affect applic qo', 'possibl aoa- for muzzl blast', 'wireless sensor network-bas mobil countersnip system', 'accuraci of gp', 'excess system cost', 'follow way', 'util of system resourc', 'p1p2p3 plane', 'physic limit', 'sensor connector', 'in realiti', 'sampl collect', 'by insurg', 'network bandwidth util', 'zc domain', 'that v', 'avail in larg amount', 'signal path', 'weapon estim', 'precis angl of arriv estim necessit', 'second edg', 'initi trigger mechan', 'show', 'condit', 'toa measur', 'same set of sensor', 'micaz mote', 'sampl', 'applic note', 'such as averag latenc', 'that human eye', 'len', 'm4 bullet deceler measur', 'avail at www dre.vanderbilt.edu/∼nshankar/hyarm/ articl', 'interv', 'time delay slot', 'intern timer', 'better qualiti video', 'i2 c bu', '1-5 ms', 'applic resourc util', 'digit compass', 'compliant radio transceiv', 'gpio line', 'in vhdl', 'link', 'direct', 'weapon', 'sensorfus algorithm', 'predict system perform', 'helmet-mount system', 'primari task', 'receiv over-util of system resourc', 'special singl sensor case', 'softwar architectur', 'real-tim corba specif', 'conspicu characterist', 'local time refer', 'maximum effect bandwidth', 'comput task', 'first edg', 'fluctuat in applic workload', 'elev', 'at p', 'echo', 'logist problem', 'muzzl blast signatur', 'vicin', 'end-to-end qualiti of servic', 'vivj', 'i.e.', 'b box', 'custom sensorboard', 'ms/', 'data messag', 'valu', 'solut step', 'section', 'overview of hyarm', 'import characterist', 'hybrid adapt resource-manag middlewar', 'typic militari rifl', 'multimedia system infrastructur', 'train data', 'system integr', 'step', 'observ of muzzl blast', 'timeli properti', 'iftop', 'angl of arriv measur', 'roll inform', 'p1 = p2', 'rid of larg measur error', 'first', 'paramet valu', 'applic qo', 'qo for qos-en', 'dure underand over-util of system resourc', 'from p at time t. both p', 'system design', 'dis-', 'requir', 'same weapon from differ place', 'distanc between uav', 'qos-en class of applic', 'direct differ', 'm249 weapon', 'accuraci', 'search algorithm', 'scale factor', 'true rang', 'fals muzzl blast detect', 's measurement-if', 'final deploy', 'len2 + len1', 'basic idea', 'rf environ', 'by research', 'minimum steep', 'center', 'mhz clock', 'receiv video', 'longer period of time comparison of applic qo', 'first zc', '= t s [ t ]', 'lmax', 'event specif featur vector', 'cot micaz mote for internod commun', 'resourc monitor for cpu util', 'contribut', 'p2 − p2', 'weapon classif', 'tstart idl', 'librari of variou acoust signatur', 'in-system program', 'fpga part', 'panason wm64pnt', 'desir util bound', '% weapon estim accuraci', 'bound', 'sever characterist of acoust shockwav', 'depend on dynam factor', 'vr1 = c', 'essenti for develop purpos', 'uart link', 'cpu util', 'ammunit pair', 'tp tp', 'dot product', 'current util', 'solut', 'rich set of interfac', 'futur implement', 'voltag', 'pipelin sink', 'avail for qos-en applic', 'mean valu of video paramet', 'captur', 'as independ ip core', 'muzzle-shock fusion u v', 'field correl toa measur', 'on pre-record signal', 'dynam nois level', 'real-tim version of linux', 'motion', 'workload', 'simpl state machin logic', 'oper', 'power suppli circuitri', 'gener sensorboard', 'variou class of applic', 'predefin threshold', 'timesi linux/net', 'tree rout', 'and/or minim hardware/softwar infrastructur depend', 'computationallyintens oper', '30-60 db', 'high sampl rate', 'broadcast', 'xilinx ise tool suit', 'same set of ip core', 'signal shape', 'desir util', 'other configur', 'analog devic ad7476', 'sinc studi', 'uav node', '% calib estim accuraci', 'at univers of utah', 'soft processor cores-execut true softwar program', 'signal process algorithm from regular audio applic', 'wherea without hyarm', 'in contrast', '-1 -0.8 -0.6 -0.4', 'muzzl blast aoa', 'cm×10 cm rectangl', 'properti', 'surveil applic', 'same relationship', 'requir gain', 'same place', 'smallest rm error', 'aoa u', 'speed of sound c t =', 'μsec', 'length l', 'workload in open dre system', 'warfigth', 'pinptr', 'analog section', 'multimedia system architectur wireless link wireless link wireless link', 'acoust channel', 'mote interfac', 'type', 'rang', 'simpl geometri valid step', 'python script', 'angl', 'wast', 'microcontroller-bas board', 'challeng for softwar develop', 'algorithm', 'precis time synchron', 'regist file', 'with hyarm', 'system', 'section plane', 'bluetooth radio', 'ad-hoc multihop network', 'micaz', 'distribut real-tim video distribut system', 'sever uart core', 'fpga', 'in accord', 'search', 'specif', 'be0', 'texa instrument cc-1000 radio', 'time interv', 'good correl', 'space', 'travel', 'energi', 'i/o', 'consist valu', 'at p1', 'toa', 'for other acoust sourc', 'effect adapt resourc manag strategi', 'wave', 'infinit number of trajectory-bullet speed pair', 'rout servic', 'system with hyarm', 'reconfigur fpga', 'station', 'oscil', 'by bbn', 'a/v stream servic', 'upon full detect', 'shooter locat', 'regist file base address', 'final trajectori', 'import of countersnip system', 'local', 'larger differ', 'posit zc', 'time convers', 'video captur by uav', 'system resourc', 'with applic', 'i2c', 'outlier', 'lower bound on system resourc util', 'exampl for hyarm', 'interrupt', 'unknown direct of arriv', 'cpu', 'video properti', 'same event', '+ y2 + z2', 'via gpio line', 'measur error', 'consist', 'qo requir such as latenc', 'uav-bas multimedia system', 'actual length', 'first gener of mote', 't2 − t1', 'dimension box', 'power function', 'bandwidth', 'rest', 'digit potentiomet driver', 'state machin', 'core set', 'honeywel hmr3300 digit compass modul', 'mm calib', 'countersnip system', 'bear estim', 'onli featur', 'latter one', 'class of applic', 'short period of time', 'static sensor network-bas solut', 'aggreg', 'match detect core', 'same size', 'statu', 'higher muzzl speed', 'microphon space', 'mbp bandwidth', 'in parallel', 'truli uniqu characterist', 'similar problem', 'with data', 'certain area', 'acoust shockwav', 'debug facil', 'control', 'singl pole recurs low-pass filter', 'sensor fusion', 'bullet rel', 'cone', 'regul', 'maximum consist', 'network resourc util of wireless network link between uav', 'usb buse soldier oper devic', 'build block', 'data', 'weapon type', 'paramet', 't3 − t1', 'if trajectori avail comput rang', 'weapon for classif purpos', 'shockwav detect core', 'infrastructur', 'mpeg-4', 'termin emul program', 'resourc util updat from monitor', 'increas in resourc demand by applic', 'muzzl blast detector', 'by earlier comput step', 'measur', 'sound sourc', 'case video', 'error function', 'higher frame rate', 'same shockwav aoa', 'detect', 'that hyarm', 'calib', 'correspond detect logic', 'of muzzl blast', 'regist', 'remark', 'increas system resourc util', 'deceler function result', 'unstabl system', 'detect messag', 'compass read', 'conveni debug facil', 'overutil', 'by hyarm', 'simpl linear regress', 'middl east', 'at p2 with time t2', 'audio channel', 'heterogen sensor fusion algorithm', 'control of complex system', 'cnn', 'as uav', 'into comparison of resourc util', 'direct of arriv consid', 'certain threshold', 'that end-to-end qo requir', 'hybrid sensor fusion', 'shockwav period t', 'act as base station', 'rang estim', 'jitter as harder qo requir', 'psram devic', 'proper deceler correct function', 'atop tao', 'for system resourc util', 'studi', 'input paramet', 'sampl per second', 'data point', 'fpga-on pair', 'differ execut algorithm', 'video applic hyarm applic adapt remot object call control input resourc util resourc util / control input control input legend', 'field assumpt', 'sensor fusion algorithm', 'latenc', 'function', 'sensor board softwar', 'algorithm consist', 'ak47', 'discret requir set', 'custom sensor board', 'muzzl blast', 'cone surfac', 'mpb physic link', 'system without hyarm with hyarm', 's [ t ]', 'qo of best-effort applic', 'mundan energi', 'constraint', 'more receiv', 's capabl', 'fpga sensor board mica radio modul', 'signal amplitud', 'effect use of system resourc', 'same collect logic', 'testb', 'start', 'usb', 'averag', 'mb l', 'perform result and analysi', 'custom', 'measur bullet speed for differ calib', 'wave front', 'area', 'if trajectori avail comput calib', 'opensourc librari', 'hour', '200-300 μs', 'vhdl', 'qos-en video distribut servic', 'trajectori estim danicki', 'l o g c', 'util', 'detect event signal with high precis time stamp', 'hyarm control', 'maximum clock speed', 'dure increas in workload condit', 'task', 'differ polici', 'upper bound on jitter', 'cpu resourc util of uav', 'number of sensor', 'previou read', 'differ audio channel', 'individu shockwav aoa measur', 'kind of sonic boom', 'mote', 'goal', 'latency/jitt', 'configur memori', 'facil', 'paradigm', '3-axi compass', 'high energi muzzl', 'second half due', 'continu variabl', 'problemat with low cost', 'compress techniqu', 'of much faster test run', 'best fit function', 'common time base', 'team', 'wire usb connect', '% calib estim error', 'dre system', 'and/or frame rate', 'adapt process', 'differ video compress scheme', 'rudimentari low-pass filter', 'us armi aberdeen test center', 'larg error', 'fourth microphon', 'coordin', 'sender mpeg1 mpeg4 real video hyarm resourc monitor a/v stream servic', 'such modif', 'total length', 'respect receiv', 'sourc local', 'address bu abstract on top', 'by default', 'direct of arriv', 'p2p2', 'although applic paramet', 'extern interfac', 'first period garbag', 'although mechan damp', 'possibl bear', 'mpeg-2', 'discret consist function', 'applic execut', 'surfac', 'bluetooth link', 'psram', 'onboard flash', 'underli polici at run-tim', 'm16 assault rifl', 'case studi', 'pp2', 'correspond time', 'in nesc', 'dre multimedia system', 'applic adapt', 'power-level', 'data distributor', 'for calib estim', 'differ ambient nois environ', 'on separ tini board', 'adapt behavior of hyarm via experi', 'ffmpeg', 'speed of sound', 'speed sound', 'central virtual regist file', 'sever nonlinear distort', 'specifi set point', '< r', 'msec', 'pda/laptop', 'orient inform', 'bridg', 'cpu resourc', 'stringent end-to-end qo requir', 'necessari modif', 'analog channel', 'mb/ effect bandwidth', 'high precis', 'orient', 'henc', 'i.e', 'overal resourc util', 'microphon membran', 'microsecond', 'uav', 'radiu paramet', 'import result', 'multipl signal from differ microphon', 'robust radio', 'border between softwar', 'long initi period', 'that system resourc', 'softwar', 'collect', 'and/or', 'measur time differ', 'for applic qo', 'detect event', 'set point on resourc util', 'end-to-end qo requir of applic', 'kbp', 'effici gradient', '% report rate per sensor', 'of them-most', 'tripod', 'mbp', 'main sensorboard with ribbon cabl', 'by camera', 'technolog for futur work', 'low-level interfac', 'inform', 'featur', 'resourc manag middlewar hyarm', 'multipath environ', 'desir system condit', 'bluetooth', 'detect result', 'sensor posit', 'wireless link', 'electret microphon', 'detect algorithm', 'run-tim paramet through differ interfac', 'monitor of system resourc', 'blast aoa estim', 'regular speech', 'in fact', 'interfac', 'dot product definit', 'resourc util', 'uniqu featur of dfrf', 'long neg segment', 'wire rs232 link', 'soft processor core', 'differ direct', 'over-util of system resourc in dre system', 'current level of system resourc util', 'singl bear estim', 'wireless commun channel', 'with micaz motes-in fact', 'm240', 'whole area of interest', 'frame video of qosen applic', 'dynam chang in resourc availabil2', 'code', 'p2 + cv', 'ballist shockwav from multipl sensor', 'receiv', 'follow softwar packag', 'function equival', 'class-bas kernel resourc manag', 'synchron', 'jitter comparison of resourc util', 'rit', 'first prototyp', 'sever trajectori candid', 'hardwar', 'signal process path', 'mote interrupt', 'process video', 'for exampl', 'consist with b', 'car', 'period', 'qualiti', 'militari oper in urban terrain', 'singl point', 'resourc avail achiev end-to-end real-tim qualiti of servic', 'bisect search algorithm', 'describ', 'network activ', 'acoust detect', 'effect system resourc util', 'number of success muzzl blast', 'differ muzzl blast', 'crude local fusion modul', 'type of entiti', 'current time', 'ad-hoc wireless network', 'best-effort class', 'static modif', 's a/v stream servic', 'that fix p', 'transmit video', 'from vector', 'compress', 'compon', 'note', 'absolut orient', '.50 cal', 'rang data from individu sensor', 'compress video signal', 'intens stimuli', 'rise time', 'plug-in architectur', 'acoust sensorboard/mot', 'util of resourc', 'coordin core', 'system cost', 'resourc util model', 'constant stream of news report', 'such as rate monoton algorithm', 'on-board bluetooth modul', 'lmin s [ t ]', 'p = p2 + cv', 'smaller time differ result', 'analysi', 'video', 'with custom usb dongl', 'primari type of resourc', 'gun', 'by whitham', 'edg', 'long analog signal path', 'dure fluctuat in applic workload', 'hardwar configur', 'vehiclemount countersnip system', 'data acquisit', 'next section', 'recommend paramet', 'expon', 'experiment data', 'm249', 'sever time', 'layer', 'vr2 = c', 'acoust phenomena', 'perpendicular miss distanc b', 'linear function', 'current mplement of hyarm', 'peopl', 'correspond valu', 'corner', 'p1 + cu', 'high-precis radio interferometr self local approach', 'cpu resourc util', 'maximum measur error', 'in octob', 'fpga core', 'independ power', 'unknown t.', 'hdl', 'estim calib class', 'irregular oscil', 'halv', 'rom', 'respons', 'overview', 'buffer', 'valu of applic paramet such as video compress scheme', 'accur event detect', 'method', 'inter-fram delay', 'set of record channel', 'aoa', 'superson projectil', 'microphon', 'heterogen inform', 'in o', 'point', 'in section 6.5-and', 'period updat', 'own gp locat data', 'pressur wave', 'command line interfac', 'area of interest', 'hyarm structur', 'mhz', 'mobil sniper team', 'in box b', 'high system perform', 'aoa pair', 'few μs per hop error', 's [ t-d ] > e', 'qo of qos-en applic', 'multihop', 'high speed usb link', 'complex voltag requir', 'disjoint point iti', 'khz', 'quadrat system', 'command line interfac for test', 'direct estim', 'by continu variabl', 'of potenti automat weapon burst', 'adapt resourc manag for distribut real-tim embed', 'underli system', 'system effect resourc util', 'stream servic', 'conclus', 'citi traffic', 'differ gain', 'averag packet loss ratio', 'process acoust data', 'dre multimedia system with hyarm hyarm', 'processor', 'ram driver implement data read/writ cycl with correct time', 'that enough resourc', 'in 8-bit data sampl', 'mbp ethernet port', 'detector', 'exampl', 'for differ messag', 'pda', 'approxim function result', 'open-sourc librari', 'normal vector', 'promis approach', 'mhz rang', 'zero cross', 'shockwav geometri', 'geograph rout', 'resourc avail', 'integr', 'pitch', 'in figur', 'sourc', 'sensor enclosur', 'notifi', 'n-wave', '3-axi digit compass', 'core set c', 'program code', 'origin', 'special emphasi', 'bear', 'sensor fusion techniqu', 'in case', 'simpl fifo with parallel data line', 'over util of system resourc', 'steep rise edg', 'differ rang', 'in plastic enclosur', 'filter sensor read', 'with greater resolut', 'differ calib projectil', 'gaussian nois', 'length', 'opensourc', 'video paramet such as resolut', 'a challeng problem', 'pictur resolut', 'soft qo requir of best-effort applic', 'characterist shape', 'file', 'develop', 'latter', 'user interfac subsystem', 'soldier', 'soldierwear mobil countersnip system', 'wherea', 'glitch', 'length ratio hold', 'same side', 'unreli wireless network with limit bandwidth', 'toa inform', 'paper', 'real video', 'emerg respons applic', 'simpl mean', 'multipl channel in parallel', 'on dynam factor', 'channel', 'threshold', 'extern memori block', 'tstart ≥ lmax', 'input work load', 'data buse', 'handheld devic', 'muzzl veloc', 'figur', 'dure system initi', 'correspond detect', 't o r a', 'accur trajectori estim', 'wherea under-util', 'theoret framework', 'i2 c interfac', 'adapt middlewar', 'terminolog', 'acoust sensor', 'shockwav length', 'lower latenc', 'wide varieti of commun protocol such as bluetooth', 'central control', '% accuraci in rang estim for longer rang shot', 'sram interfac', 'shockwav signal', 'function resourc util legend resourc alloc applic paramet', 'clear benefit', 'implement', 'pend zc', 'detect core', 's resourc monitor', 'network bandwidth monitor', 'relationship', 'time synchron', 'execut of applic', 'sensor applic reli', 'perform of hyarm', 'adapt resourc provis by mean of hyarm', 'resolut', 'standard μp interfac bu', '12-bit serial adc', 'from other sensor', 'simultan shot', 'mm nato projectil', 'variou commun interfac', 'microphon by sensor', 'mach number', 'data sampl', 'applic qo requir', 'rel distanc between uav', 'current resourc util', 'shooter posit estim algorithm', 'largest neighbor set', 'research on hyarm with relat work', 'local pda', 'in detail', 'of higher import', 'mid', 'with aoa', 'reflect', 'system in divers way', 'emulab', 'special case', 'multipl soft processor core', 'of raw video', 'under-util of system resourc', 'm2−1', 'trajectori', 'increas', 'bluegiga wt12', 'trajectori index', 'as soldier', 'independ shockwav measur', 'cm separ', 'note that measur from other node', 'bullet', 'measur speed', 'conic shape', 'as jitter', 'lmin', 'much garbag', 'decentr system', 'resent', 'garbag', 'receiv messag', 'stream', 'dre multimedia system case studi', 'consist function', 'system respons']\n"
     ]
    }
   ],
   "source": [
    "#CREATE DUMMY FIRST\n",
    "def extract_candidate(raw_data):\n",
    "    \n",
    "    #porter stemmer\n",
    "    stemmer=PorterStemmer()\n",
    "    \n",
    "    grammar=r'''CNP: {(((<JJ.*>|<NN.*>)+ | ((<JJ.*>|<NN.*>)* (<IN>)?) (<JJ.*>|<NN.*>)*) <NN.*>)}\n",
    "            NN: {<NN.*>+ <NN.*>}\n",
    "            NP: {(<JJ.*>|<NN.*>)+ <NN.*>}\n",
    "        '''\n",
    "    \n",
    "    punct = set(string.punctuation) #list of punctuation\n",
    "    chunker = RegexpParser(grammar) #chunker from nltk\n",
    "    \n",
    "    def lambda_unpack(f):\n",
    "        return lambda args:f(*args)\n",
    "    \n",
    "    postag_sents = pos_tag_sents(word_tokenize(sent) for sent in raw_data) #tokenize and create pos tag per sentence\n",
    "    #list of IOB of noun phrases based on the specific grammar\n",
    "    noun_phrases = list(chain.from_iterable(tree2conlltags(chunker.parse(tagged_sent)) for tagged_sent in postag_sents)) \n",
    "    \n",
    "    #join B-NP and I-NP tags as one noun phrase excluding O tags    \n",
    "    merged_nounphrase = [' '.join(stemmer.stem(word) for word, pos, chunk in group).lower() for key, group in\n",
    "                    itertools.groupby(noun_phrases, lambda_unpack(lambda word, pos, chunk: chunk != 'O')) if key]\n",
    "    \n",
    "    #filter noun phrases from stopwords and punctuation\n",
    "    all_nounphrases=[cand for cand in merged_nounphrase\n",
    "            if len(cand)>2 and not all(char in punct for char in cand)]\n",
    "    \n",
    "    #select distinct noun phrases\n",
    "    vocabulary=(list(set(all_nounphrases)))\n",
    "    return vocabulary\n",
    "\n",
    "    #return noun_phrases\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['very good view swimming pool', 'good beautiful shape playing house', 'good view swimming pool', 'high level machine learning', 'cold chocolate banana milkshake', 'highly shared playing house', 'shape playing house', 'hot green banana', 'addictive machine learning', 'exciting machine learning', 'shared playing house', 'machine learning', 'swimming pool', 'chocolate banana', 'banana milkshake', 'green banana', 'learning', 'bath', 'swim', 'swimming', 'banana', 'milkshake']\n",
      "[['very good view swimming pool', 'good view swimming pool', 'view swimming pool', 'swimming pool', 'pool'], ['good beautiful shape playing house', 'beautiful shape playing house', 'shape playing house', 'playing house', 'house'], ['high level machine learning', 'level machine learning', 'machine learning', 'learning'], ['cold chocolate banana milkshake', 'chocolate banana milkshake', 'banana milkshake', 'milkshake'], ['highly shared playing house', 'shared playing house', 'playing house', 'house'], ['hot green banana', 'green banana', 'banana'], ['addictive machine learning', 'machine learning', 'learning'], ['exciting machine learning', 'machine learning', 'learning'], ['bath']]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "t=[\"machine learning\",\"learning\",\"bath\",\"swim\",\"swimming\",\"swimming pool\",\"very good view swimming pool\",\n",
    "   \"good view swimming pool\",\"high level machine learning\",\"banana\", \"chocolate banana\",\"cold chocolate banana milkshake\",\n",
    "  \"good beautiful shape playing house\",\"shape playing house\",\"milkshake\",\"banana milkshake\", \"green banana\",\n",
    "  \"hot green banana\",\"addictive machine learning\",\"exciting machine learning\", \"shared playing house\",\n",
    "   \"highly shared playing house\"]\n",
    "t.sort(key= lambda s: len(s.split(\" \")), reverse=True)\n",
    "print(t)\n",
    "\n",
    "phrase_chains=[]\n",
    "rest=[]\n",
    "for n_cand in range(len(t)):\n",
    "    if len(t[n_cand].split(\" \"))==5:\n",
    "        substring=[word for word in t[n_cand].split(\" \")]\n",
    "        chain=[' '.join(substring[i:]) for i in range(0,5)]\n",
    "        phrase_chains.append(chain)\n",
    "    elif len(t[n_cand].split(\" \"))==4:\n",
    "        #check if this string has been added into another chain\n",
    "        if len([word for word in t[:n_cand] if t[n_cand] not in word]) == len(t[:n_cand]):\n",
    "            substring=[word for word in t[n_cand].split(\" \")]\n",
    "            chain=[' '.join(substring[i:]) for i in range(0,4)]\n",
    "            phrase_chains.append(chain)\n",
    "    elif len(t[n_cand].split(\" \"))==3:\n",
    "        if len([word for word in t[:n_cand] if t[n_cand] not in word]) == len(t[:n_cand]):\n",
    "            substring=[word for word in t[n_cand].split(\" \")]\n",
    "            chain=[' '.join(substring[i:]) for i in range(0,3)]\n",
    "            phrase_chains.append(chain)\n",
    "    elif len(t[n_cand].split(\" \"))==2:\n",
    "        if len([word for word in t[:n_cand] if t[n_cand] not in word]) == len(t[:n_cand]):\n",
    "            substring=[word for word in t[n_cand].split(\" \")]\n",
    "            chain=[' '.join(substring[i:]) for i in range(0,2)]\n",
    "            phrase_chains.append(chain)\n",
    "    elif len(t[n_cand].split(\" \"))==1:\n",
    "        if len([word for word in t[:n_cand] if t[n_cand] not in word]) == len(t[:n_cand]):\n",
    "            substring=[word for word in t[n_cand].split(\" \")]\n",
    "            chain=[' '.join(substring[i:]) for i in range(0,1)]\n",
    "            phrase_chains.append(chain)\n",
    "\n",
    "print(phrase_chains)\n",
    "[ 'high level machine learning',\n",
    " 'cold chocolate banana milkshake', 'highly shared playing house', 'shape playing house', 'hot green banana', \n",
    " 'addictive machine learning', 'exciting machine learning', 'shared playing house', 'machine learning',\n",
    " 'chocolate banana', 'banana milkshake', 'green banana', 'learning', 'bath', 'swim', 'swimming', 'banana', 'milkshake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##merge some related \n",
    "from itertools import chain\n",
    "\n",
    "t=[\"machine learning\",\"learning\",\"bath\",\"swim\",\"swimming\",\"swimming pool\",\"very good view swimming pool\",\n",
    "   \"good view swimming pool\",\"high level machine learning\",\"banana\", \"chocolate banana\",\"cold chocolate banana milkshake\",\n",
    "  \"good beautiful shape playing house\",\"shape playing house\",\"milkshake\",\"banana milkshake\", \"green banana\",\n",
    "  \"hot green banana\",\"addictive machine learning\",\"exciting machine learning\", \"shared playing house\",\n",
    "   \"highly shared playing house\"]\n",
    "t.sort(key= lambda s: len(s.split(\" \")), reverse=True)\n",
    "print(t)\n",
    "\n",
    "phrase_chains=[]\n",
    "rest=[]\n",
    "for n_cand in range(len(t)):\n",
    "    if len(t[n_cand].split(\" \"))==5:\n",
    "        substring=[word for word in t[n_cand].split(\" \")]\n",
    "        chain=[' '.join(substring[i:]) for i in range(0,5)]\n",
    "        phrase_chains.append(chain)\n",
    "    elif len(t[n_cand].split(\" \"))==4:\n",
    "        #check if this string has been added into another chain\n",
    "        if len([word for word in t[:n_cand] if t[n_cand] not in word]) == len(t[:n_cand]):\n",
    "            substring=[word for word in t[n_cand].split(\" \")]\n",
    "            chain=[' '.join(substring[i:]) for i in range(0,4)]\n",
    "            \n",
    "            phrase_chains.append(chain)\n",
    "    elif len(t[n_cand].split(\" \"))==3:\n",
    "        if len([word for word in t[:n_cand] if t[n_cand] not in word]) == len(t[:n_cand]):\n",
    "            substring=[word for word in t[n_cand].split(\" \")]\n",
    "            chain=[' '.join(substring[i:]) for i in range(0,3)]\n",
    "            phrase_chains.append(chain)\n",
    "    elif len(t[n_cand].split(\" \"))==2:\n",
    "        if len([word for word in t[:n_cand] if t[n_cand] not in word]) == len(t[:n_cand]):\n",
    "            substring=[word for word in t[n_cand].split(\" \")]\n",
    "            chain=[' '.join(substring[i:]) for i in range(0,2)]\n",
    "            phrase_chains.append(chain)\n",
    "    elif len(t[n_cand].split(\" \"))==1:\n",
    "        if len([word for word in t[:n_cand] if t[n_cand] not in word]) == len(t[:n_cand]):\n",
    "            substring=[word for word in t[n_cand].split(\" \")]\n",
    "            chain=[' '.join(substring[i:]) for i in range(0,1)]\n",
    "            phrase_chains.append(chain)\n",
    "print(phrase_chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'swimming pool': 4, 'very good view swimming pool': 1, 'pool': 5, 'good view swimming pool': 2, 'view swimming pool': 3}, {'playing house': 4, 'beautiful shape playing house': 2, 'good beautiful shape playing house': 1, 'shape playing house': 3, 'house': 5}, {'learning': 4, 'machine learning': 3, 'high level machine learning': 1, 'level machine learning': 2}, {'chocolate banana milkshake': 2, 'milkshake': 4, 'banana milkshake': 3, 'cold chocolate banana milkshake': 1}, {'playing house': 3, 'house': 4, 'highly shared playing house': 1, 'shared playing house': 2}, {'green banana': 2, 'banana': 3, 'hot green banana': 1}, {'learning': 3, 'machine learning': 2, 'addictive machine learning': 1}, {'learning': 3, 'machine learning': 2, 'exciting machine learning': 1}, {'bath': 1}]\n"
     ]
    }
   ],
   "source": [
    "#test dictionary\n",
    "l=[['very good view swimming pool', 'good view swimming pool', 'view swimming pool', 'swimming pool',\n",
    "    'pool'], ['good beautiful shape playing house', 'beautiful shape playing house', \n",
    "    'shape playing house', 'playing house', 'house'], ['high level machine learning', \n",
    "    'level machine learning', 'machine learning', 'learning'], ['cold chocolate banana milkshake', \n",
    "    'chocolate banana milkshake', 'banana milkshake', 'milkshake'], ['highly shared playing house', \n",
    "    'shared playing house', 'playing house', 'house'], ['hot green banana', 'green banana', 'banana'],\n",
    "   ['addictive machine learning', 'machine learning', 'learning'], \n",
    "   ['exciting machine learning', 'machine learning', 'learning'], ['bath']]\n",
    "al=[]\n",
    "for el in l:\n",
    "    d={k:v+1 for v, k in enumerate(el)}\n",
    "    al.append(d)\n",
    "print(al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['very good view swimming pool', 'good beautiful shape playing house']\n"
     ]
    }
   ],
   "source": [
    "t=['very good view swimming pool', 'good beautiful shape playing house','playing house',\"hehe\"]\n",
    "h=[3,4]\n",
    "tt=[word for word in t[:2]]\n",
    "print(tt)\n",
    "if len([word for word in t[:2] if t[2] not in word]) == len(t[:2]):\n",
    "    print(t[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.0\n"
     ]
    }
   ],
   "source": [
    "##calculate c_value DONE\n",
    "import math\n",
    "\n",
    "uniq_nested_number=0\n",
    "log2_a=(math.log(float(4))/math.log(float(2)))\n",
    "freq_a=float(7)\n",
    "\n",
    "if uniq_nested_number==0:\n",
    "    c_value=log2_a*freq_a\n",
    "else:\n",
    "    inv_uniq_nested=float(1)/float(uniq_nested_number)\n",
    "    freq_nested=float(0)\n",
    "    c_value=log2_a*(freq_a - inv_uniq_nested * freq_nested)\n",
    "    \n",
    "print(c_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
