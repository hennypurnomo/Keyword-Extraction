{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file has been stored on pickle\n"
     ]
    }
   ],
   "source": [
    "import glob, os\n",
    "import xml.etree.ElementTree as et\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "path=glob.glob('./se_xml/test/*.xml')\n",
    "\n",
    "\n",
    "def parse_xml(path):\n",
    "    all_files=[]\n",
    "    for file in path:\n",
    "        tree=et.parse(file)\n",
    "        doc=[]\n",
    "        doc.append(os.path.basename(file.rstrip('.xml')))\n",
    "        #create a list that contain a dictionary for label per document\n",
    "        content={'title': None, 'abstract': None, \n",
    "                 'introduction': None, 'method': None, \n",
    "                 'evaluation': None, 'related work': None, \n",
    "                 'conclusions': None, 'full': None, 'glabels': None}\n",
    "        \n",
    "        title=[]\n",
    "        abstract=[]\n",
    "        introduction=[]\n",
    "        method=[]\n",
    "        evaluation=[]\n",
    "        related_work=[]\n",
    "        conclusions=[]\n",
    "        unknown=[]\n",
    "        full=[]\n",
    "        #loop for every sentence\n",
    "        for sentence in tree.iterfind('./document/sentences/sentence'):\n",
    "            #create dictionary\n",
    "            if sentence.attrib['section']=='title' and sentence.attrib['type']!='sectionHeader':\n",
    "                title.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "            elif sentence.attrib['section']=='abstract' and sentence.attrib['type']!='sectionHeader':\n",
    "                abstract.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "            elif sentence.attrib['section']=='introduction' and sentence.attrib['type']!='sectionHeader':\n",
    "                introduction.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "            elif sentence.attrib['section']=='method' and sentence.attrib['type']!='sectionHeader':\n",
    "                method.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "            elif sentence.attrib['section']=='evaluation' and sentence.attrib['type']!='sectionHeader':\n",
    "                evaluation.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "            elif sentence.attrib['section']=='related work' and sentence.attrib['type']!='sectionHeader':\n",
    "                related_work.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "            elif sentence.attrib['section']=='conclusions' and sentence.attrib['type']!='sectionHeader':\n",
    "                conclusions.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "            else:\n",
    "                unknown.append(' '.join([x.text for x in sentence.findall(\"tokens/token/word\")]))\n",
    "        #still on list, must be convert to \n",
    "        \n",
    "        content['title']=title\n",
    "        content['abstract']=abstract\n",
    "        content['introduction']=introduction\n",
    "        content['method']=method\n",
    "        content['evaluation']=evaluation\n",
    "        content['related work']=related_work\n",
    "        content['conclusions']=conclusions\n",
    "        content['unknown']=conclusions\n",
    "        content['full']=title+abstract+introduction+method+evaluation+related_work+conclusions+unknown\n",
    "        \n",
    "        doc.append(content)\n",
    "        all_files.append(doc)\n",
    "    return all_files\n",
    "\n",
    "testing=parse_xml(path)\n",
    "\n",
    "'''\n",
    "i=0\n",
    "while i < len(final):\n",
    "    print(final[i][0], len(final[i][1]['conclusions']))\n",
    "    i+=1\n",
    "'''\n",
    "\n",
    "with open('semeval_testing.xml.pickle','wb') as handle:\n",
    "    pickle.dump(testing, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print('file has been stored on pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4], [3, 45, 6, 7], [6, 8, 9, 0]]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "t=[[1,2,3,4],\n",
    "  [3,45,6,7],\n",
    "  [6,8,9,0]]\n",
    "\n",
    "def create_pickle(data, name):\n",
    "    with open('%s.pickle' % name,'wb') as handle:\n",
    "        result=pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return result\n",
    "\n",
    "def open_pickle(data, name):\n",
    "    with open('%s.pickle' % name,'rb') as handle:\n",
    "        result=pickle.load(handle)\n",
    "    return result\n",
    "\n",
    "pick=create_pickle(t, 'train gfdgdg')\n",
    "o=open_pickle(pick, 'train gfdgdg')\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
