{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, re, string, itertools\n",
    "import logging\n",
    "import numpy as np\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "from nltk.stem.porter import *\n",
    "from collections import defaultdict\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "from nltk.corpus import wordnet, stopwords\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "from nltk.chunk import tree2conlltags\n",
    "from pandas import DataFrame\n",
    "from itertools import chain\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from scipy.sparse.csr import csr_matrix\n",
    "from sklearn import svm                                       #library for creating the classifier, SVM\n",
    "from sklearn.ensemble import RandomForestClassifier as RF\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(input_list):\n",
    "    result=[]\n",
    "    #remove unwanted character per line\n",
    "    for line in input_list:\n",
    "        clean=re.sub(\"(\\.)?\\n\",'', line) #remove \\n\n",
    "        clean=re.sub(\"('s)\",'', clean) #remove 's\n",
    "        clean=re.sub(\"\\[([0-9]{1,2}\\,?\\s?)+\\]\",'', clean) #remove [2]\n",
    "        clean=re.sub(\"\\(([0-9]{1,2}\\,?\\s?)+\\)\",'', clean) #remove (2)\n",
    "        #clean=re.sub(r\"\\b(iv|ix|x|v?i{0,3})+\\b\",'', clean) #remove roman number\n",
    "        #remove fig. 2 etc, need improvement to catch the sentence after it\n",
    "        #clean=re.sub(r\"\\b(i.e.g.|e.g.|i.e.)\",'', clean) #remove i.e.g., i.e., e.g.\n",
    "        clean=re.sub(\"([Ff]ig.|[Ff]igure|[Tt]ab.|[Tt]able)\\s?[0-9]{1,2}\",'', clean) #remove fig. 2 etc\n",
    "        clean=re.sub(r\"\\b((https?://|www.)[^\\s]+)\",'', clean) #remove email\n",
    "        result.append(clean)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_files(path):\n",
    "    raw=[]\n",
    "    for file in path:\n",
    "        dict_doc={'doc_id': None, 'title': None, 'abstract': None, 'full-text': None, 'candidates': None}\n",
    "        file_id=os.path.basename(file).rstrip('.txt.final') #catch only file name  \n",
    "        dict_doc['doc_id']=file_id\n",
    "        \n",
    "        source=open(file,encoding='utf-8').readlines()\n",
    "        source=clean(source)\n",
    "        \n",
    "        ##########detect title\n",
    "        beginning=re.sub(\"\\n\", \"\", source[0]) #retrieve title\n",
    "        candidate=re.sub(\"\\n\", \"\", source[1]) # retrieve title candidate\n",
    "        h_candidate=word_tokenize(re.sub(\"-\",' ',candidate)) #tokenize the candidate\n",
    "        \n",
    "        title=[]\n",
    "        name=[]\n",
    "        for word in h_candidate:\n",
    "            if wordnet.synsets(word): #check if title candidate exist on wordnet\n",
    "                title.append(word)\n",
    "            else:\n",
    "                name.append(word)\n",
    "            #if title>\n",
    "            if len(title)>len(name): \n",
    "                newtitle=beginning+' '+candidate\n",
    "            elif len(title)==len(name):\n",
    "                newtitle=beginning\n",
    "            else:\n",
    "                newtitle=beginning\n",
    "\n",
    "        dict_doc['title']=newtitle\n",
    "        \n",
    "        content=source[2:]\n",
    "        ######check header, inconsistency all file\n",
    "        r_intro=re.compile(\"^1\\.?\\s[A-Z]+\")\n",
    "        r_ref=re.compile(\"[0-9]{1,2}?\\.?\\s?R[EFERENCES|eferences]\") #detect reference\n",
    "        #r_header=re.compile(\"[0-9]{1,2}?\\.?\\s?[A-Z]\")\n",
    "        \n",
    "        in_abstract=content.index('ABSTRACT')\n",
    "        in_authorkey=content.index('Categories and Subject Descriptors')\n",
    "        \n",
    "        list_intro=[i for i, item in enumerate(content) if re.search(r_intro, item)]\n",
    "        in_intro=list_intro[0]\n",
    "        list_ref=[i for i, item in enumerate(content) if re.search(r_ref, item)]\n",
    "        in_ref=list_ref[0]\n",
    "        \n",
    "        abstract=content[in_abstract+1:in_authorkey] #eliminate keyword and category\n",
    "        body=content[in_intro+1:in_ref] #remove reference       \n",
    "        \n",
    "        list_title=[]\n",
    "        list_title.append(newtitle)\n",
    "        \n",
    "        full_text=list(chain(list_title,abstract, body))\n",
    "        dict_doc['abstract']=abstract\n",
    "        dict_doc['body']=body\n",
    "        dict_doc['full_text']=full_text\n",
    "        \n",
    "        #per sentence in a document\n",
    "        raw.append(dict_doc)\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert data to tfidfvectorizer format\n",
    "#corpus=['a','b','c']\n",
    "#RENAME TO CREATE CORPUS\n",
    "def create_corpus(raw_data):\n",
    "    train_data=[]\n",
    "    for doc in raw_data:\n",
    "        #add to list and join all element in full text into a text\n",
    "        train_data.append(' '.join(doc['full_text']))\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ngram_tfidf(corpus):\n",
    "    \n",
    "    #porter stemmer\n",
    "    stemmer=PorterStemmer()\n",
    "\n",
    "    #eliminate ngram which starts or ends from stopwords\n",
    "    #from https://stackoverflow.com/questions/49746555/sklearn-tfidfvectorizer-generate-custom\n",
    "    #-ngrams-by-not-removing-stopword-in-the/49775000#49775000\n",
    "    class NewTfidfVectorizer(TfidfVectorizer):\n",
    "        def _word_ngrams(self, tokens, stop_words=None):\n",
    "            # First get tokens without stop words\n",
    "            tokens = super(TfidfVectorizer, self)._word_ngrams(tokens, None)\n",
    "            if stop_words is not None:\n",
    "                new_tokens=[]\n",
    "                for token in tokens:\n",
    "                    split_words = token.split(' ')\n",
    "                    # Only check the first and last word for stop words\n",
    "                    if len(token)>2 and split_words[0] not in stop_words and split_words[-1] not in stop_words:\n",
    "                        #stem every word in token\n",
    "                        if len(split_words)==1 and len(split_words[0])>2:\n",
    "                            new_tokens.append(stemmer.stem(token))\n",
    "                        elif len(split_words)==2 and split_words[-1]==\"'\":\n",
    "                            del(token)\n",
    "                        elif len(split_words[0])<3 and len(split_words[1])<3:\n",
    "                            del(token)\n",
    "                        elif split_words[1]==\"'\" and split_words[2]==\"s\":\n",
    "                            new_tokens.append(stemmer.stem(split_words[0])+split_words[1]+split_words[2])\n",
    "                        else:\n",
    "                            new_tokens.append(' '.join(list(stemmer.stem(word) for word in word_tokenize(token))))\n",
    "                return new_tokens\n",
    "            return tokens\n",
    "    \n",
    "    stop_words=text.ENGLISH_STOP_WORDS\n",
    "    \n",
    "    tfidf=NewTfidfVectorizer(ngram_range=(1,5), stop_words=stop_words,\n",
    "                                token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    \n",
    "    matrix=tfidf.fit_transform(corpus)\n",
    "    feature_names=tfidf.get_feature_names()\n",
    "\n",
    "    #how to print tf-idf from https://stackoverflow.com/questions/34449127/\n",
    "    #sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-documen\n",
    "    candidates=[]\n",
    "    for doc in range(0,len(corpus)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        tfidf_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_tfidf=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in tfidf_doc]]\n",
    "        candidates.append(names_tfidf)\n",
    "    \n",
    "    #this is the candidates per document\n",
    "    #vocab_perdoc=tfidf.inverse_transform(matrix)\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate noun phrases based on corpus\n",
    "def create_phrase_vocabulary(raw_data):\n",
    "    \n",
    "    #porter stemmer\n",
    "    stemmer=PorterStemmer()\n",
    "    \n",
    "    #from http://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/\n",
    "    grammar=r'NP: {(<JJ.*>* <NN.*>+ <IN>)? (<JJ.*>* <NN.*>+)+}' #only detect noun phrases that contain specific pattern, hypen word is counted as one NN\n",
    "    \n",
    "    #test new grammar\n",
    "    #grammar=r'NP: {(<JJ>* <VBN>? <NN.*>+ <IN>)? <JJ>* <VBG>? <NN.*>+}' \n",
    "    \n",
    "    punct = set(string.punctuation) #list of punctuation\n",
    "    chunker = RegexpParser(grammar) #chunker from nltk\n",
    "    \n",
    "    def lambda_unpack(f):\n",
    "        return lambda args:f(*args)\n",
    "    \n",
    "    postag_sents = pos_tag_sents(word_tokenize(sent) for sent in raw_data) #tokenize and create pos tag per sentence\n",
    "    #list of IOB of noun phrases based on the specific grammar\n",
    "    noun_phrases = list(chain.from_iterable(tree2conlltags(chunker.parse(tagged_sent)) for tagged_sent in postag_sents)) \n",
    "    \n",
    "    #join B-NP and I-NP tags as one noun phrase excluding O tags    \n",
    "    merged_nounphrase = [' '.join(stemmer.stem(word) for word, pos, chunk in group).lower() for key, group in\n",
    "                    itertools.groupby(noun_phrases, lambda_unpack(lambda word, pos, chunk: chunk != 'O')) if key]\n",
    "    \n",
    "    #filter noun phrases from stopwords and punctuation\n",
    "    all_nounphrases=[cand for cand in merged_nounphrase\n",
    "            if len(cand)>2 and not all(char in punct for char in cand)]\n",
    "    \n",
    "    #select distinct noun phrases\n",
    "    vocabulary=(list(set(all_nounphrases)))\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nounphrase_tfidf(corpus, voc):\n",
    "    \n",
    "    stemmer=PorterStemmer()\n",
    "    \n",
    "    class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "        def build_tokenizer(self):\n",
    "            tokenizer=super(TfidfVectorizer, self).build_tokenizer()\n",
    "            return lambda doc: (stemmer.stem(token) for token in tokenizer(doc) if token not in stop_words)\n",
    "\n",
    "    stop_words=set(text.ENGLISH_STOP_WORDS)\n",
    "    s=['of','in','on','for']\n",
    "    stop_words=stop_words.difference(s)\n",
    "    tfidf=StemmedTfidfVectorizer(ngram_range=(1,5), stop_words=stop_words, vocabulary=voc, token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    \n",
    "    matrix=tfidf.fit_transform(corpus)\n",
    "    feature_names=tfidf.get_feature_names()\n",
    "\n",
    "    #how to print tf-idf from https://stackoverflow.com/questions/34449127/\n",
    "    #sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-documen\n",
    "    candidates=[]\n",
    "    for doc in range(0,len(corpus)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        tfidf_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_tfidf=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in tfidf_doc]]\n",
    "        candidates.append(names_tfidf)\n",
    "    \n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "###sorting candidates based on 15 keywords\n",
    "def get_top_candidates(candidates_list, number_keyphrases):\n",
    "    best_candidates=[]\n",
    "    for doc in candidates_list:\n",
    "        #sort candidates by tf-idf value\n",
    "        sorted_candidates=sorted(doc, key=lambda x: x[1], reverse=True)[:number_keyphrases]\n",
    "        #best_candidates.append(sorted_candidates)\n",
    "        best_candidates.append([x for x,_ in sorted_candidates])\n",
    "        #remove overlapping keywords\n",
    "    return best_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###compare candidates to goldstandard\n",
    "def extract_goldkeyphrase(gold_data):\n",
    "    \n",
    "    r_plus=re.compile(\"^.*\\+.*$\")\n",
    "    r_slash=re.compile(\"^.*\\s.*\\/.*$\")\n",
    "    \n",
    "    gold_standard=[]\n",
    "    for line in gold_data.split('\\n'):\n",
    "        doc=[]      \n",
    "        for key in line[6:].split(','):\n",
    "            if key[0]==' ':\n",
    "                doc.append(key[1:])\n",
    "            elif re.search(r_plus, key):\n",
    "                split=[]\n",
    "                for element in key.split('+'):\n",
    "                    doc.append(element)\n",
    "            elif re.search(r_slash, key):\n",
    "                split=[]\n",
    "                for element in key.split('/'):\n",
    "                    doc.append(element)\n",
    "            else:\n",
    "                doc.append(key)\n",
    "        gold_standard.append(doc)\n",
    "    return gold_standard\n",
    "\n",
    "def calculate_fmeasure(candidates_list, gold_data):\n",
    "    #true positive\n",
    "    all_matches=[]\n",
    "    for index in range(len(candidates_list)):\n",
    "        #store all measure per document in dic\n",
    "        value={'tp': None, 'fp': None, 'fn': None, 'gold': None}\n",
    "        value['gold']=len(gold_data[index])\n",
    "        #counter true positive per document\n",
    "        true_positive=0\n",
    "        #loop between elements\n",
    "        for element_candidate in candidates_list[index]:                    \n",
    "            for element_goldkeyphrase in gold_data[index]:\n",
    "                #matched predicted keyword in gold keyphrase\n",
    "                if element_candidate==element_goldkeyphrase:\n",
    "                    #matches_perdoc.append(element_candidate)\n",
    "                    true_positive+=1\n",
    "            #if need the detail of evaluation\n",
    "            value['tp']=int(true_positive) #matched pair\n",
    "            value['fp']=int(15-true_positive) #depend how many keyword should we use\n",
    "            value['fn']=int(value['gold']-value['tp'])\n",
    "        #return all metrics per document\n",
    "        all_matches.append(value)\n",
    "\n",
    "    true_positive=sum(doc['tp'] for doc in all_matches)\n",
    "    false_positive=sum(doc['fp'] for doc in all_matches)\n",
    "    false_negative=sum(doc['fn'] for doc in all_matches)\n",
    "    \n",
    "    #matched/total top n\n",
    "    precision=float(true_positive/(false_positive+true_positive))\n",
    "    #matched/total gold standard\n",
    "    recall=float(true_positive/(false_negative+true_positive))\n",
    "    # calculate with micro averagedprecision\n",
    "    f_measure=float(\"{0:.2F}\".format(2*(precision*recall)/(precision+recall)*100))\n",
    "    return f_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_term_frequency(section):\n",
    "     #porter stemmer\n",
    "    stemmer=PorterStemmer()\n",
    "    \n",
    "    #eliminate ngram which starts or ends from stopwords\n",
    "    class NewCountVectorizer(CountVectorizer):\n",
    "        def _word_ngrams(self, tokens, stop_words=None):\n",
    "            # First get tokens without stop words\n",
    "            tokens = super(CountVectorizer, self)._word_ngrams(tokens, None)\n",
    "            if stop_words is not None:\n",
    "                new_tokens=[]\n",
    "                for token in tokens:\n",
    "                    split_words = token.split(' ')\n",
    "                    # Only check the first and last word for stop words\n",
    "                    if len(token)>2 and split_words[0] not in stop_words and split_words[-1] not in stop_words:\n",
    "                        #stem every word in token\n",
    "                        if len(split_words)==1 and len(split_words[0])>2:\n",
    "                            new_tokens.append(stemmer.stem(token))\n",
    "                        elif len(split_words)==2 and split_words[-1]==\"'\":\n",
    "                            del(token)\n",
    "                        elif len(split_words[0])<3 and len(split_words[1])<3:\n",
    "                            del(token)\n",
    "                        elif split_words[1]==\"'\" and split_words[2]==\"s\":\n",
    "                            new_tokens.append(stemmer.stem(split_words[0])+split_words[1]+split_words[2])\n",
    "                        else:\n",
    "                            new_tokens.append(' '.join(list(stemmer.stem(word) for word in word_tokenize(token))))\n",
    "                return new_tokens\n",
    "            return tokens\n",
    "    \n",
    "    stop_words=text.ENGLISH_STOP_WORDS\n",
    "    \n",
    "    count_vect=NewCountVectorizer(ngram_range=(1,5), stop_words=stop_words,\n",
    "                                token_pattern=r\"(?u)\\b[A-Za-z-]+\\b\")\n",
    "    \n",
    "    matrix=count_vect.fit_transform(section)\n",
    "    feature_names=count_vect.get_feature_names()\n",
    "\n",
    "    #how to print tf-idf from https://stackoverflow.com/questions/34449127/\n",
    "    #sklearn-tfidf-transformer-how-to-get-tf-idf-values-of-given-words-in-document\n",
    "    ngrams=[]\n",
    "    for doc in range(0,len(section)):\n",
    "        feature_index=matrix[doc,:].nonzero()[1]\n",
    "        count_vect_doc=zip(feature_index, [matrix[doc, x] for x in feature_index])\n",
    "        names_count_vect=[(w, s) for w, s in [(feature_names[i], s) for (i, s) in count_vect_doc]]\n",
    "        ngrams.append(names_count_vect)\n",
    "    \n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------TF-IDF version\n",
    "###load training data\n",
    "train_directory=glob.glob('./se_txt/train/*.txt.final')\n",
    "train_raw=load_files(train_directory)\n",
    "pickle_train_raw=create_pickle(train_raw,'txt train raw')\n",
    "train_data=create_corpus(train_raw)\n",
    "pickle_train_data=create_pickle(train_data,'txt train data')\n",
    "\n",
    "#load gold keyphrase\n",
    "train_label_directory=open('./se_txt/train/train.combined.stem.final', encoding='utf-8').read()\n",
    "train_label=extract_goldkeyphrase(train_label_directory)\n",
    "pickle_train_label=create_pickle(train_label, 'txt train label')\n",
    "\n",
    "###Load testing data\n",
    "test_directory=glob.glob('./se_txt/test/*.txt.final')\n",
    "test_raw=load_files(test_directory)\n",
    "pickle_test_raw=create_pickle(test_raw,'txt test raw')\n",
    "test_data=create_corpus(test_raw)\n",
    "pickle_test_data=create_pickle(test_data,'txt test data')\n",
    "\n",
    "test_label_directory=open('./se_txt/test_answer/test.combined.stem.final', encoding='utf-8').read()\n",
    "test_label=extract_goldkeyphrase(test_label_directory)\n",
    "pickle_test_label=create_pickle(test_label, 'txt test label')\n",
    "\n",
    "#### Ngram version\n",
    "print(\"N-gram TF-IDF version\")\n",
    "ngram_candidates=calculate_ngram_tfidf(train_data) \n",
    "pickle_ngram_candidates=create_pickle(ngram_candidates, 'txt ngram candidates')\n",
    "#ngram_top_keyphrases=get_top_candidates(ngram_candidates, 15)\n",
    "#ngram_fmeasure=calculate_fmeasure(ngram_top_keyphrases, train_label)\n",
    "#print(\"F-measure on training:\", ngram_fmeasure)\n",
    "\n",
    "test_ngram_candidates=calculate_ngram_tfidf(test_data)\n",
    "pickle_test_ngram_candidates=create_pickle(test_ngram_candidates, 'txt test ngram candidates')\n",
    "#test_ngram_top_candidates=get_top_candidates(test_ngram_candidates, 15)\n",
    "#test_ngram_fmeasure=calculate_fmeasure(test_ngram_top_candidates, test_label)\n",
    "#print(\"F-measure on testing:\", test_ngram_fmeasure)\n",
    "\n",
    "\n",
    "#### Noun phrase version\n",
    "print(\"Noun phrase TF-IDF version\")\n",
    "nounphrase_vocabulary=create_phrase_vocabulary(train_data)\n",
    "nounphrase_candidates=calculate_nounphrase_tfidf(train_data, nounphrase_vocabulary)\n",
    "pickle_nounphrase_candidates=create_pickle(nounphrase_candidates, 'txt nounphrase candidates')\n",
    "#nounphrase_top_keyphrases=get_top_candidates(nounphrase_candidates, 15)\n",
    "#nounphrase_fmeasure=calculate_fmeasure(nounphrase_top_keyphrases, train_label)\n",
    "#print(\"F-measure on training:\", nounphrase_fmeasure)\n",
    "\n",
    "test_nounphrase_vocabulary=create_phrase_vocabulary(test_data)\n",
    "test_nounphrase_candidates=calculate_nounphrase_tfidf(test_data, test_nounphrase_vocabulary)\n",
    "pickle_test_nounphrase_candidates=create_pickle(test_nounphrase_candidates, 'txt test nounphrase candidates')\n",
    "#test_nounphrase_top_candidates=get_top_candidates(test_nounphrase_candidates, 15)\n",
    "#test_nounphrase_fmeasure=calculate_fmeasure(test_nounphrase_top_candidates, test_label)\n",
    "#print(\"F-measure on testing:\", test_nounphrase_fmeasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corpus(corpus):\n",
    "    clean=[]\n",
    "    stemmer=PorterStemmer()\n",
    "    for doc in corpus:\n",
    "        cleaned_words=\" \".join([word for word in word_tokenize(doc.lower()) if re.search(r\"\\b[A-Za-z-]+\\b\", word) and len(word)>2])\n",
    "        stemmed_words=[stemmer.stem(word) for word in cleaned_words.split()]\n",
    "        clean.append(\" \".join([word for word in stemmed_words]))\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(raw_data, corpus, candidates, label):\n",
    "    \n",
    "    def feature_is_title(candidates, raw_data):\n",
    "        titles=[doc['title'] for doc in raw_data]\n",
    "        title_tf=calculate_term_frequency(titles)\n",
    "        feature2=[]\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_feature in range(len(candidates[n_doc])):\n",
    "                features_perdoc=[feature for feature, value in title_tf[n_doc]]\n",
    "                if candidates[n_doc][n_feature][0] not in features_perdoc:\n",
    "                    doc.append(0)\n",
    "                else:\n",
    "                    doc.append(1)\n",
    "            feature2.append(doc)\n",
    "        return feature2\n",
    "    \n",
    "    #refine with similarity\n",
    "    def feature_is_abstract(candidates, raw_data):\n",
    "        abstracts=[' '.join(doc['abstract']) for doc in raw_data]\n",
    "        abstract_tf=calculate_term_frequency(abstracts)\n",
    "        feature3=[]\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_feature in range(len(candidates[n_doc])):\n",
    "                features_perdoc=[feature for feature, value in abstract_tf[n_doc]]\n",
    "                if candidates[n_doc][n_feature][0] not in features_perdoc:\n",
    "                    doc.append(0)\n",
    "                else:\n",
    "                    doc.append(1)\n",
    "            feature3.append(doc)\n",
    "        return feature3\n",
    "    \n",
    "    def feature_candidate_length(candidates):\n",
    "        feature4=[]\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_feature in range(len(candidates[n_doc])):\n",
    "                doc.append(len(candidates[n_doc][n_feature][0]))\n",
    "            feature4.append(doc)\n",
    "        return feature4\n",
    "    \n",
    "    def feature_term_frequency(corpus):\n",
    "        term_frequency=calculate_term_frequency(corpus) #save as pickle for term frequency, it can be used for counting n title or n abstract\n",
    "        feature5=[]\n",
    "        for n_doc in range(len(term_frequency)):\n",
    "            doc=[]\n",
    "            for n_feature in range(len(term_frequency[n_doc])):\n",
    "                doc.append(term_frequency[n_doc][n_feature][1])\n",
    "            feature5.append(doc)\n",
    "        return feature5\n",
    "    \n",
    "    def feature_supervised_keyphraseness(corpus, label): #make sure this is only keyphrase per document or all keyphrase compare\n",
    "        term_frequency=calculate_term_frequency(corpus)\n",
    "        merged_labels=list(chain.from_iterable(label))\n",
    "        feature6=[]\n",
    "        for n_doc in range(len(term_frequency)):\n",
    "            doc=[]\n",
    "            for n_feature in range(len(term_frequency[n_doc])):\n",
    "                #gold_label=list(label[n_doc])\n",
    "                if term_frequency[n_doc][n_feature][0] not in merged_labels:\n",
    "                    doc.append(0)\n",
    "                else:\n",
    "                    doc.append(term_frequency[n_doc][n_feature][1])\n",
    "            feature6.append(doc)\n",
    "        return feature6\n",
    "    \n",
    "    def feature_first_occurence(candidates, corpus):\n",
    "        feature7=[]\n",
    "        cleaned_corpus=clean_corpus(corpus)\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                index=cleaned_corpus[n_doc].lower().find(candidates[n_doc][n_cand][0])\n",
    "                preceding_words=len(cleaned_corpus[n_doc][:index].split(\" \"))-1\n",
    "                doc.append(preceding_words)\n",
    "            feature7.append(doc)\n",
    "        return feature7\n",
    "\n",
    "    def feature_distance(candidates, corpus):\n",
    "        #cleaning the CORPUS from \n",
    "        feature8=[]\n",
    "        cleaned_corpus=clean_corpus(corpus)\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            corpus_words=len(cleaned_corpus[n_doc].split(\" \"))\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                index=cleaned_corpus[n_doc].lower().find(candidates[n_doc][n_cand][0])\n",
    "                preceding_words=len(cleaned_corpus[n_doc][:index].split(\" \"))-1\n",
    "                position=float(\"{0:.2F}\".format(preceding_words/corpus_words))\n",
    "                doc.append(position)\n",
    "            feature8.append(doc)\n",
    "        return feature8\n",
    "    \n",
    "    #lists of feature\n",
    "\n",
    "    feature2=feature_is_title(candidates, raw_data)\n",
    "    feature3=feature_is_abstract(candidates, raw_data)\n",
    "    feature4=feature_candidate_length(candidates)\n",
    "    feature5=feature_term_frequency(corpus)\n",
    "    feature6=feature_supervised_keyphraseness(corpus, label)\n",
    "\n",
    "    feature7=feature_first_occurence(candidates, corpus) #important feature\n",
    "    feature8=feature_distance(candidates, corpus)\n",
    "    \n",
    "    #add values of all features into candidate list\n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_candidate in range(len(candidates[n_doc])):\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate],)\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature3[n_doc][n_candidate],)\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature4[n_doc][n_candidate],)  \n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature5[n_doc][n_candidate],)\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature6[n_doc][n_candidate],)\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature7[n_doc][n_candidate],)\n",
    "            candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature8[n_doc][n_candidate],)\n",
    "            \n",
    "    #convert the format from candidate from tuple to list\n",
    "    x_data=[]\n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_candidate in range(len(candidates[n_doc])):\n",
    "            #append only values of features. without word\n",
    "            x_data.append(list(candidates[n_doc][n_candidate][1:]))\n",
    "    return x_data     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "##create label for training or testing\n",
    "def create_label(candidates, label):\n",
    "    y_label=[]\n",
    "    for n_doc in range(len(candidates)):\n",
    "        for n_cand in range(len(candidates[n_doc])):\n",
    "            keyphrase_document=list(label[n_doc])\n",
    "            if candidates[n_doc][n_cand][0] not in keyphrase_document:\n",
    "                y_label.append(0)\n",
    "            else:\n",
    "                y_label.append(1)\n",
    "    return y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_to_fmeasure(predict_proba, candidates, labels, models):\n",
    "    #all_fmeasure=[]\n",
    "    for model in range(0, len(predict_proba)):\n",
    "        probability=[]\n",
    "        counter=0\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                doc.append((candidates[n_doc][n_cand][0], predict_proba[model][counter]))\n",
    "                counter+=1\n",
    "            probability.append(doc)\n",
    "        fmeasure=calculate_fmeasure(get_top_candidates(probability, 15), labels)\n",
    "        print(\"Model %s: %.3f\" % (models[model][0], fmeasure))\n",
    "        #all_fmeasure.append((models[model][0], fmeasure))\n",
    "    return 'finish'\n",
    "\n",
    "def predict_data(x_train, y_train, x_test, y_test, candidates, labels):\n",
    "    seed = 7 #just randomly select the number\n",
    "    models = []\n",
    "    #models.append(('LR', LogisticRegression()))\n",
    "    #models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "    #models.append(('KNN', KNeighborsClassifier()))\n",
    "    #models.append(('DT', DecisionTreeClassifier()))\n",
    "    #models.append(('NB', GaussianNB()))\n",
    "    #models.append(('SVM', SVC(probability=True)))\n",
    "    models.append(('RF', RF(n_estimators=10, max_depth=3)))\n",
    "    results = []\n",
    "    names = []\n",
    "    scoring='accuracy'\n",
    "    #print(\"\\nAccuracy on testing data:\")\n",
    "    \n",
    "    #loop as many as features\n",
    "    print(\"Take one feature out\")\n",
    "    for feature in range(len(x_train[0])):\n",
    "        print(\"Remove feature number\", feature+1)\n",
    "     \n",
    "        modified_train=[doc[:feature]+doc[feature+1:] for doc in x_train] \n",
    "        modified_test=[doc[:feature]+doc[feature+1:] for doc in x_test]\n",
    "        \n",
    "        predict_proba=[]\n",
    "        for name, model in models:\n",
    "        #calculate F-score, recall and precision\n",
    "            #print(\"%s: %.3f\" % (name, accuracy_score(model.fit(modified_train, train_label).predict(modified_test), test_label)))\n",
    "            predict_proba.append(model.fit(x_train, y_train).predict_proba(x_test)[:,1])\n",
    "            \n",
    "        #calculate f-measure\n",
    "        fmeasure=probability_to_fmeasure(predict_proba, candidates, labels, models)\n",
    "        \n",
    "    print(\"Full features:\")\n",
    "    all_predict_proba=[]\n",
    "    for name, model in models:\n",
    "        #accuracy\n",
    "        #print(\"%s: %.3f\" % (name, accuracy_score(model.fit(x_train, y_train).predict(x_test), y_test)))\n",
    "        all_predict_proba.append(model.fit(x_train, y_train).predict_proba(x_test)[:,1])\n",
    "    \n",
    "    all_fmeasure=probability_to_fmeasure(all_predict_proba, candidates, labels, models)\n",
    "    '''\n",
    "    all_fmeasure=[]\n",
    "    for model in range(0, len(all_predict_proba)):\n",
    "        probability=[]\n",
    "        counter=0\n",
    "        for n_doc in range(len(candidates)):\n",
    "            doc=[]\n",
    "            for n_cand in range(len(candidates[n_doc])):\n",
    "                doc.append((candidates[n_doc][n_cand][0], all_predict_proba[model][counter]))\n",
    "                counter+=1\n",
    "            probability.append(doc)\n",
    "        fmeasure=calculate_fmeasure(get_top_candidates(probability, 15), labels)\n",
    "        all_fmeasure.append((models[model][0], fmeasure))\n",
    "    '''\n",
    "    return 'all predictions have been completed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def create_pickle(data, name):\n",
    "    with open('%s.pickle' % name,'wb') as handle:\n",
    "        result=pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return result\n",
    "\n",
    "def open_pickle(name):\n",
    "    with open('%s.pickle' % name,'rb') as handle:\n",
    "        result=pickle.load(handle)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opening all pickles\n",
      "creating example on training..\n",
      "creating label on training..\n",
      "creating example on testing..\n",
      "creating label on testing..\n",
      "F-measure with machine learning (testing)\n",
      "Take one feature out\n",
      "Remove feature number 1\n",
      "Model RF: 15.940\n",
      "Remove feature number 2\n",
      "Model RF: 18.130\n",
      "Remove feature number 3\n",
      "Model RF: 14.010\n",
      "Remove feature number 4\n",
      "Model RF: 17.530\n",
      "Remove feature number 5\n",
      "Model RF: 17.070\n",
      "Remove feature number 6\n",
      "Model RF: 17.860\n",
      "Remove feature number 7\n",
      "Model RF: 16.930\n",
      "Remove feature number 8\n",
      "Model RF: 16.800\n",
      "Full features:\n",
      "Model RF: 15.470\n",
      "F-measure on ngram all predictions have been completed\n"
     ]
    }
   ],
   "source": [
    "####with machine learning\n",
    "##NGRAM\n",
    "#open all pickle\n",
    "print(\"opening all pickles\")\n",
    "train_raw=open_pickle('txt train raw')\n",
    "train_data=open_pickle('txt train data')\n",
    "train_label=open_pickle('txt train label')\n",
    "\n",
    "test_raw=open_pickle('txt test raw')\n",
    "test_data=open_pickle('txt test data')\n",
    "test_label=open_pickle('txt test label')\n",
    "\n",
    "ngram_candidates=open_pickle('txt ngram candidates')\n",
    "test_ngram_candidates=open_pickle('txt test ngram candidates')\n",
    "#nounphrase_candidates=open_pickle('txt nounphrase candidates')\n",
    "#test_nounphrase_candidates=open_pickle('txt test nounphrase candidates')\n",
    "\n",
    "print(\"creating example on training..\")\n",
    "ngram_x_train=create_example(train_raw, train_data, ngram_candidates, train_label)\n",
    "print(\"creating label on training..\")\n",
    "ngram_y_train=create_label(ngram_candidates, train_label)\n",
    "print(\"creating example on testing..\")\n",
    "ngram_x_test=create_example(test_raw, test_data, test_ngram_candidates, test_label)\n",
    "print(\"creating label on testing..\")\n",
    "ngram_y_test=create_label(test_ngram_candidates, test_label)\n",
    "\n",
    "#nounphrase_x_train=create_example(train_raw, train_data, nounphrase_candidates, train_label)\n",
    "#nounphrase_y_train=create_label(nounphrase_candidates, train_label)\n",
    "#nounphrase_x_test=create_example(test_raw, test_data, test_nounphrase_candidates, test_label)\n",
    "#nounphrase_y_test=create_label(test_nounphrase_candidates, test_label)\n",
    "\n",
    "\n",
    "print(\"F-measure with machine learning (testing)\")\n",
    "ngram_prediction=predict_data(ngram_x_train, ngram_y_train, ngram_x_test, ngram_y_test, test_ngram_candidates, test_label)\n",
    "print('F-measure on ngram', ngram_prediction)\n",
    "#nounphrase_prediction=predict_data(nounphrase_x_train, nounphrase_y_train, nounphrase_x_test, nounphrase_y_test, test_nounphrase_candidates, test_label)\n",
    "#print('F-measure on noun phrase', nounphrase_prediction)\n",
    "\n",
    "#print(len(x_train_ngram))#print(len(y_train_ngram))#print(len(x_test_ngram))#print(len(y_test_ngram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##trying two new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1, 1, 1, 7, 0, 0), (1, 2, 0, 0, 0, 0), (0, 0, 0, 0, 1, 15), (0, 0, 1, 8, 1, 16)], [(0, 0, 1, 9, 0, 0), (0, 0, 1, 10, 1, 17), (0, 0, 1, 11, 1, 18), (1, 3, 1, 12, 1, 19), (1, 4, 0, 0, 1, 20)], [(1, 5, 1, 13, 0, 0), (1, 6, 1, 14, 0, 0), (0, 0, 0, 0, 1, 21), (0, 0, 0, 0, 1, 22)]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "from itertools import chain\n",
    "\n",
    "def clean_corpus(corpus):\n",
    "    clean=[]\n",
    "    stemmer=PorterStemmer()\n",
    "    for doc in corpus:\n",
    "        cleaned_words=\" \".join([word for word in word_tokenize(doc.lower()) if re.search(r\"\\b[A-Za-z-]+\\b\", word) and len(word)>2])\n",
    "        stemmed_words=[stemmer.stem(word) for word in cleaned_words.split()]\n",
    "        clean.append(\" \".join([word for word in stemmed_words]))\n",
    "    return clean\n",
    "\n",
    "corpus=[\"henny is amazing henny 7877 is very nice person, henny amazing\",\n",
    "           \"Tony is a good student in his class, but he has never got a high score. student his never\",\n",
    "            \"Luca citi is teaching machine learning. teaching machine luca citi\"]\n",
    "\n",
    "candidates=[[(\"henni\",0), (\"amaz\",2),(\"amaz henni\",2),(\"veri nice\",3)],\n",
    "           [(\"student\",4),(\"hi\",9),(\"class\",4),(\"never\",5),(\"got\",3)],\n",
    "            [(\"luca\",5),(\"citi\",3),(\"teach\",8),(\"machin\",5)]]\n",
    "\n",
    "cleaned_corpus=clean_corpus(corpus)\n",
    "\n",
    "def feature_structure(candidates, raw_data):\n",
    "    title_raw=[doc['title'] for doc in raw_data]\n",
    "    abstract_raw=[' '.join(doc['abstract']) for doc in raw_data]\n",
    "    introduction_raw=[' '.join(doc['introduction']) for doc in raw_data]  \n",
    "    title=calculate_term_frequency(title_raw)\n",
    "    abstract=calculate_term_frequency(abstract_raw)\n",
    "    introduction=calculate_term_frequency(introduction_raw) \n",
    "    feature=[]\n",
    "    for n_doc in range(len(candidates)):\n",
    "        doc=[]\n",
    "        for n_cand in range(len(candidates[n_doc])):\n",
    "            title_perdoc=[feature for (feature, value) in title[n_doc]]\n",
    "            abstract_perdoc=[feature for (feature, value) in abstract[n_doc]]\n",
    "            introduction_perdoc=[feature for (feature, value) in introduction[n_doc]]\n",
    "            if candidates[n_doc][n_cand][0] in title_perdoc:\n",
    "                binary_title=1\n",
    "                value=[value for (feature, value) in title[n_doc] if feature in candidates[n_doc][n_cand][0]]\n",
    "                number_title=value[0]\n",
    "            else:\n",
    "                binary_title=0\n",
    "                number_title=0\n",
    "            if candidates[n_doc][n_cand][0] in abstract_perdoc:\n",
    "                binary_abstract=1\n",
    "                value=[value for (feature, value) in abstract[n_doc] if feature in candidates[n_doc][n_cand][0]]\n",
    "                number_abstract=value[0]\n",
    "            else:\n",
    "                binary_abstract=0\n",
    "                number_abstract=0\n",
    "            if candidates[n_doc][n_cand][0] in introduction_perdoc:\n",
    "                binary_introduction=1\n",
    "                value=[value for (feature, value) in introduction[n_doc] if feature in candidates[n_doc][n_cand][0]]\n",
    "                number_introduction=value[0]\n",
    "            else:\n",
    "                binary_introduction=0\n",
    "                number_introduction=0\n",
    "            doc.append(((binary_title, number_title, binary_abstract, number_abstract, binary_introduction, number_introduction)))\n",
    "        feature.append(doc)\n",
    "    return feature\n",
    "#print(clean_corpus(corpus))\n",
    "print(feature_structure(candidates, corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "candidates=[[(\"henni\",0), (\"amaz\",2),(\"amaz henni\",2),(\"veri nice\",3)],\n",
    "           [(\"student\",4),(\"hi\",9),(\"class\",4),(\"never\",5),(\"got\",3)],\n",
    "            [(\"luca\",5),(\"citi\",3),(\"teach\",8),(\"machin\",5)]]\n",
    "title=[[(\"henni\",1), (\"amaz\",2)],\n",
    "           [(\"never\",3),(\"got\",4)],\n",
    "            [(\"luca\",5),(\"citi\",6)]]\n",
    "abstract=[[(\"henni\",7),(\"veri nice\",8)],\n",
    "           [(\"student\",9),(\"hi\",10),(\"class\",11),(\"never\",12)],\n",
    "            [(\"luca\",13),(\"citi\",14)]]\n",
    "    introduction=[[(\"amaz henni\",15),(\"veri nice\",16)],\n",
    "           [(\"hi\",17),(\"class\",18),(\"never\",19),(\"got\",20)],\n",
    "            [(\"teach\",21),(\"machin\",22)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing o compare tfidfvalue with one feature\n",
    "\n",
    "\n",
    "#feature phrase length\n",
    "#feature1=[]\n",
    "#for n_doc in range(len(tfidf)):\n",
    "#    doc=[]\n",
    "#    for n_feature in range(len(tfidf[n_doc])):\n",
    "#        doc.append(len(tfidf[n_doc][n_feature][0]))\n",
    "#    feature1.append(doc)\n",
    "#print(feature1)\n",
    "\n",
    "tfidf=[[('dog',1),('swimming',4),('car',7)],\n",
    "      [('air',11),('bowl',14),('cone',17),('done',17)],\n",
    "       [('air of water',21),('chocolate biscuit',24)],\n",
    "      [('air conditioner',21),('hot white chocolate',24)],]\n",
    "\n",
    "title=[[('dog',0),('rabbit',0),('snake',0),('car',0)],\n",
    "      [('bowl',0),('dog',0),('rabbit',0)],\n",
    "      [('chocolate biscuits',0),('a lot air of water',0),('rabbit',0),('snake',0)],\n",
    "      [('air conditioner',0),('hot white',0)]]\n",
    "\n",
    "#is_title, is_abstract, is etc, but extract section with ngram(1,5)\n",
    "feature2=[]\n",
    "for n_doc in range(len(tfidf)):\n",
    "    doc=[]\n",
    "    for n_feature in range(len(tfidf[n_doc])):\n",
    "        #title_feature=[feature for feature in title[n_doc]]\n",
    "        title_feature=[feature for feature, value in title[n_doc]]\n",
    "        if tfidf[n_doc][n_feature][0] not in title_feature:\n",
    "            doc.append(0)\n",
    "        else:\n",
    "            doc.append(1)\n",
    "    feature2.append(doc)\n",
    "print(feature2)\n",
    "\n",
    "#is_abstract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''If need cross validation per model\n",
    "###measure accuracy with k-fold\n",
    "print(\"Accuracy on training data with Cross-validation:\")\n",
    "for name, model in models:\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model, x_train_ngram, y_train_ngram, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %.3f (%.3f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates=[[('aa',1),('a',5),('a3',5),('a6',7)],\n",
    "            [('aq',3),('aw',4),('ag',2),('ar',8)]]\n",
    "\n",
    "feature1=[[3,4,5,6],\n",
    "            [7,9,6,5]]\n",
    "feature2=[[1,2,7,8],\n",
    "            [9,90,4,3]]\n",
    "\n",
    "for n_doc in range(len(candidates)):\n",
    "    for n_candidate in range(len(candidates[n_doc])):\n",
    "        candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature1[n_doc][n_candidate],)\n",
    "        candidates[n_doc][n_candidate]=candidates[n_doc][n_candidate]+(feature2[n_doc][n_candidate],)\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unorderable types: NoneType() > float()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-7db2e3e8c518>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfocus_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfocus_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;31m#print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (sentence, focus_sentence, sentence_similarity(sentence, focus_sentence)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-29-7db2e3e8c518>\u001b[0m in \u001b[0;36msentence_similarity\u001b[1;34m(sentence1, sentence2)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msynset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynsets1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;31m# Get the similarity value of the most similar word in the other sentence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mbest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msynset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwup_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mss\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mss\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msynsets2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# Check that the similarity could have been computed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unorderable types: NoneType() > float()"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    " \n",
    "def penn_to_wn(tag):\n",
    "    \"\"\" Convert between a Penn Treebank tag to a simplified Wordnet tag \"\"\"\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return None\n",
    " \n",
    "def tagged_to_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def sentence_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the sentence similarity using Wordnet \"\"\"\n",
    "    # Tokenize and tag\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tagged_to_synset(*tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tagged_to_synset(*tagged_word) for tagged_word in sentence2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        best_score = max([synset.wup_similarity(ss) for ss in synsets2])\n",
    " \n",
    "        # Check that the similarity could have been computed\n",
    "        if best_score is not None:\n",
    "            score += best_score\n",
    "            count += 1\n",
    " \n",
    "    # Average the values\n",
    "    score /= count\n",
    "    return score\n",
    " \n",
    "sentences = [\n",
    "    \"Dogs are awesome.\",\n",
    "    \"Dolphins are swimming mammals.\",\n",
    "    \"Cats are beautiful animals.\",\n",
    "]\n",
    " \n",
    "focus_sentence = \"Cats are beautiful animals.\"\n",
    " \n",
    "for sentence in sentences:\n",
    "    print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (focus_sentence, sentence, sentence_similarity(focus_sentence, sentence)))\n",
    "    #print(\"Similarity(\\\"%s\\\", \\\"%s\\\") = %s\" % (sentence, focus_sentence, sentence_similarity(sentence, focus_sentence)))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Synset('orange.n.01'), Synset('frank.n.02')), (Synset('orange.n.04'), Synset('kat.n.01')), (Synset('orange.n.01'), Synset('banana.n.02')), (Synset('orange.n.01'), Synset('melon.n.01')), (Synset('grape.n.01'), Synset('frank.n.02')), (Synset('grapeshot.n.01'), Synset('cat-o'-nine-tails.n.01')), (Synset('grape.n.01'), Synset('banana.n.02')), (Synset('grape.n.01'), Synset('melon.n.01')), (Synset('zebra.n.01'), Synset('dog.n.01')), (Synset('zebra.n.01'), Synset('cat.n.01')), (Synset('zebra.n.01'), Synset('banana.n.01')), (Synset('zebra.n.01'), Synset('melon.n.02'))]\n"
     ]
    }
   ],
   "source": [
    "def get_best_synset_pair(word_1, word_2):\n",
    "    \"\"\"\n",
    "    Choose the pair with highest path similarity among all pairs.\n",
    "    Mimics pattern-seeking behavior of humans.\n",
    "    \"\"\"\n",
    "    max_sim = -1.0\n",
    "    synsets_1 = wn.synsets(word_1)\n",
    "    synsets_2 = wn.synsets(word_2)\n",
    "    if len(synsets_1) == 0 or len(synsets_2) == 0:\n",
    "        return None, None\n",
    "    else:\n",
    "        max_sim = -1.0\n",
    "        best_pair = None, None\n",
    "        for synset_1 in synsets_1:\n",
    "            for synset_2 in synsets_2:\n",
    "               sim = wn.path_similarity(synset_1, synset_2)\n",
    "               if sim is not None and sim > max_sim:\n",
    "                   max_sim = sim\n",
    "                   best_pair = synset_1, synset_2\n",
    "        return best_pair \n",
    "\n",
    "t=[\"dog\", \"cat\", \"banana\", \"melon\"]\n",
    "f=[\"orange\", \"grape\", \"zebra\"]\n",
    "\n",
    "#compare f to t\n",
    "al=[]\n",
    "for el in f:\n",
    "    for ele in t:\n",
    "        al.append(get_best_synset_pair(el, ele))\n",
    "print(al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
